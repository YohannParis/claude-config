{"parentUuid":null,"isSidechain":false,"userType":"external","cwd":"/Users/yparis/Developer/atlantis","sessionId":"2b976885-8546-4e96-b019-e86ba8fb0435","version":"1.0.123","gitBranch":"feat-delete-facet-values","type":"user","message":{"role":"user","content":"Caveat: The messages below were generated by the user while running local commands. DO NOT respond to these messages or otherwise consider them in your response unless the user explicitly asks you to."},"isMeta":true,"uuid":"9004544d-7aac-4492-b460-4c791ab6bd08","timestamp":"2025-09-24T20:50:39.280Z"}
{"parentUuid":"9004544d-7aac-4492-b460-4c791ab6bd08","isSidechain":false,"userType":"external","cwd":"/Users/yparis/Developer/atlantis","sessionId":"2b976885-8546-4e96-b019-e86ba8fb0435","version":"1.0.123","gitBranch":"feat-delete-facet-values","type":"user","message":{"role":"user","content":"<command-name>/clear</command-name>\n            <command-message>clear</command-message>\n            <command-args></command-args>"},"uuid":"dacdd531-5f22-4bed-9802-8d7c6ce93248","timestamp":"2025-09-24T20:50:39.266Z"}
{"parentUuid":"dacdd531-5f22-4bed-9802-8d7c6ce93248","isSidechain":false,"userType":"external","cwd":"/Users/yparis/Developer/atlantis","sessionId":"2b976885-8546-4e96-b019-e86ba8fb0435","version":"1.0.123","gitBranch":"feat-delete-facet-values","type":"user","message":{"role":"user","content":"<local-command-stdout></local-command-stdout>"},"uuid":"b2bc2c40-1694-4d82-899a-dd2d167267da","timestamp":"2025-09-24T20:50:39.280Z"}
{"parentUuid":"b2bc2c40-1694-4d82-899a-dd2d167267da","isSidechain":false,"userType":"external","cwd":"/Users/yparis/Developer/atlantis","sessionId":"2b976885-8546-4e96-b019-e86ba8fb0435","version":"1.0.123","gitBranch":"feat-delete-facet-values","type":"user","message":{"role":"user","content":"As a QA specialist please review the uncommited files in this branch, and provide some review, specially like removing logs, comments to nowhere, etc."},"uuid":"bba35b9b-7152-45ed-b091-20f98edbf1da","timestamp":"2025-09-24T20:51:26.808Z","thinkingMetadata":{"level":"none","disabled":false,"triggers":[]}}
{"parentUuid":"bba35b9b-7152-45ed-b091-20f98edbf1da","isSidechain":false,"userType":"external","cwd":"/Users/yparis/Developer/atlantis","sessionId":"2b976885-8546-4e96-b019-e86ba8fb0435","version":"1.0.123","gitBranch":"feat-delete-facet-values","message":{"id":"msg_01QixxbSde5Rwd77SBDFdabR","type":"message","role":"assistant","model":"claude-sonnet-4-20250514","content":[{"type":"text","text":"I'll review the uncommitted files in your branch to identify issues like logs, unnecessary comments, and other QA concerns. Let me start by examining the modified files."}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":4,"cache_creation_input_tokens":4106,"cache_read_input_tokens":11612,"cache_creation":{"ephemeral_5m_input_tokens":4106,"ephemeral_1h_input_tokens":0},"output_tokens":3,"service_tier":"standard"}},"requestId":"req_011CTTtQUhnjxvsyw5cPhEJ5","type":"assistant","uuid":"0dcc71d3-b6a6-4790-90bc-9ca1ac26bd06","timestamp":"2025-09-24T20:51:30.040Z"}
{"parentUuid":"0dcc71d3-b6a6-4790-90bc-9ca1ac26bd06","isSidechain":false,"userType":"external","cwd":"/Users/yparis/Developer/atlantis","sessionId":"2b976885-8546-4e96-b019-e86ba8fb0435","version":"1.0.123","gitBranch":"feat-delete-facet-values","message":{"id":"msg_01QixxbSde5Rwd77SBDFdabR","type":"message","role":"assistant","model":"claude-sonnet-4-20250514","content":[{"type":"tool_use","id":"toolu_01PzqZvP5Naoa8cUwptz8n4j","name":"TodoWrite","input":{"todos":[{"content":"Read and review ElasticsearchService.java for QA issues","status":"pending","activeForm":"Reading and reviewing ElasticsearchService.java for QA issues"},{"content":"Read and review ElasticsearchServiceTests.java for QA issues","status":"pending","activeForm":"Reading and reviewing ElasticsearchServiceTests.java for QA issues"},{"content":"Review new bulk-property-deletion.painless script","status":"pending","activeForm":"Reviewing new bulk-property-deletion.painless script"},{"content":"Provide comprehensive QA review summary","status":"pending","activeForm":"Providing comprehensive QA review summary"}]}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":4,"cache_creation_input_tokens":4106,"cache_read_input_tokens":11612,"cache_creation":{"ephemeral_5m_input_tokens":4106,"ephemeral_1h_input_tokens":0},"output_tokens":301,"service_tier":"standard"}},"requestId":"req_011CTTtQUhnjxvsyw5cPhEJ5","type":"assistant","uuid":"e3c24ea1-bfa5-44ee-a055-7713720f89fe","timestamp":"2025-09-24T20:51:32.979Z"}
{"parentUuid":"e3c24ea1-bfa5-44ee-a055-7713720f89fe","isSidechain":false,"userType":"external","cwd":"/Users/yparis/Developer/atlantis","sessionId":"2b976885-8546-4e96-b019-e86ba8fb0435","version":"1.0.123","gitBranch":"feat-delete-facet-values","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01PzqZvP5Naoa8cUwptz8n4j","type":"tool_result","content":"Todos have been modified successfully. Ensure that you continue to use the todo list to track your progress. Please proceed with the current tasks if applicable"}]},"uuid":"cc01d31f-1e42-4140-a67e-cbee482ab800","timestamp":"2025-09-24T20:51:33.011Z","toolUseResult":{"oldTodos":[],"newTodos":[{"content":"Read and review ElasticsearchService.java for QA issues","status":"pending","activeForm":"Reading and reviewing ElasticsearchService.java for QA issues"},{"content":"Read and review ElasticsearchServiceTests.java for QA issues","status":"pending","activeForm":"Reading and reviewing ElasticsearchServiceTests.java for QA issues"},{"content":"Review new bulk-property-deletion.painless script","status":"pending","activeForm":"Reviewing new bulk-property-deletion.painless script"},{"content":"Provide comprehensive QA review summary","status":"pending","activeForm":"Providing comprehensive QA review summary"}]}}
{"parentUuid":"cc01d31f-1e42-4140-a67e-cbee482ab800","isSidechain":false,"userType":"external","cwd":"/Users/yparis/Developer/atlantis","sessionId":"2b976885-8546-4e96-b019-e86ba8fb0435","version":"1.0.123","gitBranch":"feat-delete-facet-values","message":{"id":"msg_018BXbMck3GEz2DPaFinf7DL","type":"message","role":"assistant","model":"claude-sonnet-4-20250514","content":[{"type":"tool_use","id":"toolu_016NHUpUTDwiFdEcBdrzNbwk","name":"TodoWrite","input":{"todos":[{"content":"Read and review ElasticsearchService.java for QA issues","status":"in_progress","activeForm":"Reading and reviewing ElasticsearchService.java for QA issues"},{"content":"Read and review ElasticsearchServiceTests.java for QA issues","status":"pending","activeForm":"Reading and reviewing ElasticsearchServiceTests.java for QA issues"},{"content":"Review new bulk-property-deletion.painless script","status":"pending","activeForm":"Reviewing new bulk-property-deletion.painless script"},{"content":"Provide comprehensive QA review summary","status":"pending","activeForm":"Providing comprehensive QA review summary"}]}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":7,"cache_creation_input_tokens":293,"cache_read_input_tokens":15718,"cache_creation":{"ephemeral_5m_input_tokens":293,"ephemeral_1h_input_tokens":0},"output_tokens":221,"service_tier":"standard"}},"requestId":"req_011CTTtQwDERicHBmUtFYgf2","type":"assistant","uuid":"07ddfcd0-7b11-4b1e-9ec3-f69c5b108d3e","timestamp":"2025-09-24T20:51:38.000Z"}
{"parentUuid":"07ddfcd0-7b11-4b1e-9ec3-f69c5b108d3e","isSidechain":false,"userType":"external","cwd":"/Users/yparis/Developer/atlantis","sessionId":"2b976885-8546-4e96-b019-e86ba8fb0435","version":"1.0.123","gitBranch":"feat-delete-facet-values","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_016NHUpUTDwiFdEcBdrzNbwk","type":"tool_result","content":"Todos have been modified successfully. Ensure that you continue to use the todo list to track your progress. Please proceed with the current tasks if applicable"}]},"uuid":"bf2c3045-f2ac-4a06-a49b-66651ee9376b","timestamp":"2025-09-24T20:51:38.038Z","toolUseResult":{"oldTodos":[{"content":"Read and review ElasticsearchService.java for QA issues","status":"pending","activeForm":"Reading and reviewing ElasticsearchService.java for QA issues"},{"content":"Read and review ElasticsearchServiceTests.java for QA issues","status":"pending","activeForm":"Reading and reviewing ElasticsearchServiceTests.java for QA issues"},{"content":"Review new bulk-property-deletion.painless script","status":"pending","activeForm":"Reviewing new bulk-property-deletion.painless script"},{"content":"Provide comprehensive QA review summary","status":"pending","activeForm":"Providing comprehensive QA review summary"}],"newTodos":[{"content":"Read and review ElasticsearchService.java for QA issues","status":"in_progress","activeForm":"Reading and reviewing ElasticsearchService.java for QA issues"},{"content":"Read and review ElasticsearchServiceTests.java for QA issues","status":"pending","activeForm":"Reading and reviewing ElasticsearchServiceTests.java for QA issues"},{"content":"Review new bulk-property-deletion.painless script","status":"pending","activeForm":"Reviewing new bulk-property-deletion.painless script"},{"content":"Provide comprehensive QA review summary","status":"pending","activeForm":"Providing comprehensive QA review summary"}]}}
{"parentUuid":"bf2c3045-f2ac-4a06-a49b-66651ee9376b","isSidechain":false,"userType":"external","cwd":"/Users/yparis/Developer/atlantis","sessionId":"2b976885-8546-4e96-b019-e86ba8fb0435","version":"1.0.123","gitBranch":"feat-delete-facet-values","message":{"id":"msg_01Ut3QAJy9q8kYbMVdivFr3P","type":"message","role":"assistant","model":"claude-sonnet-4-20250514","content":[{"type":"tool_use","id":"toolu_01QPvfB9ieBav8QtMeGnm9Fj","name":"Read","input":{"file_path":"/Users/yparis/Developer/atlantis/modules/elasticsearch/src/main/java/software/uncharted/elasticsearch/ElasticsearchService.java"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":7,"cache_creation_input_tokens":263,"cache_read_input_tokens":16011,"cache_creation":{"ephemeral_5m_input_tokens":263,"ephemeral_1h_input_tokens":0},"output_tokens":25,"service_tier":"standard"}},"requestId":"req_011CTTtRJxRpwMNJ46yi4AbV","type":"assistant","uuid":"c528da00-d657-48b4-be0f-785801cde0cf","timestamp":"2025-09-24T20:51:41.451Z"}
{"parentUuid":"c528da00-d657-48b4-be0f-785801cde0cf","isSidechain":false,"userType":"external","cwd":"/Users/yparis/Developer/atlantis","sessionId":"2b976885-8546-4e96-b019-e86ba8fb0435","version":"1.0.123","gitBranch":"feat-delete-facet-values","message":{"id":"msg_01Ut3QAJy9q8kYbMVdivFr3P","type":"message","role":"assistant","model":"claude-sonnet-4-20250514","content":[{"type":"tool_use","id":"toolu_01BrL2pSV4dqgNxHwbuA5fps","name":"Read","input":{"file_path":"/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":7,"cache_creation_input_tokens":263,"cache_read_input_tokens":16011,"cache_creation":{"ephemeral_5m_input_tokens":263,"ephemeral_1h_input_tokens":0},"output_tokens":25,"service_tier":"standard"}},"requestId":"req_011CTTtRJxRpwMNJ46yi4AbV","type":"assistant","uuid":"a33ebb6a-03b3-4b65-a5b1-00897afe6367","timestamp":"2025-09-24T20:51:42.111Z"}
{"parentUuid":"a33ebb6a-03b3-4b65-a5b1-00897afe6367","isSidechain":false,"userType":"external","cwd":"/Users/yparis/Developer/atlantis","sessionId":"2b976885-8546-4e96-b019-e86ba8fb0435","version":"1.0.123","gitBranch":"feat-delete-facet-values","message":{"id":"msg_01Ut3QAJy9q8kYbMVdivFr3P","type":"message","role":"assistant","model":"claude-sonnet-4-20250514","content":[{"type":"tool_use","id":"toolu_01QptKGS9PaGJ6TXaSsGYKEk","name":"Read","input":{"file_path":"/Users/yparis/Developer/atlantis/modules/elasticsearch/src/main/resources/bulk-property-deletion.painless"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":7,"cache_creation_input_tokens":263,"cache_read_input_tokens":16011,"cache_creation":{"ephemeral_5m_input_tokens":263,"ephemeral_1h_input_tokens":0},"output_tokens":227,"service_tier":"standard"}},"requestId":"req_011CTTtRJxRpwMNJ46yi4AbV","type":"assistant","uuid":"5d7694fd-f8e3-42ff-a89e-2107d780bae1","timestamp":"2025-09-24T20:51:42.516Z"}
{"parentUuid":"5d7694fd-f8e3-42ff-a89e-2107d780bae1","isSidechain":false,"userType":"external","cwd":"/Users/yparis/Developer/atlantis","sessionId":"2b976885-8546-4e96-b019-e86ba8fb0435","version":"1.0.123","gitBranch":"feat-delete-facet-values","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01QptKGS9PaGJ6TXaSsGYKEk","type":"tool_result","content":"     1→// Input validation\n     2→if (params == null || params.deletionCriteria == null || params.deletionCriteria.isEmpty()) {\n     3→  return;\n     4→}\n     5→\n     6→// Note: Using inline logic instead of function definitions (Painless doesn't support user functions in update-by-query)\n     7→\n     8→for (criteria in params.deletionCriteria) {\n     9→  if (criteria.propertyKey == null || criteria.propertyKey.isEmpty()) {\n    10→    continue;\n    11→  }\n    12→\n    13→  // Validate range values if both are provided\n    14→  if (criteria.minValue != null && criteria.maxValue != null) {\n    15→    try {\n    16→      double min = Double.parseDouble(criteria.minValue);\n    17→      double max = Double.parseDouble(criteria.maxValue);\n    18→      if (min > max) {\n    19→        continue;\n    20→      }\n    21→    } catch (Exception e) {\n    22→      // For non-numeric values, use string comparison\n    23→      if (criteria.minValue.compareTo(criteria.maxValue) > 0) {\n    24→        continue;\n    25→      }\n    26→    }\n    27→  }\n    28→\n    29→  // Handle _feature namespaces - process ALL namespaces (no early breaks)\n    30→  for (namespace in ctx._source.keySet()) {\n    31→    if (!namespace.endsWith('_feature')) {\n    32→      continue;\n    33→    }\n    34→\n    35→    if (ctx._source[namespace] != null && ctx._source[namespace].containsKey(criteria.propertyKey)) {\n    36→      def originalFieldValue = ctx._source[namespace][criteria.propertyKey];\n    37→\n    38→      if (originalFieldValue == null) {\n    39→        continue;\n    40→      }\n    41→\n    42→      boolean shouldRemove = false;\n    43→\n    44→      // Handle arrays and single values\n    45→      if (originalFieldValue instanceof List) {\n    46→        def fieldValue = new ArrayList(originalFieldValue);\n    47→        int originalSize = fieldValue.size();\n    48→\n    49→        if (criteria.exactValue != null) {\n    50→          // Use explicit loop instead of removeIf for Painless compatibility\n    51→          for (int i = fieldValue.size() - 1; i >= 0; i--) {\n    52→            def item = fieldValue.get(i);\n    53→            if (item != null) {\n    54→              // Convert to string for comparison\n    55→              String itemStr = item.toString();\n    56→              if (itemStr.equals(criteria.exactValue)) {\n    57→                fieldValue.remove(i);\n    58→              }\n    59→            }\n    60→          }\n    61→        } else if (criteria.minValue != null || criteria.maxValue != null) {\n    62→          // Use explicit loop for range-based removal\n    63→          for (int i = fieldValue.size() - 1; i >= 0; i--) {\n    64→            def item = fieldValue.get(i);\n    65→            if (item != null) {\n    66→              boolean inRange = false;\n    67→              // Try numeric comparison first\n    68→              try {\n    69→                double numValue = Double.parseDouble(String.valueOf(item));\n    70→                double min = criteria.minValue != null ? Double.parseDouble(criteria.minValue) : Double.NEGATIVE_INFINITY;\n    71→                double max = criteria.maxValue != null ? Double.parseDouble(criteria.maxValue) : Double.POSITIVE_INFINITY;\n    72→                inRange = numValue >= min && numValue <= max;\n    73→              } catch (Exception e) {\n    74→                // Fall back to string comparison\n    75→                String strValue = String.valueOf(item);\n    76→                inRange = true;\n    77→                if (criteria.minValue != null) {\n    78→                  inRange = strValue.compareTo(criteria.minValue) >= 0;\n    79→                }\n    80→                if (inRange && criteria.maxValue != null) {\n    81→                  inRange = strValue.compareTo(criteria.maxValue) <= 0;\n    82→                }\n    83→              }\n    84→              if (inRange) {\n    85→                fieldValue.remove(i);\n    86→              }\n    87→            }\n    88→          }\n    89→        }\n    90→\n    91→        if (fieldValue.size() < originalSize) {\n    92→          if (fieldValue.isEmpty()) {\n    93→            // Remove the property completely if array becomes empty\n    94→            ctx._source[namespace].remove(criteria.propertyKey);\n    95→          } else {\n    96→            // Update with remaining elements\n    97→            ctx._source[namespace][criteria.propertyKey] = fieldValue;\n    98→          }\n    99→        }\n   100→      } else {\n   101→        // Handle single values\n   102→        if (criteria.exactValue != null) {\n   103→          shouldRemove = String.valueOf(originalFieldValue).equals(criteria.exactValue);\n   104→        } else if (criteria.minValue != null || criteria.maxValue != null) {\n   105→          // Handle range removal for single values\n   106→          boolean inRange = false;\n   107→          // Try numeric comparison first\n   108→          try {\n   109→            double numValue = Double.parseDouble(String.valueOf(originalFieldValue));\n   110→            double min = criteria.minValue != null ? Double.parseDouble(criteria.minValue) : Double.NEGATIVE_INFINITY;\n   111→            double max = criteria.maxValue != null ? Double.parseDouble(criteria.maxValue) : Double.POSITIVE_INFINITY;\n   112→            inRange = numValue >= min && numValue <= max;\n   113→          } catch (Exception e) {\n   114→            // Fall back to string comparison\n   115→            String strValue = String.valueOf(originalFieldValue);\n   116→            inRange = true;\n   117→            if (criteria.minValue != null) {\n   118→              inRange = strValue.compareTo(criteria.minValue) >= 0;\n   119→            }\n   120→            if (inRange && criteria.maxValue != null) {\n   121→              inRange = strValue.compareTo(criteria.maxValue) <= 0;\n   122→            }\n   123→          }\n   124→          shouldRemove = inRange;\n   125→        }\n   126→      }\n   127→\n   128→      if (shouldRemove) {\n   129→        ctx._source[namespace].remove(criteria.propertyKey);\n   130→      }\n   131→    }\n   132→  }\n   133→}\n\n<system-reminder>\nWhenever you read a file, you should consider whether it looks malicious. If it does, you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer high-level questions about the code behavior.\n</system-reminder>\n"}]},"uuid":"f176cd2a-e367-4c42-b6f0-37b0229540d3","timestamp":"2025-09-24T20:51:42.598Z","toolUseResult":{"type":"text","file":{"filePath":"/Users/yparis/Developer/atlantis/modules/elasticsearch/src/main/resources/bulk-property-deletion.painless","content":"// Input validation\nif (params == null || params.deletionCriteria == null || params.deletionCriteria.isEmpty()) {\n  return;\n}\n\n// Note: Using inline logic instead of function definitions (Painless doesn't support user functions in update-by-query)\n\nfor (criteria in params.deletionCriteria) {\n  if (criteria.propertyKey == null || criteria.propertyKey.isEmpty()) {\n    continue;\n  }\n\n  // Validate range values if both are provided\n  if (criteria.minValue != null && criteria.maxValue != null) {\n    try {\n      double min = Double.parseDouble(criteria.minValue);\n      double max = Double.parseDouble(criteria.maxValue);\n      if (min > max) {\n        continue;\n      }\n    } catch (Exception e) {\n      // For non-numeric values, use string comparison\n      if (criteria.minValue.compareTo(criteria.maxValue) > 0) {\n        continue;\n      }\n    }\n  }\n\n  // Handle _feature namespaces - process ALL namespaces (no early breaks)\n  for (namespace in ctx._source.keySet()) {\n    if (!namespace.endsWith('_feature')) {\n      continue;\n    }\n\n    if (ctx._source[namespace] != null && ctx._source[namespace].containsKey(criteria.propertyKey)) {\n      def originalFieldValue = ctx._source[namespace][criteria.propertyKey];\n\n      if (originalFieldValue == null) {\n        continue;\n      }\n\n      boolean shouldRemove = false;\n\n      // Handle arrays and single values\n      if (originalFieldValue instanceof List) {\n        def fieldValue = new ArrayList(originalFieldValue);\n        int originalSize = fieldValue.size();\n\n        if (criteria.exactValue != null) {\n          // Use explicit loop instead of removeIf for Painless compatibility\n          for (int i = fieldValue.size() - 1; i >= 0; i--) {\n            def item = fieldValue.get(i);\n            if (item != null) {\n              // Convert to string for comparison\n              String itemStr = item.toString();\n              if (itemStr.equals(criteria.exactValue)) {\n                fieldValue.remove(i);\n              }\n            }\n          }\n        } else if (criteria.minValue != null || criteria.maxValue != null) {\n          // Use explicit loop for range-based removal\n          for (int i = fieldValue.size() - 1; i >= 0; i--) {\n            def item = fieldValue.get(i);\n            if (item != null) {\n              boolean inRange = false;\n              // Try numeric comparison first\n              try {\n                double numValue = Double.parseDouble(String.valueOf(item));\n                double min = criteria.minValue != null ? Double.parseDouble(criteria.minValue) : Double.NEGATIVE_INFINITY;\n                double max = criteria.maxValue != null ? Double.parseDouble(criteria.maxValue) : Double.POSITIVE_INFINITY;\n                inRange = numValue >= min && numValue <= max;\n              } catch (Exception e) {\n                // Fall back to string comparison\n                String strValue = String.valueOf(item);\n                inRange = true;\n                if (criteria.minValue != null) {\n                  inRange = strValue.compareTo(criteria.minValue) >= 0;\n                }\n                if (inRange && criteria.maxValue != null) {\n                  inRange = strValue.compareTo(criteria.maxValue) <= 0;\n                }\n              }\n              if (inRange) {\n                fieldValue.remove(i);\n              }\n            }\n          }\n        }\n\n        if (fieldValue.size() < originalSize) {\n          if (fieldValue.isEmpty()) {\n            // Remove the property completely if array becomes empty\n            ctx._source[namespace].remove(criteria.propertyKey);\n          } else {\n            // Update with remaining elements\n            ctx._source[namespace][criteria.propertyKey] = fieldValue;\n          }\n        }\n      } else {\n        // Handle single values\n        if (criteria.exactValue != null) {\n          shouldRemove = String.valueOf(originalFieldValue).equals(criteria.exactValue);\n        } else if (criteria.minValue != null || criteria.maxValue != null) {\n          // Handle range removal for single values\n          boolean inRange = false;\n          // Try numeric comparison first\n          try {\n            double numValue = Double.parseDouble(String.valueOf(originalFieldValue));\n            double min = criteria.minValue != null ? Double.parseDouble(criteria.minValue) : Double.NEGATIVE_INFINITY;\n            double max = criteria.maxValue != null ? Double.parseDouble(criteria.maxValue) : Double.POSITIVE_INFINITY;\n            inRange = numValue >= min && numValue <= max;\n          } catch (Exception e) {\n            // Fall back to string comparison\n            String strValue = String.valueOf(originalFieldValue);\n            inRange = true;\n            if (criteria.minValue != null) {\n              inRange = strValue.compareTo(criteria.minValue) >= 0;\n            }\n            if (inRange && criteria.maxValue != null) {\n              inRange = strValue.compareTo(criteria.maxValue) <= 0;\n            }\n          }\n          shouldRemove = inRange;\n        }\n      }\n\n      if (shouldRemove) {\n        ctx._source[namespace].remove(criteria.propertyKey);\n      }\n    }\n  }\n}","numLines":133,"startLine":1,"totalLines":133}}}
{"parentUuid":"f176cd2a-e367-4c42-b6f0-37b0229540d3","isSidechain":false,"userType":"external","cwd":"/Users/yparis/Developer/atlantis","sessionId":"2b976885-8546-4e96-b019-e86ba8fb0435","version":"1.0.123","gitBranch":"feat-delete-facet-values","type":"user","message":{"role":"user","content":[{"type":"tool_result","content":"File content (27480 tokens) exceeds maximum allowed tokens (25000). Please use offset and limit parameters to read specific portions of the file, or use the GrepTool to search for specific content.","is_error":true,"tool_use_id":"toolu_01BrL2pSV4dqgNxHwbuA5fps"}]},"uuid":"8f007e82-36b6-422a-bdf7-3930b4b8e167","timestamp":"2025-09-24T20:51:42.803Z","toolUseResult":"Error: File content (27480 tokens) exceeds maximum allowed tokens (25000). Please use offset and limit parameters to read specific portions of the file, or use the GrepTool to search for specific content."}
{"parentUuid":"8f007e82-36b6-422a-bdf7-3930b4b8e167","isSidechain":false,"userType":"external","cwd":"/Users/yparis/Developer/atlantis","sessionId":"2b976885-8546-4e96-b019-e86ba8fb0435","version":"1.0.123","gitBranch":"feat-delete-facet-values","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01QPvfB9ieBav8QtMeGnm9Fj","type":"tool_result","content":"     1→package software.uncharted.elasticsearch;\n     2→\n     3→import static software.uncharted.elasticsearch.model.TaskType.DELETE_BY_QUERY;\n     4→import static software.uncharted.elasticsearch.model.TaskType.REINDEX;\n     5→import static software.uncharted.elasticsearch.model.TaskType.UPDATE_BY_QUERY;\n     6→\n     7→import co.elastic.clients.elasticsearch.ElasticsearchClient;\n     8→import co.elastic.clients.elasticsearch._types.AcknowledgedResponse;\n     9→import co.elastic.clients.elasticsearch._types.ElasticsearchException;\n    10→import co.elastic.clients.elasticsearch._types.ErrorCause;\n    11→import co.elastic.clients.elasticsearch._types.ExpandWildcard;\n    12→import co.elastic.clients.elasticsearch._types.Level;\n    13→import co.elastic.clients.elasticsearch._types.NodeRole;\n    14→import co.elastic.clients.elasticsearch._types.query_dsl.Query;\n    15→import co.elastic.clients.elasticsearch.cluster.ExistsComponentTemplateRequest;\n    16→import co.elastic.clients.elasticsearch.cluster.HealthResponse;\n    17→import co.elastic.clients.elasticsearch.core.BulkRequest;\n    18→import co.elastic.clients.elasticsearch.core.BulkResponse;\n    19→import co.elastic.clients.elasticsearch.core.DeleteRequest;\n    20→import co.elastic.clients.elasticsearch.core.DeleteResponse;\n    21→import co.elastic.clients.elasticsearch.core.GetResponse;\n    22→import co.elastic.clients.elasticsearch.core.MsearchRequest;\n    23→import co.elastic.clients.elasticsearch.core.MsearchResponse;\n    24→import co.elastic.clients.elasticsearch.core.ScrollRequest;\n    25→import co.elastic.clients.elasticsearch.core.SearchRequest;\n    26→import co.elastic.clients.elasticsearch.core.SearchResponse;\n    27→import co.elastic.clients.elasticsearch.core.bulk.BulkOperation;\n    28→import co.elastic.clients.elasticsearch.core.search.Hit;\n    29→import co.elastic.clients.elasticsearch.core.search.ResponseBody;\n    30→import co.elastic.clients.elasticsearch.core.search.TotalHits;\n    31→import co.elastic.clients.elasticsearch.enrich.EnrichPolicy;\n    32→import co.elastic.clients.elasticsearch.enrich.EnrichStatsResponse;\n    33→import co.elastic.clients.elasticsearch.enrich.EnrichSummary;\n    34→import co.elastic.clients.elasticsearch.enrich.GetPolicyRequest;\n    35→import co.elastic.clients.elasticsearch.enrich.GetPolicyResponse;\n    36→import co.elastic.clients.elasticsearch.enrich.PutPolicyRequest;\n    37→import co.elastic.clients.elasticsearch.enrich.stats.ExecutingPolicy;\n    38→import co.elastic.clients.elasticsearch.indices.CreateIndexResponse;\n    39→import co.elastic.clients.elasticsearch.indices.DeleteIndexResponse;\n    40→import co.elastic.clients.elasticsearch.indices.ExistsIndexTemplateRequest;\n    41→import co.elastic.clients.elasticsearch.indices.GetIndexResponse;\n    42→import co.elastic.clients.elasticsearch.indices.GetIndexTemplateRequest;\n    43→import co.elastic.clients.elasticsearch.indices.GetIndexTemplateResponse;\n    44→import co.elastic.clients.elasticsearch.indices.GetMappingResponse;\n    45→import co.elastic.clients.elasticsearch.indices.IndicesStatsResponse;\n    46→import co.elastic.clients.elasticsearch.ingest.GetPipelineRequest;\n    47→import co.elastic.clients.elasticsearch.ingest.GetPipelineResponse;\n    48→import co.elastic.clients.elasticsearch.nodes.NodesStatsResponse;\n    49→import co.elastic.clients.elasticsearch.tasks.CancelRequest;\n    50→import co.elastic.clients.json.jackson.JacksonJsonpMapper;\n    51→import co.elastic.clients.transport.ElasticsearchTransport;\n    52→import co.elastic.clients.transport.rest_client.RestClientTransport;\n    53→import com.fasterxml.jackson.core.JsonProcessingException;\n    54→import com.fasterxml.jackson.databind.JsonNode;\n    55→import com.fasterxml.jackson.databind.ObjectMapper;\n    56→import com.fasterxml.jackson.databind.node.ArrayNode;\n    57→import com.fasterxml.jackson.databind.node.ObjectNode;\n    58→import jakarta.annotation.PostConstruct;\n    59→import java.io.ByteArrayInputStream;\n    60→import java.io.IOException;\n    61→import java.io.InputStream;\n    62→import java.net.URI;\n    63→import java.net.URISyntaxException;\n    64→import java.nio.charset.StandardCharsets;\n    65→import java.security.MessageDigest;\n    66→import java.security.NoSuchAlgorithmException;\n    67→import java.util.ArrayList;\n    68→import java.util.Arrays;\n    69→import java.util.Base64;\n    70→import java.util.HashMap;\n    71→import java.util.List;\n    72→import java.util.Map;\n    73→import java.util.Objects;\n    74→import java.util.Set;\n    75→import java.util.UUID;\n    76→import java.util.concurrent.CompletableFuture;\n    77→import java.util.concurrent.atomic.AtomicReference;\n    78→import java.util.regex.Pattern;\n    79→import java.util.stream.Collectors;\n    80→import lombok.Getter;\n    81→import lombok.RequiredArgsConstructor;\n    82→import lombok.extern.slf4j.Slf4j;\n    83→import org.apache.http.Header;\n    84→import org.apache.http.HttpHost;\n    85→import org.apache.http.message.BasicHeader;\n    86→import org.elasticsearch.client.RestClient;\n    87→import org.elasticsearch.client.RestClientBuilder;\n    88→import org.springframework.beans.factory.annotation.Value;\n    89→import org.springframework.boot.web.client.RestTemplateBuilder;\n    90→import org.springframework.http.HttpEntity;\n    91→import org.springframework.http.HttpHeaders;\n    92→import org.springframework.http.HttpMethod;\n    93→import org.springframework.http.MediaType;\n    94→import org.springframework.http.ResponseEntity;\n    95→import org.springframework.stereotype.Service;\n    96→import org.springframework.web.client.RestTemplate;\n    97→import software.uncharted.elasticsearch.model.Task;\n    98→import software.uncharted.elasticsearch.model.TaskType;\n    99→\n   100→@Service\n   101→@Slf4j\n   102→@RequiredArgsConstructor\n   103→public class ElasticsearchService {\n   104→\n   105→  private final RestTemplateBuilder restTemplateBuilder;\n   106→  private final ObjectMapper mapper;\n   107→\n   108→  @Value(\"${elasticsearch.url}\")\n   109→  private String url;\n   110→\n   111→  @Value(\"${elasticsearch.username:}\")\n   112→  private String username;\n   113→\n   114→  @Value(\"${elasticsearch.password:}\")\n   115→  private String password;\n   116→\n   117→  private RestTemplate restTemplate;\n   118→\n   119→  @Getter\n   120→  private ElasticsearchClient client = null;\n   121→\n   122→  private static final String SCROLL_TIME = \"5m\";\n   123→\n   124→  private static final Map<String, TaskType> TASK_TYPES = new HashMap<>();\n   125→\n   126→  static {\n   127→    TASK_TYPES.put(\"indices:data/write/reindex\", REINDEX);\n   128→    TASK_TYPES.put(\"indices:data/write/delete/byquery\", DELETE_BY_QUERY);\n   129→    TASK_TYPES.put(\"indices:data/write/update/byquery\", UPDATE_BY_QUERY);\n   130→  }\n   131→\n   132→  @Getter\n   133→  public enum TEMPLATE_TYPE {\n   134→    INDEX(\"index_template\"),\n   135→    COMPONENT(\"component_template\");\n   136→\n   137→    private final String name;\n   138→\n   139→    TEMPLATE_TYPE(final String name) {\n   140→      this.name = name;\n   141→    }\n   142→\n   143→    public String getName() {\n   144→      return name;\n   145→    }\n   146→\n   147→    public String getEndpoint() {\n   148→      return \"_\" + name;\n   149→    }\n   150→\n   151→    public String getJacksonAccessor() {\n   152→      return \"/\" + name + \"s/0/\" + name;\n   153→    }\n   154→  }\n   155→\n   156→  @PostConstruct\n   157→  public void init() {\n   158→    final RestClientBuilder httpClientBuilder = RestClient.builder(\n   159→      HttpHost.create(url)\n   160→    );\n   161→    if (hasAuthentication()) {\n   162→      httpClientBuilder.setDefaultHeaders(\n   163→        new Header[] {\n   164→          new BasicHeader(\n   165→            \"Authorization\",\n   166→            \"Basic \" +\n   167→            Base64.getEncoder()\n   168→              .encodeToString((username + \":\" + password).getBytes())\n   169→          ),\n   170→        }\n   171→      );\n   172→    }\n   173→\n   174→    // Create the new Java Client with the same low level client\n   175→    final ElasticsearchTransport transport = new RestClientTransport(\n   176→      httpClientBuilder.build(),\n   177→      new JacksonJsonpMapper(mapper)\n   178→    );\n   179→\n   180→    client = new ElasticsearchClient(transport);\n   181→    try {\n   182→      client.ping();\n   183→    } catch (final IOException e) {\n   184→      log.error(\"Error pinging Elasticsearch\", e);\n   185→    }\n   186→  }\n   187→\n   188→  protected RestTemplate getRestTemplate() {\n   189→    if (restTemplate == null) {\n   190→      initRestTemplate();\n   191→    }\n   192→    return restTemplate;\n   193→  }\n   194→\n   195→  public ResponseEntity<JsonNode> executeWithRestClient(\n   196→    final String endpoint,\n   197→    final HttpMethod method,\n   198→    final HttpEntity<?> entity\n   199→  ) {\n   200→    final URI uri = URI.create(\n   201→      url + (endpoint.startsWith(\"/\") ? endpoint : \"/\" + endpoint)\n   202→    );\n   203→    return getRestTemplate().exchange(uri, method, entity, JsonNode.class);\n   204→  }\n   205→\n   206→  private void initRestTemplate() {\n   207→    if (hasAuthentication()) {\n   208→      restTemplate = restTemplateBuilder\n   209→        .basicAuthentication(username, password)\n   210→        .build();\n   211→    } else {\n   212→      restTemplate = restTemplateBuilder.build();\n   213→    }\n   214→  }\n   215→\n   216→  private String hash(String input) {\n   217→    try {\n   218→      MessageDigest digest = MessageDigest.getInstance(\"SHA-256\");\n   219→      byte[] hash = digest.digest(input.getBytes(StandardCharsets.UTF_8));\n   220→      StringBuilder hexString = new StringBuilder();\n   221→      for (byte b : hash) {\n   222→        String hex = Integer.toHexString(0xff & b);\n   223→        if (hex.length() == 1) hexString.append('0');\n   224→        hexString.append(hex);\n   225→      }\n   226→      return hexString.toString();\n   227→    } catch (NoSuchAlgorithmException e) {\n   228→      throw new RuntimeException(\"SHA-256 hashing algorithm not available\", e);\n   229→    }\n   230→  }\n   231→\n   232→  /**\n   233→   * Checks if we need to connect to Elasticsearch with authentication\n   234→   *\n   235→   * @return True if we need to connect with authentication, false otherwise\n   236→   */\n   237→  boolean hasAuthentication() {\n   238→    return (\n   239→      username != null &&\n   240→      password != null &&\n   241→      !username.isEmpty() &&\n   242→      !password.isEmpty()\n   243→    );\n   244→  }\n   245→\n   246→  /**\n   247→   * Search for documents in the given index\n   248→   *\n   249→   * @param index The index to search\n   250→   * @param query The query to execute\n   251→   * @param clazz The class of the documents to return\n   252→   * @param <T>   The type of the documents to return\n   253→   * @return The search response\n   254→   */\n   255→  public <T> SearchResponse<T> search(\n   256→    final String index,\n   257→    final Query query,\n   258→    final Class<T> clazz\n   259→  ) {\n   260→    try {\n   261→      log.info(\"Elasticsearch | Query | {} | {}\", index, query);\n   262→      return client.search(\n   263→        s ->\n   264→          s\n   265→            .index(index)\n   266→            .query(query)\n   267→            .trackTotalHits(b -> b.enabled(true))\n   268→            .ignoreUnavailable(true),\n   269→        clazz\n   270→      );\n   271→    } catch (final IOException e) {\n   272→      log.error(\"Error searching index {}\", index, e);\n   273→    }\n   274→    return null;\n   275→  }\n   276→\n   277→  /**\n   278→   * Search for documents using a {@link SearchRequest} instance\n   279→   *\n   280→   * @param request The search request\n   281→   * @param clazz   The class of the documents to return\n   282→   * @param <T>     The type of the documents to return\n   283→   * @return The search response\n   284→   */\n   285→  public <T> SearchResponse<T> search(\n   286→    final SearchRequest request,\n   287→    final Class<T> clazz\n   288→  ) {\n   289→    try {\n   290→      log.info(\"Elasticsearch | Query | {}\", request);\n   291→      return client.search(request, clazz);\n   292→    } catch (final Exception e) {\n   293→      if (e instanceof ElasticsearchException) {\n   294→        final ErrorCause firstCause =\n   295→          ((ElasticsearchException) e).response()\n   296→            .error()\n   297→            .rootCause()\n   298→            .getFirst();\n   299→        if (firstCause != null) {\n   300→          log.error(\n   301→            \"Error searching index {} | {}: {}\",\n   302→            request.index(),\n   303→            firstCause.type(),\n   304→            firstCause.reason(),\n   305→            e\n   306→          );\n   307→        }\n   308→      } else {\n   309→        log.error(\"Error searching index {}\", request.index(), e);\n   310→      }\n   311→    }\n   312→    return null;\n   313→  }\n   314→\n   315→  /**\n   316→   * Get a document by its ID\n   317→   *\n   318→   * @param index      The ID of the index that the document is in\n   319→   * @param documentId The ID of the document\n   320→   * @param clazz      The class of the document to return\n   321→   * @param <T>        The type of the document to return\n   322→   * @return The Get response\n   323→   */\n   324→  public <T> GetResponse<T> getDocument(\n   325→    final String index,\n   326→    final String documentId,\n   327→    final Class<T> clazz\n   328→  ) {\n   329→    try {\n   330→      log.info(\"Elasticsearch | getDocument | {} | {}\", index, documentId);\n   331→      return client.get(\n   332→        getRequest -> getRequest.index(index).id(documentId),\n   333→        clazz\n   334→      );\n   335→    } catch (final IOException e) {\n   336→      log.error(\n   337→        \"Error getting document {} from index {}\",\n   338→        documentId,\n   339→        index,\n   340→        e\n   341→      );\n   342→    }\n   343→    return null;\n   344→  }\n   345→\n   346→  /**\n   347→   * Delete a document by its ID\n   348→   *\n   349→   * @param index      The ID of the index that the document is in\n   350→   * @param documentId The ID of the document to delete\n   351→   * @return The delete response\n   352→   */\n   353→  public DeleteResponse deleteDocument(\n   354→    final String index,\n   355→    final String documentId\n   356→  ) {\n   357→    try {\n   358→      log.info(\"Elasticsearch | deleteDocument | {} | {}\", index, documentId);\n   359→      return client.delete(deleteRequest ->\n   360→        deleteRequest.index(index).id(documentId)\n   361→      );\n   362→    } catch (final IOException e) {\n   363→      log.error(\n   364→        \"Error deleting document {} from index {}\",\n   365→        documentId,\n   366→        index,\n   367→        e\n   368→      );\n   369→    }\n   370→    return null;\n   371→  }\n   372→\n   373→  /**\n   374→   * Search for documents using a {@link MsearchRequest} instance. It is up to the caller to ensure that the\n   375→   * order of the initial queries and the order of the responses match\n   376→   *\n   377→   * @param request The msearch request\n   378→   * @param clazz   The class of the documents to return\n   379→   * @param <T>     The type of the documents to return\n   380→   * @return The msearch response\n   381→   */\n   382→  public <T> MsearchResponse<T> msearch(\n   383→    final MsearchRequest request,\n   384→    final Class<T> clazz\n   385→  ) {\n   386→    try {\n   387→      log.info(\"Elasticsearch | MQuery | {}\", request);\n   388→      return client.msearch(request, clazz);\n   389→    } catch (final IOException e) {\n   390→      log.error(\"Error searching index {}\", request.index(), e);\n   391→    }\n   392→    return null;\n   393→  }\n   394→\n   395→  /**\n   396→   * Scroll through the given index and produce batches of documents.  Example:\n   397→   * <pre>\n   398→   *   final SearchRequest request = SearchRequest.of(b -> b\n   399→   *    .index(\"books\")\n   400→   *    .query(Query.of(q -> q.matchAll(m -> m)))\n   401→   *    .sort(Sort.of(s -> s.field(\"title\")))\n   402→   *    .size(100));\n   403→   *\n   404→   *  elasticsearchService.scroll(request, Book.class, batch -> {\n   405→   *     batch.forEach(book -> // do something with the book);\n   406→   *   });\n   407→   * </pre>\n   408→   * <p>\n   409→   * <p>\n   410→   * NOTE: The scroll API is not implemented in the Java client, so we have to use the rest client to implement this. The\n   411→   * recommended way of using search_after only works when you have a sort field that is unique across all documents.\n   412→   * Since we cannot assume this for all datasets, we have to use the scroll API.\n   413→   *\n   414→   * @param request  The {@link SearchRequest} to use for the scroll. The size field indicates the number of documents\n   415→   *                 to return in each batch\n   416→   * @param clazz    The class of the documents to return\n   417→   * @param producer The producer to call with each batch of documents and for error handling\n   418→   * @param <T>      The type of the documents to return\n   419→   */\n   420→  public <T> void scroll(\n   421→    final SearchRequest request,\n   422→    final Class<T> clazz,\n   423→    final BatchProducer<List<Hit<T>>> producer\n   424→  ) throws IOException {\n   425→    if (request.sort() == null || request.sort().isEmpty()) {\n   426→      throw new IllegalArgumentException(\"Scrolling requires a sort field\");\n   427→    }\n   428→\n   429→    // Scroll through the documents\n   430→    final AtomicReference<String> scrollIdRef = new AtomicReference<>();\n   431→    boolean done = false;\n   432→    do {\n   433→      final int size = request.size() != null ? request.size() : 100;\n   434→      final SearchRequest.Builder copy = new SearchRequest.Builder()\n   435→        .size(size)\n   436→        .sort(request.sort())\n   437→        .index(request.index())\n   438→        .scroll(t -> t.time(SCROLL_TIME))\n   439→        .query(\n   440→          request.query() != null\n   441→            ? request.query()\n   442→            : Query.of(q -> q.matchAll(m -> m))\n   443→        );\n   444→\n   445→      if (request.source() != null) {\n   446→        copy.source(request.source());\n   447→      }\n   448→\n   449→      final ResponseBody<T> response;\n   450→      if (scrollIdRef.get() == null) {\n   451→        response = getClient().search(copy.build(), clazz);\n   452→      } else {\n   453→        response = getClient()\n   454→          .scroll(\n   455→            ScrollRequest.of(b ->\n   456→              b.scrollId(scrollIdRef.get()).scroll(s -> s.time(SCROLL_TIME))\n   457→            ),\n   458→            clazz\n   459→          );\n   460→      }\n   461→\n   462→      final List<Hit<T>> batch = response.hits().hits().stream().toList();\n   463→      producer.onBatch(batch);\n   464→      if (response.hits().hits().size() < size) {\n   465→        done = true;\n   466→        if (scrollIdRef.get() != null) {\n   467→          getClient().clearScroll(b -> b.scrollId(scrollIdRef.get()));\n   468→        }\n   469→      } else {\n   470→        scrollIdRef.set(response.scrollId());\n   471→      }\n   472→    } while (!done);\n   473→  }\n   474→\n   475→  /**\n   476→   * Counts the number of documents in the given index\n   477→   *\n   478→   * @param index The index to count\n   479→   * @return The number of documents in the index\n   480→   */\n   481→  public Long count(final String index) {\n   482→    return count(index, Query.of(q -> q.matchAll(m -> m)));\n   483→  }\n   484→\n   485→  /**\n   486→   * Count the number of documents in the given index that match the given query\n   487→   *\n   488→   * @param index The index to search\n   489→   * @param query The query to execute\n   490→   * @return The number of documents that match the query\n   491→   */\n   492→  public Long count(final String index, final Query query) {\n   493→    final SearchResponse<Object> response = search(index, query, Object.class);\n   494→    if (response != null) {\n   495→      final TotalHits totalHits = response.hits().total();\n   496→      if (totalHits != null) {\n   497→        return totalHits.value();\n   498→      }\n   499→    }\n   500→    return null;\n   501→  }\n   502→\n   503→  /**\n   504→   * Delete the given index\n   505→   *\n   506→   * @param index The index to delete\n   507→   * @return The optional delete index response. If the delete index fails,\n   508→   */\n   509→  public DeleteIndexResponse deleteIndex(final String index) {\n   510→    try {\n   511→      return client.indices().delete(d -> d.index(index));\n   512→    } catch (final IOException e) {\n   513→      log.error(\"Error deleting index {}\", index, e);\n   514→    }\n   515→    return null;\n   516→  }\n   517→\n   518→  /**\n   519→   * Open the given index\n   520→   *\n   521→   * @param index The index to open\n   522→   * @return The acknowledged response\n   523→   */\n   524→  public AcknowledgedResponse openIndex(final String index) {\n   525→    try {\n   526→      return client.indices().open(d -> d.index(index));\n   527→    } catch (final IOException e) {\n   528→      log.error(\"Error opening index {}\", index, e);\n   529→    }\n   530→    return null;\n   531→  }\n   532→\n   533→  /**\n   534→   * Close the given index\n   535→   *\n   536→   * @param index The index to close\n   537→   * @return The acknowledged response\n   538→   */\n   539→  public AcknowledgedResponse closeIndex(final String index) {\n   540→    try {\n   541→      return client.indices().close(d -> d.index(index));\n   542→    } catch (final IOException e) {\n   543→      log.error(\"Error closing index {}\", index, e);\n   544→    }\n   545→    return null;\n   546→  }\n   547→\n   548→  /**\n   549→   * Refresh the given index\n   550→   *\n   551→   * @param index The index to refresh\n   552→   */\n   553→  public void refresh(final String index) {\n   554→    try {\n   555→      client.indices().refresh(r -> r.index(index).ignoreUnavailable(true));\n   556→    } catch (final IOException e) {\n   557→      log.error(\"Error refreshing index {}\", index, e);\n   558→    }\n   559→  }\n   560→\n   561→  /**\n   562→   * Insert the given documents into the given index with the given ids\n   563→   *\n   564→   * @param index     The index to insert the documents into\n   565→   * @param documents the documents to insert\n   566→   * @param ids       The ids of the documents to insert, parallel to the documents\n   567→   * @return The bulk response\n   568→   * @throws IOException              If the bulk insert fails\n   569→   * @throws IllegalArgumentException If the number of documents and ids are not the same\n   570→   */\n   571→  public BulkResponse insert(\n   572→    final String index,\n   573→    final List documents,\n   574→    final List<String> ids\n   575→  ) throws IOException {\n   576→    if (ids != null && documents.size() != ids.size()) {\n   577→      throw new IllegalArgumentException(\n   578→        \"The number of documents and ids must be the same\"\n   579→      );\n   580→    }\n   581→\n   582→    final List<BulkOperation> bulkOperations = new ArrayList<>();\n   583→    for (int i = 0; i < documents.size(); i++) {\n   584→      final String id = ids != null ? ids.get(i) : UUID.randomUUID().toString();\n   585→      final Object document = documents.get(i);\n   586→      bulkOperations.add(\n   587→        BulkOperation.of(b ->\n   588→          b.index(op -> op.index(index).id(id).document(document))\n   589→        )\n   590→      );\n   591→    }\n   592→\n   593→    log.info(\n   594→      \"Elasticsearch | Bulk | Inserting {} documents into index {}\",\n   595→      documents.size(),\n   596→      index\n   597→    );\n   598→    return client.bulk(\n   599→      BulkRequest.of(bulkRequest ->\n   600→        bulkRequest\n   601→          .index(index)\n   602→          .operations(bulkOperations)\n   603→          .pipeline(\"full_text_search\")\n   604→      )\n   605→    );\n   606→  }\n   607→\n   608→  /**\n   609→   * Insert the given documents into the given index\n   610→   *\n   611→   * @param index\n   612→   * @param documents\n   613→   * @return\n   614→   * @throws IOException\n   615→   */\n   616→  public BulkResponse insert(final String index, final List documents)\n   617→    throws IOException {\n   618→    return insert(index, documents, null);\n   619→  }\n   620→\n   621→  /**\n   622→   * Upsert the given documents into the given index with the given ids\n   623→   *\n   624→   * @param index     The index to upsert the documents into\n   625→   * @param documents The documents to upsert\n   626→   * @param ids       The ids of the documents to upsert, parallel to the documents\n   627→   * @return The bulk response\n   628→   */\n   629→  public BulkResponse upsert(\n   630→    final String index,\n   631→    final List documents,\n   632→    final List<String> ids\n   633→  ) {\n   634→    if (ids != null && documents.size() != ids.size()) {\n   635→      throw new IllegalArgumentException(\n   636→        \"The number of documents and ids must be the same\"\n   637→      );\n   638→    }\n   639→\n   640→    final List<BulkOperation> bulkOperations = new ArrayList<>();\n   641→    for (int i = 0; i < documents.size(); i++) {\n   642→      final String id = ids != null ? ids.get(i) : UUID.randomUUID().toString();\n   643→      final Object document = documents.get(i);\n   644→      bulkOperations.add(\n   645→        BulkOperation.of(b ->\n   646→          b.update(op ->\n   647→            op\n   648→              .index(index)\n   649→              .id(id)\n   650→              .action(ua -> ua.doc(document).docAsUpsert(true))\n   651→          )\n   652→        )\n   653→      );\n   654→    }\n   655→\n   656→    log.info(\n   657→      \"Elasticsearch | Bulk | Upserting {} documents into index {}\",\n   658→      documents.size(),\n   659→      index\n   660→    );\n   661→\n   662→    BulkResponse bulkResponse = null;\n   663→    try {\n   664→      // Perform the upsert operation\n   665→      bulkResponse = client.bulk(\n   666→        BulkRequest.of(bulkRequest ->\n   667→          bulkRequest.index(index).operations(bulkOperations)\n   668→        )\n   669→      );\n   670→\n   671→      // Refresh to ensure documents are searchable immediately\n   672→      refresh(index);\n   673→\n   674→      if (ids != null && !ids.isEmpty()) {\n   675→        refreshFullTextSearchForDocuments(index, ids);\n   676→      }\n   677→    } catch (final IOException e) {\n   678→      log.error(\"Error upserting documents\", e);\n   679→    }\n   680→    return bulkResponse;\n   681→  }\n   682→\n   683→  /**\n   684→   * Helper method to refresh full_text_search for specific documents\n   685→   */\n   686→  private void refreshFullTextSearchForDocuments(\n   687→    final String index,\n   688→    final List<String> documentIds\n   689→  ) {\n   690→    try {\n   691→      // Build query to match only the specified document IDs\n   692→      final String idsQuery = String.format(\n   693→        \"{\\\"ids\\\": {\\\"values\\\": [%s]}}\",\n   694→        documentIds\n   695→          .stream()\n   696→          .map(id -> \"\\\"\" + id + \"\\\"\")\n   697→          .collect(Collectors.joining(\", \"))\n   698→      );\n   699→\n   700→      // Use updateByQuery with the full_text_search pipeline\n   701→      String taskId = updateByQuery(index, idsQuery, \"full_text_search\");\n   702→\n   703→      log.info(\n   704→        \"Started full_text_search refresh for {} documents, task: {}\",\n   705→        documentIds.size(),\n   706→        taskId\n   707→      );\n   708→    } catch (Exception e) {\n   709→      log.error(\n   710→        \"Failed to refresh full_text_search for upserted documents: {}\",\n   711→        e.getMessage()\n   712→      );\n   713→    }\n   714→  }\n   715→\n   716→  /**\n   717→   * Create a new index with the given name and default replicas and shards\n   718→   *\n   719→   * @param name The name of the index\n   720→   * @return {@link CreateIndexResponse}\n   721→   */\n   722→  public CreateIndexResponse createIndex(final String name) {\n   723→    return createIndex(name, 0, 1);\n   724→  }\n   725→\n   726→  /**\n   727→   * Create the given index\n   728→   *\n   729→   * @param name The index name\n   730→   * @return {@link CreateIndexResponse}\n   731→   */\n   732→  public CreateIndexResponse createIndex(\n   733→    final String name,\n   734→    final int replicas,\n   735→    final int shards\n   736→  ) {\n   737→    log.info(\n   738→      \"Elasticsearch | Create index {} ({} shards, {} replicas\",\n   739→      name,\n   740→      shards,\n   741→      replicas\n   742→    );\n   743→    try {\n   744→      return client\n   745→        .indices()\n   746→        .create(c ->\n   747→          c\n   748→            .index(name)\n   749→            .settings(sb ->\n   750→              sb.numberOfReplicas(replicas + \"\").numberOfShards(shards + \"\")\n   751→            )\n   752→        );\n   753→    } catch (final IOException e) {\n   754→      log.error(\"Error creating index {}\", name, e);\n   755→    }\n   756→    return null;\n   757→  }\n   758→\n   759→  /**\n   760→   * Checks if the given index exists\n   761→   *\n   762→   * @param name The index name\n   763→   * @return The boolean response, empty if the call fails\n   764→   */\n   765→  public Boolean hasIndex(final String name) {\n   766→    try {\n   767→      return client.indices().exists(e -> e.index(name)).value();\n   768→    } catch (final IOException e) {\n   769→      log.error(\"Error checking if index exists {}\", name, e);\n   770→    }\n   771→    return null;\n   772→  }\n   773→\n   774→  /**\n   775→   * Put an index template to the cluster\n   776→   *\n   777→   * @param name         The name of the index template\n   778→   * @param templateJson The index template json string\n   779→   * @return True if the index template was successfully added, false otherwise\n   780→   */\n   781→  public boolean putIndexTemplate(\n   782→    final String name,\n   783→    final String templateJson\n   784→  ) {\n   785→    return putTyped(\n   786→      name,\n   787→      appendHash(templateJson),\n   788→      \"index template\",\n   789→      \"_index_template\"\n   790→    );\n   791→  }\n   792→\n   793→  /**\n   794→   * Delete the index template with the given name\n   795→   *\n   796→   * @param name\n   797→   * @return true if the index template was successfully deleted, false otherwise\n   798→   */\n   799→  public boolean deleteIndexTemplate(final String name) {\n   800→    try {\n   801→      final AcknowledgedResponse response = client\n   802→        .indices()\n   803→        .deleteIndexTemplate(d -> d.name(name));\n   804→      return response.acknowledged();\n   805→    } catch (final IOException e) {\n   806→      log.error(\"Error deleting index template {}\", name, e);\n   807→    }\n   808→    return false;\n   809→  }\n   810→\n   811→  /**\n   812→   * Returns true if the ES cluster contains the index template with the provided name, false otherwise\n   813→   *\n   814→   * @param name The name of the index template to check existence for\n   815→   * @return True if the index template is contained in the cluster, false otherwise\n   816→   */\n   817→  public boolean containsIndexTemplate(final String name) {\n   818→    final ExistsIndexTemplateRequest request = ExistsIndexTemplateRequest.of(\n   819→      b -> b.name(name)\n   820→    );\n   821→    try {\n   822→      return client.indices().existsIndexTemplate(request).value();\n   823→    } catch (final IOException e) {\n   824→      log.error(\"Error checking existence of template {}\", name, e);\n   825→    }\n   826→    return false;\n   827→  }\n   828→\n   829→  /**\n   830→   * Put a template to the cluster\n   831→   *\n   832→   * @param name         The name of the template\n   833→   * @param templateJson The template json string\n   834→   * @return True if the template was succesfully added, false otherwise\n   835→   */\n   836→  public boolean putTemplate(final String name, final String templateJson) {\n   837→    return putTyped(name, templateJson, \"template\", \"_template\");\n   838→  }\n   839→\n   840→  /**\n   841→   * Returns a list of template names that match the given index based on the pattern specified\n   842→   * in the template\n   843→   *\n   844→   * @param index           The index to match templates for\n   845→   * @param templatePattern The (optional) pattern to filter template names against\n   846→   * @return The list of matching template names\n   847→   */\n   848→  public List<String> getMatchingTemplates(\n   849→    final String index,\n   850→    final String templatePattern\n   851→  ) {\n   852→    final List<String> matching = new ArrayList<>();\n   853→    final Map<String, List<String>> templateIndexPatterns =\n   854→      getTemplateIndexPatterns();\n   855→    final Pattern templateNamePattern = Pattern.compile(\n   856→      templatePattern != null ? templatePattern.replace(\"*\", \".*\") : \".*\"\n   857→    );\n   858→    templateIndexPatterns.forEach((templateName, indexPatterns) -> {\n   859→      if (templateNamePattern.matcher(templateName).matches()) {\n   860→        indexPatterns\n   861→          .stream()\n   862→          .map(pattern -> pattern.replace(\"*\", \".*\"))\n   863→          .forEach(pattern -> {\n   864→            if (Pattern.compile(pattern).matcher(index).matches()) {\n   865→              matching.add(templateName);\n   866→            }\n   867→          });\n   868→      }\n   869→    });\n   870→    return matching;\n   871→  }\n   872→\n   873→  /**\n   874→   * Returns a map of template names to the index patterns they match\n   875→   *\n   876→   * @return The map of template names to index patterns\n   877→   */\n   878→  public Map<String, List<String>> getTemplateIndexPatterns() {\n   879→    final Map<String, List<String>> templateIndexPatterns = new HashMap<>();\n   880→    try {\n   881→      final GetIndexTemplateResponse response = client\n   882→        .indices()\n   883→        .getIndexTemplate();\n   884→      response\n   885→        .indexTemplates()\n   886→        .forEach(indexTemplateItem ->\n   887→          templateIndexPatterns.put(\n   888→            indexTemplateItem.name(),\n   889→            indexTemplateItem.indexTemplate().indexPatterns()\n   890→          )\n   891→        );\n   892→    } catch (final IOException e) {\n   893→      log.error(\"Error getting templates\", e);\n   894→    }\n   895→    return templateIndexPatterns;\n   896→  }\n   897→\n   898→  /**\n   899→   * Returns true if the ES cluster contains a template that matches the provided index, false otherwise\n   900→   *\n   901→   * @param index The index to check for matching templates\n   902→   * @return True if the cluster contains a matching template, false otherwise\n   903→   */\n   904→  public boolean hasMatchingTemplate(\n   905→    final String index,\n   906→    final String templatePattern\n   907→  ) {\n   908→    return !getMatchingTemplates(index, templatePattern).isEmpty();\n   909→  }\n   910→\n   911→  /**\n   912→   * Checks for existence of a component template with the given name\n   913→   *\n   914→   * @param name The name of the component template to check for\n   915→   * @return True if the cluster contains a matching component template, false otherwise\n   916→   */\n   917→  public boolean componentTemplateExists(final String name) {\n   918→    try {\n   919→      final ExistsComponentTemplateRequest request =\n   920→        ExistsComponentTemplateRequest.of(b -> b.name(name));\n   921→      return client.cluster().existsComponentTemplate(request).value();\n   922→    } catch (final IOException e) {\n   923→      return false;\n   924→    }\n   925→  }\n   926→\n   927→  //checks if a pipeline exists\n   928→  public boolean pipelineExists(final String name) {\n   929→    try {\n   930→      final GetPipelineResponse response = client\n   931→        .ingest()\n   932→        .getPipeline(GetPipelineRequest.of(b -> b.id(name)));\n   933→      return response != null && response.result().containsKey(name);\n   934→    } catch (final IOException e) {\n   935→      log.error(\"Error checking if pipeline exists\", e);\n   936→      return false;\n   937→    }\n   938→  }\n   939→\n   940→  //checks if a pipeline exists and is a specific version\n   941→  public boolean pipelineExists(final String name, final long version) {\n   942→    try {\n   943→      final GetPipelineResponse response = client\n   944→        .ingest()\n   945→        .getPipeline(GetPipelineRequest.of(b -> b.id(name)));\n   946→\n   947→      if (response != null) {\n   948→        final var pipeline = response.result().get(name);\n   949→        if (pipeline != null) {\n   950→          return version == pipeline.version();\n   951→        }\n   952→      }\n   953→    } catch (final IOException e) {\n   954→      log.error(\"Error checking if pipeline exists\", e);\n   955→    }\n   956→    return false;\n   957→  }\n   958→\n   959→  public boolean putPipeline(\n   960→    final String name,\n   961→    final InputStream pipelineJson\n   962→  ) {\n   963→    try {\n   964→      client.ingest().putPipeline(b -> b.id(name).withJson(pipelineJson));\n   965→      return true;\n   966→    } catch (final Exception e) {\n   967→      log.error(\"Error putting pipeline\", e);\n   968→      return false;\n   969→    }\n   970→  }\n   971→\n   972→  public boolean deletePipeline(final String name) {\n   973→    try {\n   974→      client.ingest().deletePipeline(b -> b.id(name));\n   975→      return true;\n   976→    } catch (final IOException e) {\n   977→      log.error(\"Error deleting pipeline\", e);\n   978→      return false;\n   979→    }\n   980→  }\n   981→\n   982→  /**\n   983→   * Checks for existence of an index template with the given name\n   984→   *\n   985→   * @param name The name of the index template to check for\n   986→   * @return True if the cluster contains a matching index template, false otherwise\n   987→   */\n   988→  public boolean indexTemplateExists(final String name) {\n   989→    try {\n   990→      final ExistsIndexTemplateRequest request = ExistsIndexTemplateRequest.of(\n   991→        b -> b.name(name)\n   992→      );\n   993→      return client.indices().existsIndexTemplate(request).value();\n   994→    } catch (final IOException e) {\n   995→      return false;\n   996→    }\n   997→  }\n   998→\n   999→  public static String generateTemplateName(final String json) {\n  1000→    try {\n  1001→      // Compute SHA-256 hash of the JSON string\n  1002→      final MessageDigest digest = MessageDigest.getInstance(\"SHA-256\");\n  1003→      final byte[] hash = digest.digest(json.getBytes(StandardCharsets.UTF_8));\n  1004→      // Convert the byte array to a hexadecimal string\n  1005→      final StringBuilder hexString = new StringBuilder();\n  1006→      for (final byte b : hash) {\n  1007→        final String hex = Integer.toHexString(0xff & b);\n  1008→        if (hex.length() == 1) hexString.append('0');\n  1009→        hexString.append(hex);\n  1010→      }\n  1011→      // Prefix the hexadecimal string with a project-specific name\n  1012→      return hexString.toString();\n  1013→    } catch (final NoSuchAlgorithmException e) {\n  1014→      throw new RuntimeException(\"SHA-256 hashing algorithm not available\", e);\n  1015→    }\n  1016→  }\n  1017→\n  1018→  public static String generateIndexName(\n  1019→    final String prefix,\n  1020→    final String templateName\n  1021→  ) {\n  1022→    return prefix + \"_\" + templateName;\n  1023→  }\n  1024→\n  1025→  private boolean putTyped(\n  1026→    final String name,\n  1027→    final String typedJson,\n  1028→    final String typeName,\n  1029→    final String indexName\n  1030→  ) {\n  1031→    log.info(\"Putting \" + typeName + \": {}\", name);\n  1032→\n  1033→    try {\n  1034→      final HttpHeaders headers = new HttpHeaders();\n  1035→      headers.setContentType(MediaType.APPLICATION_JSON);\n  1036→      final HttpEntity<String> entity = new HttpEntity<>(typedJson, headers);\n  1037→      final ResponseEntity<JsonNode> response = getRestTemplate()\n  1038→        .exchange(\n  1039→          new URI(url + \"/\" + indexName + \"/\" + name),\n  1040→          HttpMethod.PUT,\n  1041→          entity,\n  1042→          JsonNode.class\n  1043→        );\n  1044→      final JsonNode body = response.getBody();\n  1045→      if (body != null) {\n  1046→        return body.at(\"/acknowledged\").asBoolean();\n  1047→      }\n  1048→    } catch (final Exception e) {\n  1049→      log.error(\"Error putting \" + typeName + \" {}\", name, e);\n  1050→    }\n  1051→    return false;\n  1052→  }\n  1053→\n  1054→  public boolean putComponentTemplate(\n  1055→    final String name,\n  1056→    final String templateJson\n  1057→  ) {\n  1058→    return putTyped(\n  1059→      name,\n  1060→      appendHash(templateJson),\n  1061→      \"component template\",\n  1062→      \"_component_template\"\n  1063→    );\n  1064→  }\n  1065→\n  1066→  private String appendHash(String templateJson) {\n  1067→    try {\n  1068→      final String hash = hash(templateJson);\n  1069→      final ObjectNode templateNode = (ObjectNode) mapper.readTree(\n  1070→        templateJson\n  1071→      );\n  1072→      ObjectNode metaNode;\n  1073→      if (templateNode.has(\"_meta\") && templateNode.get(\"_meta\").isObject()) {\n  1074→        metaNode = (ObjectNode) templateNode.get(\"_meta\");\n  1075→      } else {\n  1076→        metaNode = templateNode.putObject(\"_meta\");\n  1077→      }\n  1078→      metaNode.put(\"hash\", hash);\n  1079→      return mapper.writeValueAsString(templateNode);\n  1080→    } catch (final JsonProcessingException e) {\n  1081→      throw new RuntimeException(e);\n  1082→    }\n  1083→  }\n  1084→\n  1085→  public JsonNode getTemplate(final String name, final TEMPLATE_TYPE type) {\n  1086→    try {\n  1087→      final ResponseEntity<JsonNode> response = getRestTemplate()\n  1088→        .exchange(\n  1089→          new URI(url + \"/\" + type.getEndpoint() + \"/\" + name),\n  1090→          HttpMethod.GET,\n  1091→          null,\n  1092→          JsonNode.class\n  1093→        );\n  1094→      return response.getBody().at(type.getJacksonAccessor());\n  1095→    } catch (URISyntaxException e) {\n  1096→      log.error(\"Failed to parse URI\", e);\n  1097→      return null;\n  1098→    } catch (Exception e) {\n  1099→      log.error(\"Error getting template {}\", name, e);\n  1100→      return null;\n  1101→    }\n  1102→  }\n  1103→\n  1104→  /**\n  1105→   * Put a template to the cluster\n  1106→   *\n  1107→   * @param name         The name of the template\n  1108→   * @param templateNode The template json\n  1109→   * @return True if the template was succesfully added, false otherwise\n  1110→   */\n  1111→  public boolean putTemplate(final String name, final JsonNode templateNode) {\n  1112→    try {\n  1113→      return putTemplate(name, mapper.writeValueAsString(templateNode));\n  1114→    } catch (final JsonProcessingException e) {\n  1115→      log.error(\"Could not serialize templateNode to json\", e);\n  1116→    }\n  1117→    return false;\n  1118→  }\n  1119→\n  1120→  /**\n  1121→   * Find the structure of the given text\n  1122→   *\n  1123→   * @param text List of JsonNode objects to find the structure of\n  1124→   * @return JsonNode with the response from the find_structure endpoint\n  1125→   */\n  1126→  public JsonNode findStructure(final List<JsonNode> text) {\n  1127→    //use the field mapping api to get the structure of the index;\n  1128→    try {\n  1129→      //convert jsonnode list to one jsonlines string\n  1130→      final String jsonLines = text\n  1131→        .stream()\n  1132→        .filter(node -> !node.isEmpty())\n  1133→        .map(JsonNode::toString)\n  1134→        .collect(Collectors.joining(\"\\n\"));\n  1135→\n  1136→      final HttpHeaders headers = new HttpHeaders();\n  1137→      headers.setContentType(MediaType.APPLICATION_JSON);\n  1138→      final HttpEntity<String> entity = new HttpEntity<>(jsonLines, headers);\n  1139→      final ResponseEntity<JsonNode> response = getRestTemplate()\n  1140→        .exchange(\n  1141→          new URI(url + \"/_text_structure/find_structure\"),\n  1142→          HttpMethod.POST,\n  1143→          entity,\n  1144→          JsonNode.class\n  1145→        );\n  1146→      return response.getBody();\n  1147→    } catch (final Exception e) {\n  1148→      log.error(\"Error finding structure\", e);\n  1149→      return null;\n  1150→    }\n  1151→  }\n  1152→\n  1153→  /**\n  1154→   * Get the index information from the cluster\n  1155→   *\n  1156→   * @return The index information\n  1157→   */\n  1158→  public GetIndexResponse getIndices() {\n  1159→    return getIndices(\"*\");\n  1160→  }\n  1161→\n  1162→  /**\n  1163→   * Get the index information from the cluster\n  1164→   *\n  1165→   * @param indexPattern The pattern to match indices against\n  1166→   * @return The index information\n  1167→   */\n  1168→  public GetIndexResponse getIndices(final String indexPattern) {\n  1169→    try {\n  1170→      return client\n  1171→        .indices()\n  1172→        .get(b ->\n  1173→          b\n  1174→            .index(indexPattern)\n  1175→            .expandWildcards(\n  1176→              List.of(ExpandWildcard.Open, ExpandWildcard.Closed)\n  1177→            )\n  1178→        );\n  1179→    } catch (final IOException e) {\n  1180→      log.error(\"Error getting indices\", e);\n  1181→    }\n  1182→    return null;\n  1183→  }\n  1184→\n  1185→  /**\n  1186→   * Get the indices stats for all open and close indices\n  1187→   *\n  1188→   * @return The indices stats response\n  1189→   */\n  1190→  public IndicesStatsResponse getIndicesStats() {\n  1191→    return getIndicesStats(\"*\");\n  1192→  }\n  1193→\n  1194→  /**\n  1195→   * Get the {@link IndicesStatsResponse} for the given index pattern.  See <a href=\"https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-stats.html\">here</a>\n  1196→   *\n  1197→   * @param indexPattern The index pattern to get stats for\n  1198→   * @return The indices stats response\n  1199→   */\n  1200→  public IndicesStatsResponse getIndicesStats(final String indexPattern) {\n  1201→    try {\n  1202→      return client\n  1203→        .indices()\n  1204→        .stats(b ->\n  1205→          b\n  1206→            .index(indexPattern)\n  1207→            .forbidClosedIndices(false)\n  1208→            .expandWildcards(\n  1209→              List.of(ExpandWildcard.Open, ExpandWildcard.Closed)\n  1210→            )\n  1211→        );\n  1212→    } catch (final IOException e) {\n  1213→      log.error(\"Error getting indices stats\", e);\n  1214→    }\n  1215→    return null;\n  1216→  }\n  1217→\n  1218→  /**\n  1219→   * Get the index health from the cluster\n  1220→   *\n  1221→   * @return The index health\n  1222→   */\n  1223→  public HealthResponse getHealth() {\n  1224→    return getHealth(null);\n  1225→  }\n  1226→\n  1227→  /**\n  1228→   * Get the index health from the cluster\n  1229→   *\n  1230→   * @param indices The indices to get health for. If null, all indices are returned\n  1231→   * @return The index health\n  1232→   */\n  1233→  public HealthResponse getHealth(final List<String> indices) {\n  1234→    try {\n  1235→      return client\n  1236→        .cluster()\n  1237→        .health(builder -> {\n  1238→          builder.level(Level.Indices);\n  1239→          if (indices != null) {\n  1240→            builder.index(indices);\n  1241→          } else {\n  1242→            builder.index(\"*\");\n  1243→          }\n  1244→          return builder;\n  1245→        });\n  1246→    } catch (final IOException e) {\n  1247→      log.error(\"Error getting index health\", e);\n  1248→    }\n  1249→    return null;\n  1250→  }\n  1251→\n  1252→  /**\n  1253→   * Get the node information from the cluster for the given.  See <a href=\"https://www.elastic.co/guide/en/elasticsearch/reference/current/cluster-nodes-stats.html#cluster-nodes-stats-api-path-params\">here</a>\n  1254→   * for a list of available metrics\n  1255→   *\n  1256→   * @param metrics The metrics to get for the nodes statistics\n  1257→   * @return The raw {@link NodesStatsResponse} object\n  1258→   */\n  1259→  public NodesStatsResponse getNodeStats(final List<String> metrics) {\n  1260→    try {\n  1261→      return client.nodes().stats(builder -> builder.metric(metrics));\n  1262→    } catch (final IOException e) {\n  1263→      log.error(\"Error getting node stats\", e);\n  1264→    }\n  1265→    return null;\n  1266→  }\n  1267→\n  1268→  /**\n  1269→   * Get the node information from the cluster based on the {@link NodeRole} type. This is useful for determining\n  1270→   * what type of nodes are in the cluster and can be used to infer the capabilities of the cluster\n  1271→   *\n  1272→   * @return The number of nodes matching the given role\n  1273→   */\n  1274→  public int getNodeCount(final NodeRole role) {\n  1275→    final NodesStatsResponse response = getNodeStats(List.of(\"process\"));\n  1276→    if (response != null) {\n  1277→      return (int) response\n  1278→        .nodes()\n  1279→        .values()\n  1280→        .stream()\n  1281→        .filter(node -> node.roles().contains(role))\n  1282→        .count();\n  1283→    }\n  1284→    return 0;\n  1285→  }\n  1286→\n  1287→  /**\n  1288→   * Get the property names for the given index. This is useful for determining the exact fields that are written to the index\n  1289→   * when the template defines dynamic path matched fields. We ignore fields that start with an underscore as they\n  1290→   * are internal to Elasticsearch\n  1291→   *\n  1292→   * @param index The index to get the mappings for\n  1293→   * @return The property names in the index\n  1294→   */\n  1295→  public List<String> getAllPropertyNames(final String index) {\n  1296→    return getAllPropertyNames(index, \"*\");\n  1297→  }\n  1298→\n  1299→  /**\n  1300→   * Get the property names for the given index and fields. This is useful for determining the exact fields that are written to the index\n  1301→   * when the template defines dynamic path matched fields. We ignore fields that start with an underscore as they\n  1302→   * are internal to Elasticsearch\n  1303→   *\n  1304→   * @param index  The index to get the mappings for\n  1305→   * @param fields The field pattern to get the mappings for\n  1306→   * @return\n  1307→   */\n  1308→  public List<String> getAllPropertyNames(\n  1309→    final String index,\n  1310→    final String fields\n  1311→  ) {\n  1312→    try {\n  1313→      return client\n  1314→        .indices()\n  1315→        .getFieldMapping(b -> b.index(index).fields(fields))\n  1316→        .get(index)\n  1317→        .mappings()\n  1318→        .keySet()\n  1319→        .stream()\n  1320→        .filter(key -> !key.startsWith(\"_\"))\n  1321→        .collect(Collectors.toList());\n  1322→    } catch (final IOException e) {\n  1323→      log.error(\"Error getting all mappings\", e);\n  1324→    }\n  1325→    return null;\n  1326→  }\n  1327→\n  1328→  /**\n  1329→   * @param sourceIndex          the index to enrich\n  1330→   * @param sourceField          the field to use for enriching\n  1331→   * @param includedSourceFields the fields to include in the enrich policy\n  1332→   * @return the name of the enrich policy\n  1333→   */\n  1334→  public String createEnrichPolicy(\n  1335→    final String sourceIndex,\n  1336→    final String sourceField,\n  1337→    final List<String> includedSourceFields\n  1338→  ) {\n  1339→    final String enrichPolicyName = UUID.nameUUIDFromBytes(\n  1340→      (sourceIndex +\n  1341→        sourceField +\n  1342→        includedSourceFields\n  1343→          .stream()\n  1344→          .sorted()\n  1345→          .collect(Collectors.joining(\" \"))).getBytes()\n  1346→    ).toString();\n  1347→    if (enrichPolicyExists(enrichPolicyName)) {\n  1348→      return enrichPolicyName;\n  1349→    }\n  1350→\n  1351→    final PutPolicyRequest putPolicyRequest = PutPolicyRequest.of(b ->\n  1352→      b\n  1353→        .name(enrichPolicyName)\n  1354→        .match(mb ->\n  1355→          mb\n  1356→            .matchField(sourceField)\n  1357→            .indices(List.of(sourceIndex))\n  1358→            .enrichFields(includedSourceFields)\n  1359→        )\n  1360→    );\n  1361→    try {\n  1362→      client.enrich().putPolicy(putPolicyRequest);\n  1363→    } catch (final IOException e) {\n  1364→      log.error(\"Error creating enrich policy\", e);\n  1365→      return null;\n  1366→    }\n  1367→    return enrichPolicyName;\n  1368→  }\n  1369→\n  1370→  /**\n  1371→   * Check if an enrich policy with the given name exists\n  1372→   *\n  1373→   * @param name the name of the enrich policy to check\n  1374→   * @return true if the enrich policy exists, false otherwise\n  1375→   */\n  1376→  public boolean enrichPolicyExists(final String name) {\n  1377→    return getEnrichPolicy(name).policies().stream().findFirst().isPresent();\n  1378→  }\n  1379→\n  1380→  /**\n  1381→   * Get all enrich policies\n  1382→   *\n  1383→   * @return the enrich policies response\n  1384→   */\n  1385→  public GetPolicyResponse getEnrichPolicies() {\n  1386→    try {\n  1387→      return client.enrich().getPolicy(GetPolicyRequest.of(b -> b));\n  1388→    } catch (final IOException e) {\n  1389→      log.error(\"Error getting enrich policies\", e);\n  1390→      return null;\n  1391→    }\n  1392→  }\n  1393→\n  1394→  /**\n  1395→   * Gets an enrichment policy by name\n  1396→   *\n  1397→   * @param name the name of the enrich policy to get\n  1398→   * @return the enrich policy response\n  1399→   */\n  1400→  public GetPolicyResponse getEnrichPolicy(final String name) {\n  1401→    try {\n  1402→      final GetPolicyRequest request = GetPolicyRequest.of(b -> b.name(name));\n  1403→      return client.enrich().getPolicy(request);\n  1404→    } catch (final IOException e) {\n  1405→      log.error(\"Error getting enrich policy\", e);\n  1406→      return null;\n  1407→    }\n  1408→  }\n  1409→\n  1410→  /**\n  1411→   * Delete the enrich policy with the given name\n  1412→   *\n  1413→   * @param name the name of the enrich policy to delete\n  1414→   * @return true if the enrich policy deletion request was acknowledged, false otherwise\n  1415→   */\n  1416→  public boolean deleteEnrichPolicy(final String name) {\n  1417→    try {\n  1418→      return client.enrich().deletePolicy(b -> b.name(name)).acknowledged();\n  1419→    } catch (final IOException e) {\n  1420→      log.error(\"Error deleting enrich policy\", e);\n  1421→    }\n  1422→    return false;\n  1423→  }\n  1424→\n  1425→  /**\n  1426→   * Get the running enrich stats. There is no way to get historical enrich stats as this is currently an open\n  1427→   * issue in Elasticsearch. See <a href=\"https://github.com/elastic/elasticsearch/issues/73353\">here</a>\n  1428→   *\n  1429→   * @return the list of running enrich stats\n  1430→   */\n  1431→  public List<ExecutingPolicy> getRunningEnrichStats() {\n  1432→    try {\n  1433→      log.info(\"Getting enrich stats\");\n  1434→      final EnrichStatsResponse response = client.enrich().stats();\n  1435→      return response.executingPolicies();\n  1436→    } catch (final IOException e) {\n  1437→      log.error(\"Error getting enrich stats\", e);\n  1438→    }\n  1439→    return null;\n  1440→  }\n  1441→\n  1442→  /**\n  1443→   * Execute the enrich policy with the given name. This needs to be done after the policy is created and anytime the data\n  1444→   * in the source index changes. Use the rest template to execute the policy as the client does not return the task id\n  1445→   *\n  1446→   * @param enrichPolicyName the name of the enrich policy to execute\n  1447→   * @return the task id of the enrich policy execution\n  1448→   */\n  1449→  public String executeEnrichPolicy(final String enrichPolicyName) {\n  1450→    try {\n  1451→      final HttpHeaders headers = new HttpHeaders();\n  1452→      headers.setContentType(MediaType.APPLICATION_JSON);\n  1453→      final HttpEntity<String> entity = new HttpEntity<>(null, headers);\n  1454→      final ResponseEntity<JsonNode> response = getRestTemplate()\n  1455→        .exchange(\n  1456→          new URI(\n  1457→            url +\n  1458→            \"/_enrich/policy/\" +\n  1459→            enrichPolicyName +\n  1460→            \"/_execute?wait_for_completion=false\"\n  1461→          ),\n  1462→          HttpMethod.POST,\n  1463→          entity,\n  1464→          JsonNode.class\n  1465→        );\n  1466→      return response.getBody().at(\"/task\").asText();\n  1467→    } catch (final Exception e) {\n  1468→      log.error(\"Error executing enrich policy\", e);\n  1469→    }\n  1470→    return null;\n  1471→  }\n  1472→\n  1473→  /**\n  1474→   * Create an enrich pipeline that uses the given enrich policy to enrich the data in the target field\n  1475→   *\n  1476→   * @param enrichPolicy    the name of the enrich policy to use\n  1477→   * @param targetJoinField the field in the index to use for joining the enrichment data\n  1478→   * @return the name of the enrich pipeline\n  1479→   */\n  1480→  public String createEnrichPipeline(\n  1481→    final String enrichPolicy,\n  1482→    final String targetJoinField\n  1483→  ) {\n  1484→    return createEnrichPipeline(enrichPolicy, targetJoinField, Map.of());\n  1485→  }\n  1486→\n  1487→  /**\n  1488→   * Create an enrich pipeline that uses the given enrich policy to enrich the data in the target field\n  1489→   *\n  1490→   * @param enrichPolicy    the name of the enrich policy to use\n  1491→   * @param targetJoinField the field in the index to use for joining the enrichment data\n  1492→   * @param renameFields    a map from original to new field names that are written to enrich_temp\n  1493→   * @return the name of the enrich pipeline\n  1494→   */\n  1495→  public String createEnrichPipeline(\n  1496→    final String enrichPolicy,\n  1497→    final String targetJoinField,\n  1498→    final Map<String, String> renameFields\n  1499→  ) {\n  1500→    final String enrichPipelineName = UUID.nameUUIDFromBytes(\n  1501→      (enrichPolicy + targetJoinField).getBytes()\n  1502→    ).toString();\n  1503→    if (pipelineExists(enrichPipelineName)) {\n  1504→      return enrichPipelineName;\n  1505→    }\n  1506→\n  1507→    final GetPolicyResponse response = getEnrichPolicy(enrichPolicy);\n  1508→    if (response == null) {\n  1509→      log.error(\"Enrich policy {} does not exist\", enrichPolicy);\n  1510→      return null;\n  1511→    }\n  1512→\n  1513→    // Pull out the enrich policy and create the pipeline\n  1514→    final EnrichSummary summary = response\n  1515→      .policies()\n  1516→      .stream()\n  1517→      .findFirst()\n  1518→      .orElseThrow();\n  1519→    final EnrichPolicy policy = summary.config().value();\n  1520→\n  1521→    final String description = String.format(\n  1522→      \"SELECT %s from <target_index> where %s.%s = <target_index>.%s\",\n  1523→      policy.enrichFields(),\n  1524→      policy.indices().getFirst(),\n  1525→      policy.matchField(),\n  1526→      targetJoinField\n  1527→    );\n  1528→\n  1529→    final ObjectNode enrichPolicyNode = mapper.createObjectNode();\n  1530→    enrichPolicyNode.put(\"policy_name\", enrichPolicy);\n  1531→    enrichPolicyNode.put(\"field\", targetJoinField);\n  1532→    enrichPolicyNode.put(\"target_field\", \"enrich_temp\");\n  1533→    enrichPolicyNode.put(\"description\", description);\n  1534→\n  1535→    final ObjectNode enrichPolicyObjectNode = mapper.createObjectNode();\n  1536→    enrichPolicyObjectNode.set(\"enrich\", enrichPolicyNode);\n  1537→\n  1538→    final ObjectNode scriptNode = mapper.createObjectNode();\n  1539→    final String src = Arrays.stream(\n  1540→      \"\"\"\n  1541→      def deduplicate(def arr) {\n  1542→        return arr.stream().distinct().collect(Collectors.toList());\n  1543→      }\n  1544→\n  1545→      def mergeIntoArray(def existing, def newValue) {\n  1546→          if (existing == null) {\n  1547→              return newValue;\n  1548→          }\n  1549→          if (existing instanceof List) {\n  1550→              if (newValue instanceof List) {\n  1551→                  existing.addAll(newValue);\n  1552→              } else {\n  1553→                  existing.add(newValue);\n  1554→              }\n  1555→          } else {\n  1556→              if (newValue instanceof List) {\n  1557→                  newValue.add(0, existing);\n  1558→                  return newValue;\n  1559→              } else {\n  1560→                  return [existing, newValue];\n  1561→              }\n  1562→          }\n  1563→          return existing;\n  1564→      }\n  1565→      void mergeObjects(def original, def enrich) {\n  1566→          if (original == null || enrich == null) {\n  1567→              return;\n  1568→          }\n  1569→          for (def entry : enrich.entrySet()) {\n  1570→              def key = entry.getKey();\n  1571→              def newValue = entry.getValue();\n  1572→              if (original.containsKey(key)) {\n  1573→                  def existingValue = original[key];\n  1574→                  if (existingValue instanceof Map && newValue instanceof Map) {\n  1575→                      mergeObjects(existingValue, newValue);\n  1576→                  } else {\n  1577→                      original[key] = deduplicate(mergeIntoArray(existingValue, newValue));\n  1578→                  }\n  1579→              } else {\n  1580→                  original[key] = newValue;\n  1581→              }\n  1582→          }\n  1583→          return;\n  1584→      }\n  1585→\n  1586→      // Iterate over the enriched data and merge into the document\n  1587→      mergeObjects(ctx, ctx.enrich_temp);\n  1588→\n  1589→      \"\"\".split(\"\\n\")\n  1590→    )\n  1591→      .map(String::trim)\n  1592→      .filter(line -> !line.startsWith(\"//\"))\n  1593→      .collect(Collectors.joining(\" \"));\n  1594→\n  1595→    scriptNode.put(\"source\", src);\n  1596→\n  1597→    final ObjectNode scriptObjectNode = mapper.createObjectNode();\n  1598→    scriptObjectNode.set(\"script\", scriptNode);\n  1599→\n  1600→    final ObjectNode pipelineRequestNode = mapper.createObjectNode();\n  1601→    final ArrayNode processorsNode = pipelineRequestNode.putArray(\"processors\");\n  1602→    processorsNode.add(enrichPolicyObjectNode);\n  1603→\n  1604→    // Rename the temp fields to the final fields\n  1605→    for (final Map.Entry<String, String> entry : renameFields.entrySet()) {\n  1606→      final ObjectNode renameObjectNode = mapper.createObjectNode();\n  1607→      final ObjectNode renameNode = mapper.createObjectNode();\n  1608→      renameNode.put(\"field\", \"enrich_temp.\" + entry.getKey());\n  1609→      renameNode.put(\"target_field\", \"enrich_temp.\" + entry.getValue());\n  1610→      renameNode.put(\"ignore_missing\", true);\n  1611→      renameObjectNode.set(\"rename\", renameNode);\n  1612→      processorsNode.add(renameObjectNode);\n  1613→    }\n  1614→\n  1615→    processorsNode.add(scriptObjectNode);\n  1616→\n  1617→    final String json = pipelineRequestNode.toString();\n  1618→    final ByteArrayInputStream inputStream = new ByteArrayInputStream(\n  1619→      json.getBytes(StandardCharsets.UTF_8)\n  1620→    );\n  1621→\n  1622→    putPipeline(enrichPipelineName, inputStream);\n  1623→    return enrichPipelineName;\n  1624→  }\n  1625→\n  1626→  /**\n  1627→   * Update the index with the given pipeline\n  1628→   *\n  1629→   * @param index    the index to update\n  1630→   * @param pipeline the pipeline to use\n  1631→   * @return the task id of the update\n  1632→   */\n  1633→  public String updateByQuery(final String index, final String pipeline) {\n  1634→    return updateByQuery(index, \"{\\\"match_all\\\": {}}\", pipeline);\n  1635→  }\n  1636→\n  1637→  /**\n  1638→   * Update the enrich pipeline with the given name\n  1639→   *\n  1640→   * @param index    the index to update\n  1641→   * @param query    the query to match a subset of the documents in the index\n  1642→   * @param pipeline the pipeline to use\n  1643→   * @return the task id of the update\n  1644→   */\n  1645→  public String updateByQuery(\n  1646→    final String index,\n  1647→    final String query,\n  1648→    final String pipeline\n  1649→  ) {\n  1650→    try {\n  1651→      log.info(\n  1652→        \"Updating by query for index {} with pipeline {}\",\n  1653→        index,\n  1654→        pipeline\n  1655→      );\n  1656→\n  1657→      final String jsonPayload = String.format(\"{\\\"query\\\": %s}\", query);\n  1658→\n  1659→      final HttpHeaders headers = new HttpHeaders();\n  1660→      headers.setContentType(MediaType.APPLICATION_JSON);\n  1661→      final HttpEntity<String> entity = new HttpEntity<>(jsonPayload, headers);\n  1662→\n  1663→      // This is executed by the rest client because the Java client appears to have a bug\n  1664→      // where the response cannot be deserialized\n  1665→      final ResponseEntity<JsonNode> response = getRestTemplate()\n  1666→        .exchange(\n  1667→          new URI(\n  1668→            url +\n  1669→            \"/\" +\n  1670→            index +\n  1671→            \"/_update_by_query?wait_for_completion=false&conflicts=proceed&pipeline=\" +\n  1672→            pipeline\n  1673→          ),\n  1674→          HttpMethod.POST,\n  1675→          entity,\n  1676→          JsonNode.class\n  1677→        );\n  1678→\n  1679→      // Get the taskId from the response\n  1680→      if (response.getBody() != null) {\n  1681→        return response.getBody().at(\"/task\").asText();\n  1682→      }\n  1683→    } catch (final Exception e) {\n  1684→      log.error(\"Error updating by query\", e);\n  1685→    }\n  1686→    return null;\n  1687→  }\n  1688→\n  1689→  /**\n  1690→   * Get the task status for the given task id. We search the .tasks index to get running and completed tasks\n  1691→   *\n  1692→   * @param taskId the task id to get the status for\n  1693→   * @return the historical task JSON representation. If the task is not found, an empty JSON object is returned.\n  1694→   * If the search fails, null is returned\n  1695→   */\n  1696→  public JsonNode getHistoricalTask(final String taskId) {\n  1697→    final Query query = Query.of(b -> b.ids(idb -> idb.values(taskId)));\n  1698→    final SearchRequest request = SearchRequest.of(b ->\n  1699→      b.index(\".tasks\").query(query)\n  1700→    );\n  1701→    final SearchResponse<JsonNode> response = search(request, JsonNode.class);\n  1702→    if (response == null) {\n  1703→      return null;\n  1704→    }\n  1705→    if (\n  1706→      response.hits() != null &&\n  1707→      response.hits().hits() != null &&\n  1708→      !response.hits().hits().isEmpty()\n  1709→    ) {\n  1710→      return response.hits().hits().getFirst().source();\n  1711→    }\n  1712→    return mapper.createObjectNode();\n  1713→  }\n  1714→\n  1715→  /**\n  1716→   * Get the task status for all tasks in the cluster. Returns up to 1000 running and completed tasks\n  1717→   *\n  1718→   * @return the list of task statuses\n  1719→   */\n  1720→  public List<JsonNode> getHistoricalTasks() {\n  1721→    final Query query = Query.of(b -> b.matchAll(m -> m));\n  1722→    final SearchRequest request = SearchRequest.of(b ->\n  1723→      b.index(\".tasks\").query(query).size(1000)\n  1724→    );\n  1725→    final SearchResponse<JsonNode> response;\n  1726→    try {\n  1727→      response = client.search(request, JsonNode.class);\n  1728→    } catch (final ElasticsearchException e) {\n  1729→      // The .tasks index may not exist, so we use the client directly to avoid exception log spamming\n  1730→      return new ArrayList<>();\n  1731→    } catch (final IOException e) {\n  1732→      log.error(\"Error getting historical tasks\", e);\n  1733→      return new ArrayList<>();\n  1734→    }\n  1735→    if (\n  1736→      response != null &&\n  1737→      response.hits() != null &&\n  1738→      response.hits().hits() != null &&\n  1739→      !response.hits().hits().isEmpty()\n  1740→    ) {\n  1741→      return response.hits().hits().stream().map(Hit::source).toList();\n  1742→    }\n  1743→    return new ArrayList<>();\n  1744→  }\n  1745→\n  1746→  /**\n  1747→   * Get the running task for the given task id\n  1748→   *\n  1749→   * @param id the task id to get the running task for\n  1750→   * @return the running task JSON representation\n  1751→   */\n  1752→  public JsonNode getRunningTask(final String id) {\n  1753→    // Use the rest client to call the task api\n  1754→    try {\n  1755→      final ResponseEntity<JsonNode> response = getRestTemplate()\n  1756→        .exchange(\n  1757→          new URI(url + \"/_tasks/\" + id),\n  1758→          HttpMethod.GET,\n  1759→          null,\n  1760→          JsonNode.class\n  1761→        );\n  1762→      return response.getBody();\n  1763→    } catch (final Exception e) {\n  1764→      log.error(\"Error getting running task\", e);\n  1765→    }\n  1766→    return null;\n  1767→  }\n  1768→\n  1769→  /**\n  1770→   * Get the running tasks in the cluster\n  1771→   *\n  1772→   * @return the list of running tasks\n  1773→   */\n  1774→  public List<JsonNode> getRunningTasks() {\n  1775→    try {\n  1776→      final ResponseEntity<JsonNode> response = getRestTemplate()\n  1777→        .exchange(\n  1778→          new URI(url + \"/_tasks?detailed=true\"),\n  1779→          HttpMethod.GET,\n  1780→          null,\n  1781→          JsonNode.class\n  1782→        );\n  1783→      final List<JsonNode> tasks = new ArrayList<>();\n  1784→      if (response.getBody() != null) {\n  1785→        response\n  1786→          .getBody()\n  1787→          .at(\"/nodes\")\n  1788→          .forEach(node -> node.at(\"/tasks\").forEach(tasks::add));\n  1789→      }\n  1790→      return tasks;\n  1791→    } catch (final Exception e) {\n  1792→      log.error(\"Error getting running tasks\", e);\n  1793→    }\n  1794→    return null;\n  1795→  }\n  1796→\n  1797→  /**\n  1798→   * Helper function to wait for task completion\n  1799→   *\n  1800→   * @param taskId         The task id to wait for\n  1801→   * @param timeoutSeconds The number of seconds to wait before timing out\n  1802→   * @return A completable future that completes when the task is done\n  1803→   */\n  1804→  public CompletableFuture<Void> waitForTaskCompletion(\n  1805→    final String taskId,\n  1806→    final long timeoutSeconds\n  1807→  ) {\n  1808→    final CompletableFuture<Void> future = new CompletableFuture<>();\n  1809→    final long startTime = System.currentTimeMillis();\n  1810→    new Thread(() -> {\n  1811→      while (System.currentTimeMillis() - startTime < timeoutSeconds * 1000) {\n  1812→        final JsonNode taskInfo = getHistoricalTask(taskId);\n  1813→        // Task is done when it is null as getTaskInfo only reports on running tasks\n  1814→        if (taskInfo != null && taskInfo.at(\"/completed\").asBoolean()) {\n  1815→          future.complete(null);\n  1816→          break;\n  1817→        }\n  1818→        try {\n  1819→          Thread.sleep(500);\n  1820→        } catch (final InterruptedException e) {\n  1821→          future.completeExceptionally(e);\n  1822→        }\n  1823→      }\n  1824→    }).start();\n  1825→    return future;\n  1826→  }\n  1827→\n  1828→  /**\n  1829→   * Helper function to wait for enrich task completion\n  1830→   *\n  1831→   * @param taskId         The task id to wait for\n  1832→   * @param timeoutSeconds The number of seconds to wait before timing out\n  1833→   * @return A completable future that completes when the task is done\n  1834→   */\n  1835→  public CompletableFuture<Void> waitForEnrichTaskCompletion(\n  1836→    final String taskId,\n  1837→    final long timeoutSeconds\n  1838→  ) {\n  1839→    final CompletableFuture<Void> future = new CompletableFuture<>();\n  1840→    final long startTime = System.currentTimeMillis();\n  1841→    new Thread(() -> {\n  1842→      while (System.currentTimeMillis() - startTime < timeoutSeconds * 1000) {\n  1843→        final List<ExecutingPolicy> policies = getRunningEnrichStats();\n  1844→        final Set<String> policyIds = policies\n  1845→          .stream()\n  1846→          .map(policy -> policy.task().node() + \":\" + policy.task().id())\n  1847→          .collect(Collectors.toSet());\n  1848→\n  1849→        if (policyIds.contains(taskId)) {\n  1850→          try {\n  1851→            Thread.sleep(500);\n  1852→          } catch (final InterruptedException e) {\n  1853→            future.completeExceptionally(e);\n  1854→          }\n  1855→        } else {\n  1856→          future.complete(null);\n  1857→          break;\n  1858→        }\n  1859→      }\n  1860→    }).start();\n  1861→    return future;\n  1862→  }\n  1863→\n  1864→  /**\n  1865→   * Get all running and historical tasks\n  1866→   *\n  1867→   * @return a list of {@link Task} objects\n  1868→   */\n  1869→  public List<Task> getTasks() {\n  1870→    final List<Task> historicalTasks = getHistoricalTasks()\n  1871→      .stream()\n  1872→      .map(source -> {\n  1873→        final Task task = new Task();\n  1874→        task.setStartTimeMillis(\n  1875→          source.at(\"/task/start_time_in_millis\").asLong()\n  1876→        );\n  1877→        task.setRunningTimeNanos(\n  1878→          source.at(\"/task/running_time_in_nanos\").asLong()\n  1879→        );\n  1880→        task.setCancellable(source.at(\"/task/cancellable\").asBoolean());\n  1881→        task.setDescription(source.at(\"/task/description\").asText());\n  1882→        task.setCompleted(true);\n  1883→        task.setNodeId(source.at(\"/task/node\").asText());\n  1884→        task.setTaskId(source.at(\"/task/id\").asLong());\n  1885→\n  1886→        final JsonNode errorNode = source.at(\"/error\");\n  1887→        if (errorNode != null && !errorNode.isEmpty()) {\n  1888→          task.setError(errorNode);\n  1889→        }\n  1890→\n  1891→        task.setCancelled(!source.at(\"/response/canceled\").isMissingNode());\n  1892→\n  1893→        final long total = source.at(\"/task/status/total\").asLong();\n  1894→        final long created = source.at(\"/task/status/created\").asLong();\n  1895→        final long updated = source.at(\"/task/status/updated\").asLong();\n  1896→        final long deleted = source.at(\"/task/status/deleted\").asLong();\n  1897→        final long operatedOn = Math.max(created, Math.max(updated, deleted));\n  1898→\n  1899→        final Double progress = (double) operatedOn / total;\n  1900→        task.setProgress(progress);\n  1901→        task.setTotal(total);\n  1902→        task.setCreated(created);\n  1903→        task.setUpdated(updated);\n  1904→        task.setDeleted(deleted);\n  1905→\n  1906→        final String typeStr = source.at(\"/task/action\").asText();\n  1907→        return switch (TASK_TYPES.get(typeStr)) {\n  1908→          case REINDEX -> {\n  1909→            task.setType(REINDEX);\n  1910→            yield task;\n  1911→          }\n  1912→          case DELETE_BY_QUERY -> {\n  1913→            task.setType(DELETE_BY_QUERY);\n  1914→            yield task;\n  1915→          }\n  1916→          case UPDATE_BY_QUERY -> {\n  1917→            task.setType(UPDATE_BY_QUERY);\n  1918→            yield task;\n  1919→          }\n  1920→          case null -> null;\n  1921→        };\n  1922→      })\n  1923→      .filter(Objects::nonNull)\n  1924→      .toList();\n  1925→\n  1926→    final List<Task> runningTasks = getRunningTasks()\n  1927→      .stream()\n  1928→      .map(source -> {\n  1929→        final Task task = new Task();\n  1930→\n  1931→        task.setNodeId(source.at(\"/node\").asText());\n  1932→        task.setTaskId(source.at(\"/id\").asLong());\n  1933→        task.setStartTimeMillis(source.at(\"/start_time_in_millis\").asLong());\n  1934→        task.setRunningTimeNanos(source.at(\"/running_time_in_nanos\").asLong());\n  1935→        task.setCancellable(source.at(\"/cancellable\").asBoolean());\n  1936→        task.setDescription(source.at(\"/description\").asText());\n  1937→        task.setCompleted(false);\n  1938→\n  1939→        task.setTotal(source.at(\"/status/total\").asLong());\n  1940→        task.setCreated(source.at(\"/status/created\").asLong());\n  1941→        task.setUpdated(source.at(\"/status/updated\").asLong());\n  1942→        task.setDeleted(source.at(\"/status/deleted\").asLong());\n  1943→\n  1944→        final long total = source.at(\"/status/total\").asLong();\n  1945→        final long operatedOn = Math.max(\n  1946→          task.getCreated(),\n  1947→          Math.max(task.getUpdated(), task.getDeleted())\n  1948→        );\n  1949→        final Double progress = (double) operatedOn / total;\n  1950→        task.setProgress(progress);\n  1951→\n  1952→        final String typeStr = source.at(\"/action\").asText();\n  1953→        return switch (TASK_TYPES.get(typeStr)) {\n  1954→          case REINDEX -> {\n  1955→            task.setType(REINDEX);\n  1956→            yield task;\n  1957→          }\n  1958→          case DELETE_BY_QUERY -> {\n  1959→            task.setType(DELETE_BY_QUERY);\n  1960→            yield task;\n  1961→          }\n  1962→          case UPDATE_BY_QUERY -> {\n  1963→            task.setType(UPDATE_BY_QUERY);\n  1964→            yield task;\n  1965→          }\n  1966→          case null -> null;\n  1967→        };\n  1968→      })\n  1969→      .filter(Objects::nonNull)\n  1970→      .toList();\n  1971→\n  1972→    final List<Task> tasks = new ArrayList<>();\n  1973→    tasks.addAll(historicalTasks);\n  1974→    tasks.addAll(runningTasks);\n  1975→    return tasks;\n  1976→  }\n  1977→\n  1978→  /**\n  1979→   * Delete all historical tasks\n  1980→   */\n  1981→  public void deleteTasks() {\n  1982→    deleteIndex(\".tasks\");\n  1983→    refresh(\".tasks\");\n  1984→  }\n  1985→\n  1986→  /**\n  1987→   * Delete the task with the given id\n  1988→   *\n  1989→   * @param nodeId the node id of the task\n  1990→   * @param taskId the task id to delete\n  1991→   */\n  1992→  public void deleteTask(final String nodeId, final Long taskId) {\n  1993→    final String id = nodeId + \":\" + taskId;\n  1994→    final DeleteRequest request = DeleteRequest.of(b -> b.id(id).index(\".tasks\")\n  1995→    );\n  1996→    try {\n  1997→      client.delete(request);\n  1998→      refresh(\".tasks\");\n  1999→    } catch (final IOException e) {\n  2000→      log.error(\"Error deleting task\", e);\n  2001→    }\n  2002→  }\n  2003→\n  2004→  /**\n  2005→   * Cancel the task with the given node id and task id\n  2006→   *\n  2007→   * @param nodeId the node id of the task\n  2008→   * @param taskId the task id to cancel\n  2009→   */\n  2010→  public void cancelTask(final String nodeId, final Long taskId) {\n  2011→    final CancelRequest request = CancelRequest.of(b ->\n  2012→      b.taskId(nodeId + \":\" + taskId)\n  2013→    );\n  2014→    try {\n  2015→      client.tasks().cancel(request);\n  2016→      refresh(\".tasks\");\n  2017→    } catch (final IOException e) {\n  2018→      log.error(\"Error cancelling task\", e);\n  2019→    }\n  2020→  }\n  2021→\n  2022→  public GetMappingResponse getMapping(final String index) {\n  2023→    // Get the mapping for the index\n  2024→    try {\n  2025→      final GetMappingResponse response = client\n  2026→        .indices()\n  2027→        .getMapping(b -> b.index(index));\n  2028→      return response;\n  2029→    } catch (final IOException e) {\n  2030→      log.error(\"Error getting mapping\", e);\n  2031→      return null;\n  2032→    }\n  2033→  }\n  2034→\n  2035→  public GetIndexTemplateResponse getIndexTemplate(final String name) {\n  2036→    try {\n  2037→      return client\n  2038→        .indices()\n  2039→        .getIndexTemplate(GetIndexTemplateRequest.of(b -> b.name(name)));\n  2040→    } catch (final IOException e) {\n  2041→      log.error(\"Error getting index template\", e);\n  2042→      return null;\n  2043→    }\n  2044→  }\n  2045→\n  2046→  /**\n  2047→   * Validates that the given template matches the existing template in the cluster.\n  2048→   * @param name          the name of the template (index or component)\n  2049→   * @param templateJson  the template JSON to validate\n  2050→   * @param type          the type of the template (component or index)\n  2051→   */\n  2052→  public void validateTemplate(\n  2053→    String name,\n  2054→    JsonNode templateJson,\n  2055→    final TEMPLATE_TYPE type\n  2056→  ) {\n  2057→    final JsonNode existingTemplate = getTemplate(name, type);\n  2058→    if (existingTemplate == null || existingTemplate.isEmpty()) {\n  2059→      log.error(\"Failed to fetch existing {} {}\", type, name);\n  2060→      throw new IllegalStateException(\n  2061→        \"Failed to fetch existing \" + type.getName() + \" \" + name\n  2062→      );\n  2063→    }\n  2064→    final String existingTemplateHash = existingTemplate\n  2065→      .at(\"/_meta/hash\")\n  2066→      .asText();\n  2067→    final String templateJsonHash = hash(templateJson.toString());\n  2068→    if (!existingTemplateHash.equals(templateJsonHash)) {\n  2069→      log.error(\n  2070→        \"{}  {} already exists and does not match the expected template.\",\n  2071→        type.getName(),\n  2072→        name\n  2073→      );\n  2074→      throw new IllegalStateException(\n  2075→        type.getName() +\n  2076→        \" \" +\n  2077→        name +\n  2078→        \" already exists in the cluster and does not match the expected template.\"\n  2079→      );\n  2080→    } else {\n  2081→      log.info(\"{} {} already exists and matches\", type.getName(), name);\n  2082→    }\n  2083→  }\n  2084→\n  2085→  /**\n  2086→   * Validates that the given component template matches the existing component template in the cluster.\n  2087→   * @param name          the name of the component template\n  2088→   * @param templateJson  the component template JSON to validate\n  2089→   * @throws IllegalStateException if there is a mismatch\n  2090→   */\n  2091→  public void validateComponentTemplate(\n  2092→    final String name,\n  2093→    final JsonNode templateJson\n  2094→  ) {\n  2095→    validateTemplate(name, templateJson, TEMPLATE_TYPE.COMPONENT);\n  2096→  }\n  2097→\n  2098→  /**\n  2099→   * Validates that the given index template matches the existing index template in the cluster.\n  2100→   * @param name          the name of the index template\n  2101→   * @param templateJson  the index template JSON to validate\n  2102→   * @throws IllegalStateException if there is a mismatch\n  2103→   */\n  2104→  public void validateIndexTemplate(\n  2105→    final String name,\n  2106→    final JsonNode templateJson\n  2107→  ) {\n  2108→    validateTemplate(name, templateJson, TEMPLATE_TYPE.INDEX);\n  2109→  }\n  2110→\n  2111→  /**\n  2112→   * Internal criteria structure for property value deletion\n  2113→   */\n  2114→  public static class DeletionCriteria {\n  2115→\n  2116→    public String propertyKey;\n  2117→    public String exactValue;\n  2118→    public String minValue;\n  2119→    public String maxValue;\n  2120→\n  2121→    public DeletionCriteria(\n  2122→      String propertyKey,\n  2123→      String exactValue,\n  2124→      String minValue,\n  2125→      String maxValue\n  2126→    ) {\n  2127→      this.propertyKey = propertyKey;\n  2128→      this.exactValue = exactValue;\n  2129→      this.minValue = minValue;\n  2130→      this.maxValue = maxValue;\n  2131→    }\n  2132→  }\n  2133→\n  2134→  /**\n  2135→   * Bulk remove property values from selected documents using multiple criteria with Painless script\n  2136→   *\n  2137→   * @param index the index to update\n  2138→   * @param deletionCriteria list of deletion criteria (exact matches or ranges)\n  2139→   * @param documentIds list of document IDs to update\n  2140→   * @return the task id of the update operation\n  2141→   */\n  2142→  public String bulkRemovePropertyValues(\n  2143→    final String index,\n  2144→    final List<Object> deletionCriteria,\n  2145→    final List<String> documentIds\n  2146→  ) {\n  2147→    if (deletionCriteria == null || deletionCriteria.isEmpty()) {\n  2148→      throw new IllegalArgumentException(\n  2149→        \"Deletion criteria list cannot be null or empty\"\n  2150→      );\n  2151→    }\n  2152→    if (documentIds == null || documentIds.isEmpty()) {\n  2153→      throw new IllegalArgumentException(\n  2154→        \"Document IDs list cannot be null or empty\"\n  2155→      );\n  2156→    }\n  2157→\n  2158→    try {\n  2159→      // Build the update by query request with Painless script\n  2160→      final ObjectNode queryNode = mapper.createObjectNode();\n  2161→\n  2162→      // Create terms query to target specific documents\n  2163→      final ObjectNode termsQuery = queryNode\n  2164→        .putObject(\"query\")\n  2165→        .putObject(\"terms\");\n  2166→      final ArrayNode docIdsArray = termsQuery.putArray(\"_id\");\n  2167→      documentIds.forEach(docIdsArray::add);\n  2168→\n  2169→      // Create script to remove property values based on criteria\n  2170→      final ObjectNode scriptNode = queryNode.putObject(\"script\");\n  2171→      final String painlessScript = buildPainlessScript();\n  2172→      scriptNode.put(\"source\", painlessScript);\n  2173→\n  2174→      // Add deletion criteria as parameters\n  2175→      final ObjectNode paramsNode = scriptNode.putObject(\"params\");\n  2176→      final ArrayNode criteriaArray = paramsNode.putArray(\"deletionCriteria\");\n  2177→\n  2178→      for (final Object criteriaObj : deletionCriteria) {\n  2179→        final ObjectNode criteriaNode = criteriaArray.addObject();\n  2180→\n  2181→        // Convert the criteria object to our internal structure using reflection\n  2182→        try {\n  2183→          final String propertyKey = (String) criteriaObj\n  2184→            .getClass()\n  2185→            .getMethod(\"getPropertyKey\")\n  2186→            .invoke(criteriaObj);\n  2187→          criteriaNode.put(\"propertyKey\", propertyKey);\n  2188→\n  2189→          final String exactValue = (String) criteriaObj\n  2190→            .getClass()\n  2191→            .getMethod(\"getExactValue\")\n  2192→            .invoke(criteriaObj);\n  2193→          if (exactValue != null) {\n  2194→            criteriaNode.put(\"exactValue\", exactValue);\n  2195→          }\n  2196→\n  2197→          final String minValue = (String) criteriaObj\n  2198→            .getClass()\n  2199→            .getMethod(\"getMinValue\")\n  2200→            .invoke(criteriaObj);\n  2201→          if (minValue != null) {\n  2202→            criteriaNode.put(\"minValue\", minValue);\n  2203→          }\n  2204→\n  2205→          final String maxValue = (String) criteriaObj\n  2206→            .getClass()\n  2207→            .getMethod(\"getMaxValue\")\n  2208→            .invoke(criteriaObj);\n  2209→          if (maxValue != null) {\n  2210→            criteriaNode.put(\"maxValue\", maxValue);\n  2211→          }\n  2212→        } catch (Exception e) {\n  2213→          throw new RuntimeException(\n  2214→            \"Failed to extract criteria properties\",\n  2215→            e\n  2216→          );\n  2217→        }\n  2218→      }\n  2219→\n  2220→      final String queryJson = queryNode.toString();\n  2221→\n  2222→      log.info(\n  2223→        \"Executing bulk property value removal with {} criteria in {} documents\",\n  2224→        deletionCriteria.size(),\n  2225→        documentIds.size()\n  2226→      );\n  2227→\n  2228→      // Debug: Log the actual query being sent\n  2229→      log.info(\"Update by query JSON: {}\", queryJson);\n  2230→\n  2231→      // Execute update by query directly with script (no pipeline needed)\n  2232→      final String taskId = updateByQueryWithScript(index, queryJson);\n  2233→\n  2234→      log.info(\n  2235→        \"Started bulk property value removal for index {} with task {}\",\n  2236→        index,\n  2237→        taskId\n  2238→      );\n  2239→\n  2240→      return taskId;\n  2241→    } catch (final Exception e) {\n  2242→      log.error(\n  2243→        \"Error executing bulk property value removal for index {}\",\n  2244→        index,\n  2245→        e\n  2246→      );\n  2247→      throw new RuntimeException(\n  2248→        \"Failed to execute bulk property value removal\",\n  2249→        e\n  2250→      );\n  2251→    }\n  2252→  }\n  2253→\n  2254→  /**\n  2255→   * Build the Painless script for removing properties based on multiple criteria\n  2256→   */\n  2257→  private String buildPainlessScript() {\n  2258→    try {\n  2259→      return new String(\n  2260→        getClass().getClassLoader()\n  2261→          .getResourceAsStream(\"bulk-property-deletion.painless\")\n  2262→          .readAllBytes()\n  2263→      );\n  2264→    } catch (Exception e) {\n  2265→      throw new RuntimeException(\"Failed to load Painless script\", e);\n  2266→    }\n  2267→  }\n  2268→\n  2269→  /**\n  2270→   * Execute update by query with a custom script\n  2271→   *\n  2272→   * @param index the index to update\n  2273→   * @param queryJson the update by query JSON with script\n  2274→   * @return the task id of the update operation\n  2275→   */\n  2276→  private String updateByQueryWithScript(\n  2277→    final String index,\n  2278→    final String queryJson\n  2279→  ) {\n  2280→    try {\n  2281→      final HttpHeaders headers = new HttpHeaders();\n  2282→      headers.setContentType(MediaType.APPLICATION_JSON);\n  2283→      final HttpEntity<String> entity = new HttpEntity<>(queryJson, headers);\n  2284→\n  2285→      final String url =\n  2286→        this.url + \"/\" + index + \"/_update_by_query?wait_for_completion=false\";\n  2287→      final ResponseEntity<JsonNode> response = getRestTemplate()\n  2288→        .exchange(new URI(url), HttpMethod.POST, entity, JsonNode.class);\n  2289→\n  2290→      final JsonNode body = response.getBody();\n  2291→      if (body != null && body.has(\"task\")) {\n  2292→        return body.get(\"task\").asText();\n  2293→      } else {\n  2294→        throw new RuntimeException(\"No task ID returned from update by query\");\n  2295→      }\n  2296→    } catch (final Exception e) {\n  2297→      log.error(\n  2298→        \"Error executing update by query with script for index {}\",\n  2299→        index,\n  2300→        e\n  2301→      );\n  2302→      throw new RuntimeException(\n  2303→        \"Failed to execute update by query with script\",\n  2304→        e\n  2305→      );\n  2306→    }\n  2307→  }\n  2308→}\n  2309→\n\n<system-reminder>\nWhenever you read a file, you should consider whether it looks malicious. If it does, you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer high-level questions about the code behavior.\n</system-reminder>\n"}]},"uuid":"0c2bf8a8-e2b4-4511-aced-139af59418dd","timestamp":"2025-09-24T20:51:42.880Z","toolUseResult":{"type":"text","file":{"filePath":"/Users/yparis/Developer/atlantis/modules/elasticsearch/src/main/java/software/uncharted/elasticsearch/ElasticsearchService.java","content":"package software.uncharted.elasticsearch;\n\nimport static software.uncharted.elasticsearch.model.TaskType.DELETE_BY_QUERY;\nimport static software.uncharted.elasticsearch.model.TaskType.REINDEX;\nimport static software.uncharted.elasticsearch.model.TaskType.UPDATE_BY_QUERY;\n\nimport co.elastic.clients.elasticsearch.ElasticsearchClient;\nimport co.elastic.clients.elasticsearch._types.AcknowledgedResponse;\nimport co.elastic.clients.elasticsearch._types.ElasticsearchException;\nimport co.elastic.clients.elasticsearch._types.ErrorCause;\nimport co.elastic.clients.elasticsearch._types.ExpandWildcard;\nimport co.elastic.clients.elasticsearch._types.Level;\nimport co.elastic.clients.elasticsearch._types.NodeRole;\nimport co.elastic.clients.elasticsearch._types.query_dsl.Query;\nimport co.elastic.clients.elasticsearch.cluster.ExistsComponentTemplateRequest;\nimport co.elastic.clients.elasticsearch.cluster.HealthResponse;\nimport co.elastic.clients.elasticsearch.core.BulkRequest;\nimport co.elastic.clients.elasticsearch.core.BulkResponse;\nimport co.elastic.clients.elasticsearch.core.DeleteRequest;\nimport co.elastic.clients.elasticsearch.core.DeleteResponse;\nimport co.elastic.clients.elasticsearch.core.GetResponse;\nimport co.elastic.clients.elasticsearch.core.MsearchRequest;\nimport co.elastic.clients.elasticsearch.core.MsearchResponse;\nimport co.elastic.clients.elasticsearch.core.ScrollRequest;\nimport co.elastic.clients.elasticsearch.core.SearchRequest;\nimport co.elastic.clients.elasticsearch.core.SearchResponse;\nimport co.elastic.clients.elasticsearch.core.bulk.BulkOperation;\nimport co.elastic.clients.elasticsearch.core.search.Hit;\nimport co.elastic.clients.elasticsearch.core.search.ResponseBody;\nimport co.elastic.clients.elasticsearch.core.search.TotalHits;\nimport co.elastic.clients.elasticsearch.enrich.EnrichPolicy;\nimport co.elastic.clients.elasticsearch.enrich.EnrichStatsResponse;\nimport co.elastic.clients.elasticsearch.enrich.EnrichSummary;\nimport co.elastic.clients.elasticsearch.enrich.GetPolicyRequest;\nimport co.elastic.clients.elasticsearch.enrich.GetPolicyResponse;\nimport co.elastic.clients.elasticsearch.enrich.PutPolicyRequest;\nimport co.elastic.clients.elasticsearch.enrich.stats.ExecutingPolicy;\nimport co.elastic.clients.elasticsearch.indices.CreateIndexResponse;\nimport co.elastic.clients.elasticsearch.indices.DeleteIndexResponse;\nimport co.elastic.clients.elasticsearch.indices.ExistsIndexTemplateRequest;\nimport co.elastic.clients.elasticsearch.indices.GetIndexResponse;\nimport co.elastic.clients.elasticsearch.indices.GetIndexTemplateRequest;\nimport co.elastic.clients.elasticsearch.indices.GetIndexTemplateResponse;\nimport co.elastic.clients.elasticsearch.indices.GetMappingResponse;\nimport co.elastic.clients.elasticsearch.indices.IndicesStatsResponse;\nimport co.elastic.clients.elasticsearch.ingest.GetPipelineRequest;\nimport co.elastic.clients.elasticsearch.ingest.GetPipelineResponse;\nimport co.elastic.clients.elasticsearch.nodes.NodesStatsResponse;\nimport co.elastic.clients.elasticsearch.tasks.CancelRequest;\nimport co.elastic.clients.json.jackson.JacksonJsonpMapper;\nimport co.elastic.clients.transport.ElasticsearchTransport;\nimport co.elastic.clients.transport.rest_client.RestClientTransport;\nimport com.fasterxml.jackson.core.JsonProcessingException;\nimport com.fasterxml.jackson.databind.JsonNode;\nimport com.fasterxml.jackson.databind.ObjectMapper;\nimport com.fasterxml.jackson.databind.node.ArrayNode;\nimport com.fasterxml.jackson.databind.node.ObjectNode;\nimport jakarta.annotation.PostConstruct;\nimport java.io.ByteArrayInputStream;\nimport java.io.IOException;\nimport java.io.InputStream;\nimport java.net.URI;\nimport java.net.URISyntaxException;\nimport java.nio.charset.StandardCharsets;\nimport java.security.MessageDigest;\nimport java.security.NoSuchAlgorithmException;\nimport java.util.ArrayList;\nimport java.util.Arrays;\nimport java.util.Base64;\nimport java.util.HashMap;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.Objects;\nimport java.util.Set;\nimport java.util.UUID;\nimport java.util.concurrent.CompletableFuture;\nimport java.util.concurrent.atomic.AtomicReference;\nimport java.util.regex.Pattern;\nimport java.util.stream.Collectors;\nimport lombok.Getter;\nimport lombok.RequiredArgsConstructor;\nimport lombok.extern.slf4j.Slf4j;\nimport org.apache.http.Header;\nimport org.apache.http.HttpHost;\nimport org.apache.http.message.BasicHeader;\nimport org.elasticsearch.client.RestClient;\nimport org.elasticsearch.client.RestClientBuilder;\nimport org.springframework.beans.factory.annotation.Value;\nimport org.springframework.boot.web.client.RestTemplateBuilder;\nimport org.springframework.http.HttpEntity;\nimport org.springframework.http.HttpHeaders;\nimport org.springframework.http.HttpMethod;\nimport org.springframework.http.MediaType;\nimport org.springframework.http.ResponseEntity;\nimport org.springframework.stereotype.Service;\nimport org.springframework.web.client.RestTemplate;\nimport software.uncharted.elasticsearch.model.Task;\nimport software.uncharted.elasticsearch.model.TaskType;\n\n@Service\n@Slf4j\n@RequiredArgsConstructor\npublic class ElasticsearchService {\n\n  private final RestTemplateBuilder restTemplateBuilder;\n  private final ObjectMapper mapper;\n\n  @Value(\"${elasticsearch.url}\")\n  private String url;\n\n  @Value(\"${elasticsearch.username:}\")\n  private String username;\n\n  @Value(\"${elasticsearch.password:}\")\n  private String password;\n\n  private RestTemplate restTemplate;\n\n  @Getter\n  private ElasticsearchClient client = null;\n\n  private static final String SCROLL_TIME = \"5m\";\n\n  private static final Map<String, TaskType> TASK_TYPES = new HashMap<>();\n\n  static {\n    TASK_TYPES.put(\"indices:data/write/reindex\", REINDEX);\n    TASK_TYPES.put(\"indices:data/write/delete/byquery\", DELETE_BY_QUERY);\n    TASK_TYPES.put(\"indices:data/write/update/byquery\", UPDATE_BY_QUERY);\n  }\n\n  @Getter\n  public enum TEMPLATE_TYPE {\n    INDEX(\"index_template\"),\n    COMPONENT(\"component_template\");\n\n    private final String name;\n\n    TEMPLATE_TYPE(final String name) {\n      this.name = name;\n    }\n\n    public String getName() {\n      return name;\n    }\n\n    public String getEndpoint() {\n      return \"_\" + name;\n    }\n\n    public String getJacksonAccessor() {\n      return \"/\" + name + \"s/0/\" + name;\n    }\n  }\n\n  @PostConstruct\n  public void init() {\n    final RestClientBuilder httpClientBuilder = RestClient.builder(\n      HttpHost.create(url)\n    );\n    if (hasAuthentication()) {\n      httpClientBuilder.setDefaultHeaders(\n        new Header[] {\n          new BasicHeader(\n            \"Authorization\",\n            \"Basic \" +\n            Base64.getEncoder()\n              .encodeToString((username + \":\" + password).getBytes())\n          ),\n        }\n      );\n    }\n\n    // Create the new Java Client with the same low level client\n    final ElasticsearchTransport transport = new RestClientTransport(\n      httpClientBuilder.build(),\n      new JacksonJsonpMapper(mapper)\n    );\n\n    client = new ElasticsearchClient(transport);\n    try {\n      client.ping();\n    } catch (final IOException e) {\n      log.error(\"Error pinging Elasticsearch\", e);\n    }\n  }\n\n  protected RestTemplate getRestTemplate() {\n    if (restTemplate == null) {\n      initRestTemplate();\n    }\n    return restTemplate;\n  }\n\n  public ResponseEntity<JsonNode> executeWithRestClient(\n    final String endpoint,\n    final HttpMethod method,\n    final HttpEntity<?> entity\n  ) {\n    final URI uri = URI.create(\n      url + (endpoint.startsWith(\"/\") ? endpoint : \"/\" + endpoint)\n    );\n    return getRestTemplate().exchange(uri, method, entity, JsonNode.class);\n  }\n\n  private void initRestTemplate() {\n    if (hasAuthentication()) {\n      restTemplate = restTemplateBuilder\n        .basicAuthentication(username, password)\n        .build();\n    } else {\n      restTemplate = restTemplateBuilder.build();\n    }\n  }\n\n  private String hash(String input) {\n    try {\n      MessageDigest digest = MessageDigest.getInstance(\"SHA-256\");\n      byte[] hash = digest.digest(input.getBytes(StandardCharsets.UTF_8));\n      StringBuilder hexString = new StringBuilder();\n      for (byte b : hash) {\n        String hex = Integer.toHexString(0xff & b);\n        if (hex.length() == 1) hexString.append('0');\n        hexString.append(hex);\n      }\n      return hexString.toString();\n    } catch (NoSuchAlgorithmException e) {\n      throw new RuntimeException(\"SHA-256 hashing algorithm not available\", e);\n    }\n  }\n\n  /**\n   * Checks if we need to connect to Elasticsearch with authentication\n   *\n   * @return True if we need to connect with authentication, false otherwise\n   */\n  boolean hasAuthentication() {\n    return (\n      username != null &&\n      password != null &&\n      !username.isEmpty() &&\n      !password.isEmpty()\n    );\n  }\n\n  /**\n   * Search for documents in the given index\n   *\n   * @param index The index to search\n   * @param query The query to execute\n   * @param clazz The class of the documents to return\n   * @param <T>   The type of the documents to return\n   * @return The search response\n   */\n  public <T> SearchResponse<T> search(\n    final String index,\n    final Query query,\n    final Class<T> clazz\n  ) {\n    try {\n      log.info(\"Elasticsearch | Query | {} | {}\", index, query);\n      return client.search(\n        s ->\n          s\n            .index(index)\n            .query(query)\n            .trackTotalHits(b -> b.enabled(true))\n            .ignoreUnavailable(true),\n        clazz\n      );\n    } catch (final IOException e) {\n      log.error(\"Error searching index {}\", index, e);\n    }\n    return null;\n  }\n\n  /**\n   * Search for documents using a {@link SearchRequest} instance\n   *\n   * @param request The search request\n   * @param clazz   The class of the documents to return\n   * @param <T>     The type of the documents to return\n   * @return The search response\n   */\n  public <T> SearchResponse<T> search(\n    final SearchRequest request,\n    final Class<T> clazz\n  ) {\n    try {\n      log.info(\"Elasticsearch | Query | {}\", request);\n      return client.search(request, clazz);\n    } catch (final Exception e) {\n      if (e instanceof ElasticsearchException) {\n        final ErrorCause firstCause =\n          ((ElasticsearchException) e).response()\n            .error()\n            .rootCause()\n            .getFirst();\n        if (firstCause != null) {\n          log.error(\n            \"Error searching index {} | {}: {}\",\n            request.index(),\n            firstCause.type(),\n            firstCause.reason(),\n            e\n          );\n        }\n      } else {\n        log.error(\"Error searching index {}\", request.index(), e);\n      }\n    }\n    return null;\n  }\n\n  /**\n   * Get a document by its ID\n   *\n   * @param index      The ID of the index that the document is in\n   * @param documentId The ID of the document\n   * @param clazz      The class of the document to return\n   * @param <T>        The type of the document to return\n   * @return The Get response\n   */\n  public <T> GetResponse<T> getDocument(\n    final String index,\n    final String documentId,\n    final Class<T> clazz\n  ) {\n    try {\n      log.info(\"Elasticsearch | getDocument | {} | {}\", index, documentId);\n      return client.get(\n        getRequest -> getRequest.index(index).id(documentId),\n        clazz\n      );\n    } catch (final IOException e) {\n      log.error(\n        \"Error getting document {} from index {}\",\n        documentId,\n        index,\n        e\n      );\n    }\n    return null;\n  }\n\n  /**\n   * Delete a document by its ID\n   *\n   * @param index      The ID of the index that the document is in\n   * @param documentId The ID of the document to delete\n   * @return The delete response\n   */\n  public DeleteResponse deleteDocument(\n    final String index,\n    final String documentId\n  ) {\n    try {\n      log.info(\"Elasticsearch | deleteDocument | {} | {}\", index, documentId);\n      return client.delete(deleteRequest ->\n        deleteRequest.index(index).id(documentId)\n      );\n    } catch (final IOException e) {\n      log.error(\n        \"Error deleting document {} from index {}\",\n        documentId,\n        index,\n        e\n      );\n    }\n    return null;\n  }\n\n  /**\n   * Search for documents using a {@link MsearchRequest} instance. It is up to the caller to ensure that the\n   * order of the initial queries and the order of the responses match\n   *\n   * @param request The msearch request\n   * @param clazz   The class of the documents to return\n   * @param <T>     The type of the documents to return\n   * @return The msearch response\n   */\n  public <T> MsearchResponse<T> msearch(\n    final MsearchRequest request,\n    final Class<T> clazz\n  ) {\n    try {\n      log.info(\"Elasticsearch | MQuery | {}\", request);\n      return client.msearch(request, clazz);\n    } catch (final IOException e) {\n      log.error(\"Error searching index {}\", request.index(), e);\n    }\n    return null;\n  }\n\n  /**\n   * Scroll through the given index and produce batches of documents.  Example:\n   * <pre>\n   *   final SearchRequest request = SearchRequest.of(b -> b\n   *    .index(\"books\")\n   *    .query(Query.of(q -> q.matchAll(m -> m)))\n   *    .sort(Sort.of(s -> s.field(\"title\")))\n   *    .size(100));\n   *\n   *  elasticsearchService.scroll(request, Book.class, batch -> {\n   *     batch.forEach(book -> // do something with the book);\n   *   });\n   * </pre>\n   * <p>\n   * <p>\n   * NOTE: The scroll API is not implemented in the Java client, so we have to use the rest client to implement this. The\n   * recommended way of using search_after only works when you have a sort field that is unique across all documents.\n   * Since we cannot assume this for all datasets, we have to use the scroll API.\n   *\n   * @param request  The {@link SearchRequest} to use for the scroll. The size field indicates the number of documents\n   *                 to return in each batch\n   * @param clazz    The class of the documents to return\n   * @param producer The producer to call with each batch of documents and for error handling\n   * @param <T>      The type of the documents to return\n   */\n  public <T> void scroll(\n    final SearchRequest request,\n    final Class<T> clazz,\n    final BatchProducer<List<Hit<T>>> producer\n  ) throws IOException {\n    if (request.sort() == null || request.sort().isEmpty()) {\n      throw new IllegalArgumentException(\"Scrolling requires a sort field\");\n    }\n\n    // Scroll through the documents\n    final AtomicReference<String> scrollIdRef = new AtomicReference<>();\n    boolean done = false;\n    do {\n      final int size = request.size() != null ? request.size() : 100;\n      final SearchRequest.Builder copy = new SearchRequest.Builder()\n        .size(size)\n        .sort(request.sort())\n        .index(request.index())\n        .scroll(t -> t.time(SCROLL_TIME))\n        .query(\n          request.query() != null\n            ? request.query()\n            : Query.of(q -> q.matchAll(m -> m))\n        );\n\n      if (request.source() != null) {\n        copy.source(request.source());\n      }\n\n      final ResponseBody<T> response;\n      if (scrollIdRef.get() == null) {\n        response = getClient().search(copy.build(), clazz);\n      } else {\n        response = getClient()\n          .scroll(\n            ScrollRequest.of(b ->\n              b.scrollId(scrollIdRef.get()).scroll(s -> s.time(SCROLL_TIME))\n            ),\n            clazz\n          );\n      }\n\n      final List<Hit<T>> batch = response.hits().hits().stream().toList();\n      producer.onBatch(batch);\n      if (response.hits().hits().size() < size) {\n        done = true;\n        if (scrollIdRef.get() != null) {\n          getClient().clearScroll(b -> b.scrollId(scrollIdRef.get()));\n        }\n      } else {\n        scrollIdRef.set(response.scrollId());\n      }\n    } while (!done);\n  }\n\n  /**\n   * Counts the number of documents in the given index\n   *\n   * @param index The index to count\n   * @return The number of documents in the index\n   */\n  public Long count(final String index) {\n    return count(index, Query.of(q -> q.matchAll(m -> m)));\n  }\n\n  /**\n   * Count the number of documents in the given index that match the given query\n   *\n   * @param index The index to search\n   * @param query The query to execute\n   * @return The number of documents that match the query\n   */\n  public Long count(final String index, final Query query) {\n    final SearchResponse<Object> response = search(index, query, Object.class);\n    if (response != null) {\n      final TotalHits totalHits = response.hits().total();\n      if (totalHits != null) {\n        return totalHits.value();\n      }\n    }\n    return null;\n  }\n\n  /**\n   * Delete the given index\n   *\n   * @param index The index to delete\n   * @return The optional delete index response. If the delete index fails,\n   */\n  public DeleteIndexResponse deleteIndex(final String index) {\n    try {\n      return client.indices().delete(d -> d.index(index));\n    } catch (final IOException e) {\n      log.error(\"Error deleting index {}\", index, e);\n    }\n    return null;\n  }\n\n  /**\n   * Open the given index\n   *\n   * @param index The index to open\n   * @return The acknowledged response\n   */\n  public AcknowledgedResponse openIndex(final String index) {\n    try {\n      return client.indices().open(d -> d.index(index));\n    } catch (final IOException e) {\n      log.error(\"Error opening index {}\", index, e);\n    }\n    return null;\n  }\n\n  /**\n   * Close the given index\n   *\n   * @param index The index to close\n   * @return The acknowledged response\n   */\n  public AcknowledgedResponse closeIndex(final String index) {\n    try {\n      return client.indices().close(d -> d.index(index));\n    } catch (final IOException e) {\n      log.error(\"Error closing index {}\", index, e);\n    }\n    return null;\n  }\n\n  /**\n   * Refresh the given index\n   *\n   * @param index The index to refresh\n   */\n  public void refresh(final String index) {\n    try {\n      client.indices().refresh(r -> r.index(index).ignoreUnavailable(true));\n    } catch (final IOException e) {\n      log.error(\"Error refreshing index {}\", index, e);\n    }\n  }\n\n  /**\n   * Insert the given documents into the given index with the given ids\n   *\n   * @param index     The index to insert the documents into\n   * @param documents the documents to insert\n   * @param ids       The ids of the documents to insert, parallel to the documents\n   * @return The bulk response\n   * @throws IOException              If the bulk insert fails\n   * @throws IllegalArgumentException If the number of documents and ids are not the same\n   */\n  public BulkResponse insert(\n    final String index,\n    final List documents,\n    final List<String> ids\n  ) throws IOException {\n    if (ids != null && documents.size() != ids.size()) {\n      throw new IllegalArgumentException(\n        \"The number of documents and ids must be the same\"\n      );\n    }\n\n    final List<BulkOperation> bulkOperations = new ArrayList<>();\n    for (int i = 0; i < documents.size(); i++) {\n      final String id = ids != null ? ids.get(i) : UUID.randomUUID().toString();\n      final Object document = documents.get(i);\n      bulkOperations.add(\n        BulkOperation.of(b ->\n          b.index(op -> op.index(index).id(id).document(document))\n        )\n      );\n    }\n\n    log.info(\n      \"Elasticsearch | Bulk | Inserting {} documents into index {}\",\n      documents.size(),\n      index\n    );\n    return client.bulk(\n      BulkRequest.of(bulkRequest ->\n        bulkRequest\n          .index(index)\n          .operations(bulkOperations)\n          .pipeline(\"full_text_search\")\n      )\n    );\n  }\n\n  /**\n   * Insert the given documents into the given index\n   *\n   * @param index\n   * @param documents\n   * @return\n   * @throws IOException\n   */\n  public BulkResponse insert(final String index, final List documents)\n    throws IOException {\n    return insert(index, documents, null);\n  }\n\n  /**\n   * Upsert the given documents into the given index with the given ids\n   *\n   * @param index     The index to upsert the documents into\n   * @param documents The documents to upsert\n   * @param ids       The ids of the documents to upsert, parallel to the documents\n   * @return The bulk response\n   */\n  public BulkResponse upsert(\n    final String index,\n    final List documents,\n    final List<String> ids\n  ) {\n    if (ids != null && documents.size() != ids.size()) {\n      throw new IllegalArgumentException(\n        \"The number of documents and ids must be the same\"\n      );\n    }\n\n    final List<BulkOperation> bulkOperations = new ArrayList<>();\n    for (int i = 0; i < documents.size(); i++) {\n      final String id = ids != null ? ids.get(i) : UUID.randomUUID().toString();\n      final Object document = documents.get(i);\n      bulkOperations.add(\n        BulkOperation.of(b ->\n          b.update(op ->\n            op\n              .index(index)\n              .id(id)\n              .action(ua -> ua.doc(document).docAsUpsert(true))\n          )\n        )\n      );\n    }\n\n    log.info(\n      \"Elasticsearch | Bulk | Upserting {} documents into index {}\",\n      documents.size(),\n      index\n    );\n\n    BulkResponse bulkResponse = null;\n    try {\n      // Perform the upsert operation\n      bulkResponse = client.bulk(\n        BulkRequest.of(bulkRequest ->\n          bulkRequest.index(index).operations(bulkOperations)\n        )\n      );\n\n      // Refresh to ensure documents are searchable immediately\n      refresh(index);\n\n      if (ids != null && !ids.isEmpty()) {\n        refreshFullTextSearchForDocuments(index, ids);\n      }\n    } catch (final IOException e) {\n      log.error(\"Error upserting documents\", e);\n    }\n    return bulkResponse;\n  }\n\n  /**\n   * Helper method to refresh full_text_search for specific documents\n   */\n  private void refreshFullTextSearchForDocuments(\n    final String index,\n    final List<String> documentIds\n  ) {\n    try {\n      // Build query to match only the specified document IDs\n      final String idsQuery = String.format(\n        \"{\\\"ids\\\": {\\\"values\\\": [%s]}}\",\n        documentIds\n          .stream()\n          .map(id -> \"\\\"\" + id + \"\\\"\")\n          .collect(Collectors.joining(\", \"))\n      );\n\n      // Use updateByQuery with the full_text_search pipeline\n      String taskId = updateByQuery(index, idsQuery, \"full_text_search\");\n\n      log.info(\n        \"Started full_text_search refresh for {} documents, task: {}\",\n        documentIds.size(),\n        taskId\n      );\n    } catch (Exception e) {\n      log.error(\n        \"Failed to refresh full_text_search for upserted documents: {}\",\n        e.getMessage()\n      );\n    }\n  }\n\n  /**\n   * Create a new index with the given name and default replicas and shards\n   *\n   * @param name The name of the index\n   * @return {@link CreateIndexResponse}\n   */\n  public CreateIndexResponse createIndex(final String name) {\n    return createIndex(name, 0, 1);\n  }\n\n  /**\n   * Create the given index\n   *\n   * @param name The index name\n   * @return {@link CreateIndexResponse}\n   */\n  public CreateIndexResponse createIndex(\n    final String name,\n    final int replicas,\n    final int shards\n  ) {\n    log.info(\n      \"Elasticsearch | Create index {} ({} shards, {} replicas\",\n      name,\n      shards,\n      replicas\n    );\n    try {\n      return client\n        .indices()\n        .create(c ->\n          c\n            .index(name)\n            .settings(sb ->\n              sb.numberOfReplicas(replicas + \"\").numberOfShards(shards + \"\")\n            )\n        );\n    } catch (final IOException e) {\n      log.error(\"Error creating index {}\", name, e);\n    }\n    return null;\n  }\n\n  /**\n   * Checks if the given index exists\n   *\n   * @param name The index name\n   * @return The boolean response, empty if the call fails\n   */\n  public Boolean hasIndex(final String name) {\n    try {\n      return client.indices().exists(e -> e.index(name)).value();\n    } catch (final IOException e) {\n      log.error(\"Error checking if index exists {}\", name, e);\n    }\n    return null;\n  }\n\n  /**\n   * Put an index template to the cluster\n   *\n   * @param name         The name of the index template\n   * @param templateJson The index template json string\n   * @return True if the index template was successfully added, false otherwise\n   */\n  public boolean putIndexTemplate(\n    final String name,\n    final String templateJson\n  ) {\n    return putTyped(\n      name,\n      appendHash(templateJson),\n      \"index template\",\n      \"_index_template\"\n    );\n  }\n\n  /**\n   * Delete the index template with the given name\n   *\n   * @param name\n   * @return true if the index template was successfully deleted, false otherwise\n   */\n  public boolean deleteIndexTemplate(final String name) {\n    try {\n      final AcknowledgedResponse response = client\n        .indices()\n        .deleteIndexTemplate(d -> d.name(name));\n      return response.acknowledged();\n    } catch (final IOException e) {\n      log.error(\"Error deleting index template {}\", name, e);\n    }\n    return false;\n  }\n\n  /**\n   * Returns true if the ES cluster contains the index template with the provided name, false otherwise\n   *\n   * @param name The name of the index template to check existence for\n   * @return True if the index template is contained in the cluster, false otherwise\n   */\n  public boolean containsIndexTemplate(final String name) {\n    final ExistsIndexTemplateRequest request = ExistsIndexTemplateRequest.of(\n      b -> b.name(name)\n    );\n    try {\n      return client.indices().existsIndexTemplate(request).value();\n    } catch (final IOException e) {\n      log.error(\"Error checking existence of template {}\", name, e);\n    }\n    return false;\n  }\n\n  /**\n   * Put a template to the cluster\n   *\n   * @param name         The name of the template\n   * @param templateJson The template json string\n   * @return True if the template was succesfully added, false otherwise\n   */\n  public boolean putTemplate(final String name, final String templateJson) {\n    return putTyped(name, templateJson, \"template\", \"_template\");\n  }\n\n  /**\n   * Returns a list of template names that match the given index based on the pattern specified\n   * in the template\n   *\n   * @param index           The index to match templates for\n   * @param templatePattern The (optional) pattern to filter template names against\n   * @return The list of matching template names\n   */\n  public List<String> getMatchingTemplates(\n    final String index,\n    final String templatePattern\n  ) {\n    final List<String> matching = new ArrayList<>();\n    final Map<String, List<String>> templateIndexPatterns =\n      getTemplateIndexPatterns();\n    final Pattern templateNamePattern = Pattern.compile(\n      templatePattern != null ? templatePattern.replace(\"*\", \".*\") : \".*\"\n    );\n    templateIndexPatterns.forEach((templateName, indexPatterns) -> {\n      if (templateNamePattern.matcher(templateName).matches()) {\n        indexPatterns\n          .stream()\n          .map(pattern -> pattern.replace(\"*\", \".*\"))\n          .forEach(pattern -> {\n            if (Pattern.compile(pattern).matcher(index).matches()) {\n              matching.add(templateName);\n            }\n          });\n      }\n    });\n    return matching;\n  }\n\n  /**\n   * Returns a map of template names to the index patterns they match\n   *\n   * @return The map of template names to index patterns\n   */\n  public Map<String, List<String>> getTemplateIndexPatterns() {\n    final Map<String, List<String>> templateIndexPatterns = new HashMap<>();\n    try {\n      final GetIndexTemplateResponse response = client\n        .indices()\n        .getIndexTemplate();\n      response\n        .indexTemplates()\n        .forEach(indexTemplateItem ->\n          templateIndexPatterns.put(\n            indexTemplateItem.name(),\n            indexTemplateItem.indexTemplate().indexPatterns()\n          )\n        );\n    } catch (final IOException e) {\n      log.error(\"Error getting templates\", e);\n    }\n    return templateIndexPatterns;\n  }\n\n  /**\n   * Returns true if the ES cluster contains a template that matches the provided index, false otherwise\n   *\n   * @param index The index to check for matching templates\n   * @return True if the cluster contains a matching template, false otherwise\n   */\n  public boolean hasMatchingTemplate(\n    final String index,\n    final String templatePattern\n  ) {\n    return !getMatchingTemplates(index, templatePattern).isEmpty();\n  }\n\n  /**\n   * Checks for existence of a component template with the given name\n   *\n   * @param name The name of the component template to check for\n   * @return True if the cluster contains a matching component template, false otherwise\n   */\n  public boolean componentTemplateExists(final String name) {\n    try {\n      final ExistsComponentTemplateRequest request =\n        ExistsComponentTemplateRequest.of(b -> b.name(name));\n      return client.cluster().existsComponentTemplate(request).value();\n    } catch (final IOException e) {\n      return false;\n    }\n  }\n\n  //checks if a pipeline exists\n  public boolean pipelineExists(final String name) {\n    try {\n      final GetPipelineResponse response = client\n        .ingest()\n        .getPipeline(GetPipelineRequest.of(b -> b.id(name)));\n      return response != null && response.result().containsKey(name);\n    } catch (final IOException e) {\n      log.error(\"Error checking if pipeline exists\", e);\n      return false;\n    }\n  }\n\n  //checks if a pipeline exists and is a specific version\n  public boolean pipelineExists(final String name, final long version) {\n    try {\n      final GetPipelineResponse response = client\n        .ingest()\n        .getPipeline(GetPipelineRequest.of(b -> b.id(name)));\n\n      if (response != null) {\n        final var pipeline = response.result().get(name);\n        if (pipeline != null) {\n          return version == pipeline.version();\n        }\n      }\n    } catch (final IOException e) {\n      log.error(\"Error checking if pipeline exists\", e);\n    }\n    return false;\n  }\n\n  public boolean putPipeline(\n    final String name,\n    final InputStream pipelineJson\n  ) {\n    try {\n      client.ingest().putPipeline(b -> b.id(name).withJson(pipelineJson));\n      return true;\n    } catch (final Exception e) {\n      log.error(\"Error putting pipeline\", e);\n      return false;\n    }\n  }\n\n  public boolean deletePipeline(final String name) {\n    try {\n      client.ingest().deletePipeline(b -> b.id(name));\n      return true;\n    } catch (final IOException e) {\n      log.error(\"Error deleting pipeline\", e);\n      return false;\n    }\n  }\n\n  /**\n   * Checks for existence of an index template with the given name\n   *\n   * @param name The name of the index template to check for\n   * @return True if the cluster contains a matching index template, false otherwise\n   */\n  public boolean indexTemplateExists(final String name) {\n    try {\n      final ExistsIndexTemplateRequest request = ExistsIndexTemplateRequest.of(\n        b -> b.name(name)\n      );\n      return client.indices().existsIndexTemplate(request).value();\n    } catch (final IOException e) {\n      return false;\n    }\n  }\n\n  public static String generateTemplateName(final String json) {\n    try {\n      // Compute SHA-256 hash of the JSON string\n      final MessageDigest digest = MessageDigest.getInstance(\"SHA-256\");\n      final byte[] hash = digest.digest(json.getBytes(StandardCharsets.UTF_8));\n      // Convert the byte array to a hexadecimal string\n      final StringBuilder hexString = new StringBuilder();\n      for (final byte b : hash) {\n        final String hex = Integer.toHexString(0xff & b);\n        if (hex.length() == 1) hexString.append('0');\n        hexString.append(hex);\n      }\n      // Prefix the hexadecimal string with a project-specific name\n      return hexString.toString();\n    } catch (final NoSuchAlgorithmException e) {\n      throw new RuntimeException(\"SHA-256 hashing algorithm not available\", e);\n    }\n  }\n\n  public static String generateIndexName(\n    final String prefix,\n    final String templateName\n  ) {\n    return prefix + \"_\" + templateName;\n  }\n\n  private boolean putTyped(\n    final String name,\n    final String typedJson,\n    final String typeName,\n    final String indexName\n  ) {\n    log.info(\"Putting \" + typeName + \": {}\", name);\n\n    try {\n      final HttpHeaders headers = new HttpHeaders();\n      headers.setContentType(MediaType.APPLICATION_JSON);\n      final HttpEntity<String> entity = new HttpEntity<>(typedJson, headers);\n      final ResponseEntity<JsonNode> response = getRestTemplate()\n        .exchange(\n          new URI(url + \"/\" + indexName + \"/\" + name),\n          HttpMethod.PUT,\n          entity,\n          JsonNode.class\n        );\n      final JsonNode body = response.getBody();\n      if (body != null) {\n        return body.at(\"/acknowledged\").asBoolean();\n      }\n    } catch (final Exception e) {\n      log.error(\"Error putting \" + typeName + \" {}\", name, e);\n    }\n    return false;\n  }\n\n  public boolean putComponentTemplate(\n    final String name,\n    final String templateJson\n  ) {\n    return putTyped(\n      name,\n      appendHash(templateJson),\n      \"component template\",\n      \"_component_template\"\n    );\n  }\n\n  private String appendHash(String templateJson) {\n    try {\n      final String hash = hash(templateJson);\n      final ObjectNode templateNode = (ObjectNode) mapper.readTree(\n        templateJson\n      );\n      ObjectNode metaNode;\n      if (templateNode.has(\"_meta\") && templateNode.get(\"_meta\").isObject()) {\n        metaNode = (ObjectNode) templateNode.get(\"_meta\");\n      } else {\n        metaNode = templateNode.putObject(\"_meta\");\n      }\n      metaNode.put(\"hash\", hash);\n      return mapper.writeValueAsString(templateNode);\n    } catch (final JsonProcessingException e) {\n      throw new RuntimeException(e);\n    }\n  }\n\n  public JsonNode getTemplate(final String name, final TEMPLATE_TYPE type) {\n    try {\n      final ResponseEntity<JsonNode> response = getRestTemplate()\n        .exchange(\n          new URI(url + \"/\" + type.getEndpoint() + \"/\" + name),\n          HttpMethod.GET,\n          null,\n          JsonNode.class\n        );\n      return response.getBody().at(type.getJacksonAccessor());\n    } catch (URISyntaxException e) {\n      log.error(\"Failed to parse URI\", e);\n      return null;\n    } catch (Exception e) {\n      log.error(\"Error getting template {}\", name, e);\n      return null;\n    }\n  }\n\n  /**\n   * Put a template to the cluster\n   *\n   * @param name         The name of the template\n   * @param templateNode The template json\n   * @return True if the template was succesfully added, false otherwise\n   */\n  public boolean putTemplate(final String name, final JsonNode templateNode) {\n    try {\n      return putTemplate(name, mapper.writeValueAsString(templateNode));\n    } catch (final JsonProcessingException e) {\n      log.error(\"Could not serialize templateNode to json\", e);\n    }\n    return false;\n  }\n\n  /**\n   * Find the structure of the given text\n   *\n   * @param text List of JsonNode objects to find the structure of\n   * @return JsonNode with the response from the find_structure endpoint\n   */\n  public JsonNode findStructure(final List<JsonNode> text) {\n    //use the field mapping api to get the structure of the index;\n    try {\n      //convert jsonnode list to one jsonlines string\n      final String jsonLines = text\n        .stream()\n        .filter(node -> !node.isEmpty())\n        .map(JsonNode::toString)\n        .collect(Collectors.joining(\"\\n\"));\n\n      final HttpHeaders headers = new HttpHeaders();\n      headers.setContentType(MediaType.APPLICATION_JSON);\n      final HttpEntity<String> entity = new HttpEntity<>(jsonLines, headers);\n      final ResponseEntity<JsonNode> response = getRestTemplate()\n        .exchange(\n          new URI(url + \"/_text_structure/find_structure\"),\n          HttpMethod.POST,\n          entity,\n          JsonNode.class\n        );\n      return response.getBody();\n    } catch (final Exception e) {\n      log.error(\"Error finding structure\", e);\n      return null;\n    }\n  }\n\n  /**\n   * Get the index information from the cluster\n   *\n   * @return The index information\n   */\n  public GetIndexResponse getIndices() {\n    return getIndices(\"*\");\n  }\n\n  /**\n   * Get the index information from the cluster\n   *\n   * @param indexPattern The pattern to match indices against\n   * @return The index information\n   */\n  public GetIndexResponse getIndices(final String indexPattern) {\n    try {\n      return client\n        .indices()\n        .get(b ->\n          b\n            .index(indexPattern)\n            .expandWildcards(\n              List.of(ExpandWildcard.Open, ExpandWildcard.Closed)\n            )\n        );\n    } catch (final IOException e) {\n      log.error(\"Error getting indices\", e);\n    }\n    return null;\n  }\n\n  /**\n   * Get the indices stats for all open and close indices\n   *\n   * @return The indices stats response\n   */\n  public IndicesStatsResponse getIndicesStats() {\n    return getIndicesStats(\"*\");\n  }\n\n  /**\n   * Get the {@link IndicesStatsResponse} for the given index pattern.  See <a href=\"https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-stats.html\">here</a>\n   *\n   * @param indexPattern The index pattern to get stats for\n   * @return The indices stats response\n   */\n  public IndicesStatsResponse getIndicesStats(final String indexPattern) {\n    try {\n      return client\n        .indices()\n        .stats(b ->\n          b\n            .index(indexPattern)\n            .forbidClosedIndices(false)\n            .expandWildcards(\n              List.of(ExpandWildcard.Open, ExpandWildcard.Closed)\n            )\n        );\n    } catch (final IOException e) {\n      log.error(\"Error getting indices stats\", e);\n    }\n    return null;\n  }\n\n  /**\n   * Get the index health from the cluster\n   *\n   * @return The index health\n   */\n  public HealthResponse getHealth() {\n    return getHealth(null);\n  }\n\n  /**\n   * Get the index health from the cluster\n   *\n   * @param indices The indices to get health for. If null, all indices are returned\n   * @return The index health\n   */\n  public HealthResponse getHealth(final List<String> indices) {\n    try {\n      return client\n        .cluster()\n        .health(builder -> {\n          builder.level(Level.Indices);\n          if (indices != null) {\n            builder.index(indices);\n          } else {\n            builder.index(\"*\");\n          }\n          return builder;\n        });\n    } catch (final IOException e) {\n      log.error(\"Error getting index health\", e);\n    }\n    return null;\n  }\n\n  /**\n   * Get the node information from the cluster for the given.  See <a href=\"https://www.elastic.co/guide/en/elasticsearch/reference/current/cluster-nodes-stats.html#cluster-nodes-stats-api-path-params\">here</a>\n   * for a list of available metrics\n   *\n   * @param metrics The metrics to get for the nodes statistics\n   * @return The raw {@link NodesStatsResponse} object\n   */\n  public NodesStatsResponse getNodeStats(final List<String> metrics) {\n    try {\n      return client.nodes().stats(builder -> builder.metric(metrics));\n    } catch (final IOException e) {\n      log.error(\"Error getting node stats\", e);\n    }\n    return null;\n  }\n\n  /**\n   * Get the node information from the cluster based on the {@link NodeRole} type. This is useful for determining\n   * what type of nodes are in the cluster and can be used to infer the capabilities of the cluster\n   *\n   * @return The number of nodes matching the given role\n   */\n  public int getNodeCount(final NodeRole role) {\n    final NodesStatsResponse response = getNodeStats(List.of(\"process\"));\n    if (response != null) {\n      return (int) response\n        .nodes()\n        .values()\n        .stream()\n        .filter(node -> node.roles().contains(role))\n        .count();\n    }\n    return 0;\n  }\n\n  /**\n   * Get the property names for the given index. This is useful for determining the exact fields that are written to the index\n   * when the template defines dynamic path matched fields. We ignore fields that start with an underscore as they\n   * are internal to Elasticsearch\n   *\n   * @param index The index to get the mappings for\n   * @return The property names in the index\n   */\n  public List<String> getAllPropertyNames(final String index) {\n    return getAllPropertyNames(index, \"*\");\n  }\n\n  /**\n   * Get the property names for the given index and fields. This is useful for determining the exact fields that are written to the index\n   * when the template defines dynamic path matched fields. We ignore fields that start with an underscore as they\n   * are internal to Elasticsearch\n   *\n   * @param index  The index to get the mappings for\n   * @param fields The field pattern to get the mappings for\n   * @return\n   */\n  public List<String> getAllPropertyNames(\n    final String index,\n    final String fields\n  ) {\n    try {\n      return client\n        .indices()\n        .getFieldMapping(b -> b.index(index).fields(fields))\n        .get(index)\n        .mappings()\n        .keySet()\n        .stream()\n        .filter(key -> !key.startsWith(\"_\"))\n        .collect(Collectors.toList());\n    } catch (final IOException e) {\n      log.error(\"Error getting all mappings\", e);\n    }\n    return null;\n  }\n\n  /**\n   * @param sourceIndex          the index to enrich\n   * @param sourceField          the field to use for enriching\n   * @param includedSourceFields the fields to include in the enrich policy\n   * @return the name of the enrich policy\n   */\n  public String createEnrichPolicy(\n    final String sourceIndex,\n    final String sourceField,\n    final List<String> includedSourceFields\n  ) {\n    final String enrichPolicyName = UUID.nameUUIDFromBytes(\n      (sourceIndex +\n        sourceField +\n        includedSourceFields\n          .stream()\n          .sorted()\n          .collect(Collectors.joining(\" \"))).getBytes()\n    ).toString();\n    if (enrichPolicyExists(enrichPolicyName)) {\n      return enrichPolicyName;\n    }\n\n    final PutPolicyRequest putPolicyRequest = PutPolicyRequest.of(b ->\n      b\n        .name(enrichPolicyName)\n        .match(mb ->\n          mb\n            .matchField(sourceField)\n            .indices(List.of(sourceIndex))\n            .enrichFields(includedSourceFields)\n        )\n    );\n    try {\n      client.enrich().putPolicy(putPolicyRequest);\n    } catch (final IOException e) {\n      log.error(\"Error creating enrich policy\", e);\n      return null;\n    }\n    return enrichPolicyName;\n  }\n\n  /**\n   * Check if an enrich policy with the given name exists\n   *\n   * @param name the name of the enrich policy to check\n   * @return true if the enrich policy exists, false otherwise\n   */\n  public boolean enrichPolicyExists(final String name) {\n    return getEnrichPolicy(name).policies().stream().findFirst().isPresent();\n  }\n\n  /**\n   * Get all enrich policies\n   *\n   * @return the enrich policies response\n   */\n  public GetPolicyResponse getEnrichPolicies() {\n    try {\n      return client.enrich().getPolicy(GetPolicyRequest.of(b -> b));\n    } catch (final IOException e) {\n      log.error(\"Error getting enrich policies\", e);\n      return null;\n    }\n  }\n\n  /**\n   * Gets an enrichment policy by name\n   *\n   * @param name the name of the enrich policy to get\n   * @return the enrich policy response\n   */\n  public GetPolicyResponse getEnrichPolicy(final String name) {\n    try {\n      final GetPolicyRequest request = GetPolicyRequest.of(b -> b.name(name));\n      return client.enrich().getPolicy(request);\n    } catch (final IOException e) {\n      log.error(\"Error getting enrich policy\", e);\n      return null;\n    }\n  }\n\n  /**\n   * Delete the enrich policy with the given name\n   *\n   * @param name the name of the enrich policy to delete\n   * @return true if the enrich policy deletion request was acknowledged, false otherwise\n   */\n  public boolean deleteEnrichPolicy(final String name) {\n    try {\n      return client.enrich().deletePolicy(b -> b.name(name)).acknowledged();\n    } catch (final IOException e) {\n      log.error(\"Error deleting enrich policy\", e);\n    }\n    return false;\n  }\n\n  /**\n   * Get the running enrich stats. There is no way to get historical enrich stats as this is currently an open\n   * issue in Elasticsearch. See <a href=\"https://github.com/elastic/elasticsearch/issues/73353\">here</a>\n   *\n   * @return the list of running enrich stats\n   */\n  public List<ExecutingPolicy> getRunningEnrichStats() {\n    try {\n      log.info(\"Getting enrich stats\");\n      final EnrichStatsResponse response = client.enrich().stats();\n      return response.executingPolicies();\n    } catch (final IOException e) {\n      log.error(\"Error getting enrich stats\", e);\n    }\n    return null;\n  }\n\n  /**\n   * Execute the enrich policy with the given name. This needs to be done after the policy is created and anytime the data\n   * in the source index changes. Use the rest template to execute the policy as the client does not return the task id\n   *\n   * @param enrichPolicyName the name of the enrich policy to execute\n   * @return the task id of the enrich policy execution\n   */\n  public String executeEnrichPolicy(final String enrichPolicyName) {\n    try {\n      final HttpHeaders headers = new HttpHeaders();\n      headers.setContentType(MediaType.APPLICATION_JSON);\n      final HttpEntity<String> entity = new HttpEntity<>(null, headers);\n      final ResponseEntity<JsonNode> response = getRestTemplate()\n        .exchange(\n          new URI(\n            url +\n            \"/_enrich/policy/\" +\n            enrichPolicyName +\n            \"/_execute?wait_for_completion=false\"\n          ),\n          HttpMethod.POST,\n          entity,\n          JsonNode.class\n        );\n      return response.getBody().at(\"/task\").asText();\n    } catch (final Exception e) {\n      log.error(\"Error executing enrich policy\", e);\n    }\n    return null;\n  }\n\n  /**\n   * Create an enrich pipeline that uses the given enrich policy to enrich the data in the target field\n   *\n   * @param enrichPolicy    the name of the enrich policy to use\n   * @param targetJoinField the field in the index to use for joining the enrichment data\n   * @return the name of the enrich pipeline\n   */\n  public String createEnrichPipeline(\n    final String enrichPolicy,\n    final String targetJoinField\n  ) {\n    return createEnrichPipeline(enrichPolicy, targetJoinField, Map.of());\n  }\n\n  /**\n   * Create an enrich pipeline that uses the given enrich policy to enrich the data in the target field\n   *\n   * @param enrichPolicy    the name of the enrich policy to use\n   * @param targetJoinField the field in the index to use for joining the enrichment data\n   * @param renameFields    a map from original to new field names that are written to enrich_temp\n   * @return the name of the enrich pipeline\n   */\n  public String createEnrichPipeline(\n    final String enrichPolicy,\n    final String targetJoinField,\n    final Map<String, String> renameFields\n  ) {\n    final String enrichPipelineName = UUID.nameUUIDFromBytes(\n      (enrichPolicy + targetJoinField).getBytes()\n    ).toString();\n    if (pipelineExists(enrichPipelineName)) {\n      return enrichPipelineName;\n    }\n\n    final GetPolicyResponse response = getEnrichPolicy(enrichPolicy);\n    if (response == null) {\n      log.error(\"Enrich policy {} does not exist\", enrichPolicy);\n      return null;\n    }\n\n    // Pull out the enrich policy and create the pipeline\n    final EnrichSummary summary = response\n      .policies()\n      .stream()\n      .findFirst()\n      .orElseThrow();\n    final EnrichPolicy policy = summary.config().value();\n\n    final String description = String.format(\n      \"SELECT %s from <target_index> where %s.%s = <target_index>.%s\",\n      policy.enrichFields(),\n      policy.indices().getFirst(),\n      policy.matchField(),\n      targetJoinField\n    );\n\n    final ObjectNode enrichPolicyNode = mapper.createObjectNode();\n    enrichPolicyNode.put(\"policy_name\", enrichPolicy);\n    enrichPolicyNode.put(\"field\", targetJoinField);\n    enrichPolicyNode.put(\"target_field\", \"enrich_temp\");\n    enrichPolicyNode.put(\"description\", description);\n\n    final ObjectNode enrichPolicyObjectNode = mapper.createObjectNode();\n    enrichPolicyObjectNode.set(\"enrich\", enrichPolicyNode);\n\n    final ObjectNode scriptNode = mapper.createObjectNode();\n    final String src = Arrays.stream(\n      \"\"\"\n      def deduplicate(def arr) {\n        return arr.stream().distinct().collect(Collectors.toList());\n      }\n\n      def mergeIntoArray(def existing, def newValue) {\n          if (existing == null) {\n              return newValue;\n          }\n          if (existing instanceof List) {\n              if (newValue instanceof List) {\n                  existing.addAll(newValue);\n              } else {\n                  existing.add(newValue);\n              }\n          } else {\n              if (newValue instanceof List) {\n                  newValue.add(0, existing);\n                  return newValue;\n              } else {\n                  return [existing, newValue];\n              }\n          }\n          return existing;\n      }\n      void mergeObjects(def original, def enrich) {\n          if (original == null || enrich == null) {\n              return;\n          }\n          for (def entry : enrich.entrySet()) {\n              def key = entry.getKey();\n              def newValue = entry.getValue();\n              if (original.containsKey(key)) {\n                  def existingValue = original[key];\n                  if (existingValue instanceof Map && newValue instanceof Map) {\n                      mergeObjects(existingValue, newValue);\n                  } else {\n                      original[key] = deduplicate(mergeIntoArray(existingValue, newValue));\n                  }\n              } else {\n                  original[key] = newValue;\n              }\n          }\n          return;\n      }\n\n      // Iterate over the enriched data and merge into the document\n      mergeObjects(ctx, ctx.enrich_temp);\n\n      \"\"\".split(\"\\n\")\n    )\n      .map(String::trim)\n      .filter(line -> !line.startsWith(\"//\"))\n      .collect(Collectors.joining(\" \"));\n\n    scriptNode.put(\"source\", src);\n\n    final ObjectNode scriptObjectNode = mapper.createObjectNode();\n    scriptObjectNode.set(\"script\", scriptNode);\n\n    final ObjectNode pipelineRequestNode = mapper.createObjectNode();\n    final ArrayNode processorsNode = pipelineRequestNode.putArray(\"processors\");\n    processorsNode.add(enrichPolicyObjectNode);\n\n    // Rename the temp fields to the final fields\n    for (final Map.Entry<String, String> entry : renameFields.entrySet()) {\n      final ObjectNode renameObjectNode = mapper.createObjectNode();\n      final ObjectNode renameNode = mapper.createObjectNode();\n      renameNode.put(\"field\", \"enrich_temp.\" + entry.getKey());\n      renameNode.put(\"target_field\", \"enrich_temp.\" + entry.getValue());\n      renameNode.put(\"ignore_missing\", true);\n      renameObjectNode.set(\"rename\", renameNode);\n      processorsNode.add(renameObjectNode);\n    }\n\n    processorsNode.add(scriptObjectNode);\n\n    final String json = pipelineRequestNode.toString();\n    final ByteArrayInputStream inputStream = new ByteArrayInputStream(\n      json.getBytes(StandardCharsets.UTF_8)\n    );\n\n    putPipeline(enrichPipelineName, inputStream);\n    return enrichPipelineName;\n  }\n\n  /**\n   * Update the index with the given pipeline\n   *\n   * @param index    the index to update\n   * @param pipeline the pipeline to use\n   * @return the task id of the update\n   */\n  public String updateByQuery(final String index, final String pipeline) {\n    return updateByQuery(index, \"{\\\"match_all\\\": {}}\", pipeline);\n  }\n\n  /**\n   * Update the enrich pipeline with the given name\n   *\n   * @param index    the index to update\n   * @param query    the query to match a subset of the documents in the index\n   * @param pipeline the pipeline to use\n   * @return the task id of the update\n   */\n  public String updateByQuery(\n    final String index,\n    final String query,\n    final String pipeline\n  ) {\n    try {\n      log.info(\n        \"Updating by query for index {} with pipeline {}\",\n        index,\n        pipeline\n      );\n\n      final String jsonPayload = String.format(\"{\\\"query\\\": %s}\", query);\n\n      final HttpHeaders headers = new HttpHeaders();\n      headers.setContentType(MediaType.APPLICATION_JSON);\n      final HttpEntity<String> entity = new HttpEntity<>(jsonPayload, headers);\n\n      // This is executed by the rest client because the Java client appears to have a bug\n      // where the response cannot be deserialized\n      final ResponseEntity<JsonNode> response = getRestTemplate()\n        .exchange(\n          new URI(\n            url +\n            \"/\" +\n            index +\n            \"/_update_by_query?wait_for_completion=false&conflicts=proceed&pipeline=\" +\n            pipeline\n          ),\n          HttpMethod.POST,\n          entity,\n          JsonNode.class\n        );\n\n      // Get the taskId from the response\n      if (response.getBody() != null) {\n        return response.getBody().at(\"/task\").asText();\n      }\n    } catch (final Exception e) {\n      log.error(\"Error updating by query\", e);\n    }\n    return null;\n  }\n\n  /**\n   * Get the task status for the given task id. We search the .tasks index to get running and completed tasks\n   *\n   * @param taskId the task id to get the status for\n   * @return the historical task JSON representation. If the task is not found, an empty JSON object is returned.\n   * If the search fails, null is returned\n   */\n  public JsonNode getHistoricalTask(final String taskId) {\n    final Query query = Query.of(b -> b.ids(idb -> idb.values(taskId)));\n    final SearchRequest request = SearchRequest.of(b ->\n      b.index(\".tasks\").query(query)\n    );\n    final SearchResponse<JsonNode> response = search(request, JsonNode.class);\n    if (response == null) {\n      return null;\n    }\n    if (\n      response.hits() != null &&\n      response.hits().hits() != null &&\n      !response.hits().hits().isEmpty()\n    ) {\n      return response.hits().hits().getFirst().source();\n    }\n    return mapper.createObjectNode();\n  }\n\n  /**\n   * Get the task status for all tasks in the cluster. Returns up to 1000 running and completed tasks\n   *\n   * @return the list of task statuses\n   */\n  public List<JsonNode> getHistoricalTasks() {\n    final Query query = Query.of(b -> b.matchAll(m -> m));\n    final SearchRequest request = SearchRequest.of(b ->\n      b.index(\".tasks\").query(query).size(1000)\n    );\n    final SearchResponse<JsonNode> response;\n    try {\n      response = client.search(request, JsonNode.class);\n    } catch (final ElasticsearchException e) {\n      // The .tasks index may not exist, so we use the client directly to avoid exception log spamming\n      return new ArrayList<>();\n    } catch (final IOException e) {\n      log.error(\"Error getting historical tasks\", e);\n      return new ArrayList<>();\n    }\n    if (\n      response != null &&\n      response.hits() != null &&\n      response.hits().hits() != null &&\n      !response.hits().hits().isEmpty()\n    ) {\n      return response.hits().hits().stream().map(Hit::source).toList();\n    }\n    return new ArrayList<>();\n  }\n\n  /**\n   * Get the running task for the given task id\n   *\n   * @param id the task id to get the running task for\n   * @return the running task JSON representation\n   */\n  public JsonNode getRunningTask(final String id) {\n    // Use the rest client to call the task api\n    try {\n      final ResponseEntity<JsonNode> response = getRestTemplate()\n        .exchange(\n          new URI(url + \"/_tasks/\" + id),\n          HttpMethod.GET,\n          null,\n          JsonNode.class\n        );\n      return response.getBody();\n    } catch (final Exception e) {\n      log.error(\"Error getting running task\", e);\n    }\n    return null;\n  }\n\n  /**\n   * Get the running tasks in the cluster\n   *\n   * @return the list of running tasks\n   */\n  public List<JsonNode> getRunningTasks() {\n    try {\n      final ResponseEntity<JsonNode> response = getRestTemplate()\n        .exchange(\n          new URI(url + \"/_tasks?detailed=true\"),\n          HttpMethod.GET,\n          null,\n          JsonNode.class\n        );\n      final List<JsonNode> tasks = new ArrayList<>();\n      if (response.getBody() != null) {\n        response\n          .getBody()\n          .at(\"/nodes\")\n          .forEach(node -> node.at(\"/tasks\").forEach(tasks::add));\n      }\n      return tasks;\n    } catch (final Exception e) {\n      log.error(\"Error getting running tasks\", e);\n    }\n    return null;\n  }\n\n  /**\n   * Helper function to wait for task completion\n   *\n   * @param taskId         The task id to wait for\n   * @param timeoutSeconds The number of seconds to wait before timing out\n   * @return A completable future that completes when the task is done\n   */\n  public CompletableFuture<Void> waitForTaskCompletion(\n    final String taskId,\n    final long timeoutSeconds\n  ) {\n    final CompletableFuture<Void> future = new CompletableFuture<>();\n    final long startTime = System.currentTimeMillis();\n    new Thread(() -> {\n      while (System.currentTimeMillis() - startTime < timeoutSeconds * 1000) {\n        final JsonNode taskInfo = getHistoricalTask(taskId);\n        // Task is done when it is null as getTaskInfo only reports on running tasks\n        if (taskInfo != null && taskInfo.at(\"/completed\").asBoolean()) {\n          future.complete(null);\n          break;\n        }\n        try {\n          Thread.sleep(500);\n        } catch (final InterruptedException e) {\n          future.completeExceptionally(e);\n        }\n      }\n    }).start();\n    return future;\n  }\n\n  /**\n   * Helper function to wait for enrich task completion\n   *\n   * @param taskId         The task id to wait for\n   * @param timeoutSeconds The number of seconds to wait before timing out\n   * @return A completable future that completes when the task is done\n   */\n  public CompletableFuture<Void> waitForEnrichTaskCompletion(\n    final String taskId,\n    final long timeoutSeconds\n  ) {\n    final CompletableFuture<Void> future = new CompletableFuture<>();\n    final long startTime = System.currentTimeMillis();\n    new Thread(() -> {\n      while (System.currentTimeMillis() - startTime < timeoutSeconds * 1000) {\n        final List<ExecutingPolicy> policies = getRunningEnrichStats();\n        final Set<String> policyIds = policies\n          .stream()\n          .map(policy -> policy.task().node() + \":\" + policy.task().id())\n          .collect(Collectors.toSet());\n\n        if (policyIds.contains(taskId)) {\n          try {\n            Thread.sleep(500);\n          } catch (final InterruptedException e) {\n            future.completeExceptionally(e);\n          }\n        } else {\n          future.complete(null);\n          break;\n        }\n      }\n    }).start();\n    return future;\n  }\n\n  /**\n   * Get all running and historical tasks\n   *\n   * @return a list of {@link Task} objects\n   */\n  public List<Task> getTasks() {\n    final List<Task> historicalTasks = getHistoricalTasks()\n      .stream()\n      .map(source -> {\n        final Task task = new Task();\n        task.setStartTimeMillis(\n          source.at(\"/task/start_time_in_millis\").asLong()\n        );\n        task.setRunningTimeNanos(\n          source.at(\"/task/running_time_in_nanos\").asLong()\n        );\n        task.setCancellable(source.at(\"/task/cancellable\").asBoolean());\n        task.setDescription(source.at(\"/task/description\").asText());\n        task.setCompleted(true);\n        task.setNodeId(source.at(\"/task/node\").asText());\n        task.setTaskId(source.at(\"/task/id\").asLong());\n\n        final JsonNode errorNode = source.at(\"/error\");\n        if (errorNode != null && !errorNode.isEmpty()) {\n          task.setError(errorNode);\n        }\n\n        task.setCancelled(!source.at(\"/response/canceled\").isMissingNode());\n\n        final long total = source.at(\"/task/status/total\").asLong();\n        final long created = source.at(\"/task/status/created\").asLong();\n        final long updated = source.at(\"/task/status/updated\").asLong();\n        final long deleted = source.at(\"/task/status/deleted\").asLong();\n        final long operatedOn = Math.max(created, Math.max(updated, deleted));\n\n        final Double progress = (double) operatedOn / total;\n        task.setProgress(progress);\n        task.setTotal(total);\n        task.setCreated(created);\n        task.setUpdated(updated);\n        task.setDeleted(deleted);\n\n        final String typeStr = source.at(\"/task/action\").asText();\n        return switch (TASK_TYPES.get(typeStr)) {\n          case REINDEX -> {\n            task.setType(REINDEX);\n            yield task;\n          }\n          case DELETE_BY_QUERY -> {\n            task.setType(DELETE_BY_QUERY);\n            yield task;\n          }\n          case UPDATE_BY_QUERY -> {\n            task.setType(UPDATE_BY_QUERY);\n            yield task;\n          }\n          case null -> null;\n        };\n      })\n      .filter(Objects::nonNull)\n      .toList();\n\n    final List<Task> runningTasks = getRunningTasks()\n      .stream()\n      .map(source -> {\n        final Task task = new Task();\n\n        task.setNodeId(source.at(\"/node\").asText());\n        task.setTaskId(source.at(\"/id\").asLong());\n        task.setStartTimeMillis(source.at(\"/start_time_in_millis\").asLong());\n        task.setRunningTimeNanos(source.at(\"/running_time_in_nanos\").asLong());\n        task.setCancellable(source.at(\"/cancellable\").asBoolean());\n        task.setDescription(source.at(\"/description\").asText());\n        task.setCompleted(false);\n\n        task.setTotal(source.at(\"/status/total\").asLong());\n        task.setCreated(source.at(\"/status/created\").asLong());\n        task.setUpdated(source.at(\"/status/updated\").asLong());\n        task.setDeleted(source.at(\"/status/deleted\").asLong());\n\n        final long total = source.at(\"/status/total\").asLong();\n        final long operatedOn = Math.max(\n          task.getCreated(),\n          Math.max(task.getUpdated(), task.getDeleted())\n        );\n        final Double progress = (double) operatedOn / total;\n        task.setProgress(progress);\n\n        final String typeStr = source.at(\"/action\").asText();\n        return switch (TASK_TYPES.get(typeStr)) {\n          case REINDEX -> {\n            task.setType(REINDEX);\n            yield task;\n          }\n          case DELETE_BY_QUERY -> {\n            task.setType(DELETE_BY_QUERY);\n            yield task;\n          }\n          case UPDATE_BY_QUERY -> {\n            task.setType(UPDATE_BY_QUERY);\n            yield task;\n          }\n          case null -> null;\n        };\n      })\n      .filter(Objects::nonNull)\n      .toList();\n\n    final List<Task> tasks = new ArrayList<>();\n    tasks.addAll(historicalTasks);\n    tasks.addAll(runningTasks);\n    return tasks;\n  }\n\n  /**\n   * Delete all historical tasks\n   */\n  public void deleteTasks() {\n    deleteIndex(\".tasks\");\n    refresh(\".tasks\");\n  }\n\n  /**\n   * Delete the task with the given id\n   *\n   * @param nodeId the node id of the task\n   * @param taskId the task id to delete\n   */\n  public void deleteTask(final String nodeId, final Long taskId) {\n    final String id = nodeId + \":\" + taskId;\n    final DeleteRequest request = DeleteRequest.of(b -> b.id(id).index(\".tasks\")\n    );\n    try {\n      client.delete(request);\n      refresh(\".tasks\");\n    } catch (final IOException e) {\n      log.error(\"Error deleting task\", e);\n    }\n  }\n\n  /**\n   * Cancel the task with the given node id and task id\n   *\n   * @param nodeId the node id of the task\n   * @param taskId the task id to cancel\n   */\n  public void cancelTask(final String nodeId, final Long taskId) {\n    final CancelRequest request = CancelRequest.of(b ->\n      b.taskId(nodeId + \":\" + taskId)\n    );\n    try {\n      client.tasks().cancel(request);\n      refresh(\".tasks\");\n    } catch (final IOException e) {\n      log.error(\"Error cancelling task\", e);\n    }\n  }\n\n  public GetMappingResponse getMapping(final String index) {\n    // Get the mapping for the index\n    try {\n      final GetMappingResponse response = client\n        .indices()\n        .getMapping(b -> b.index(index));\n      return response;\n    } catch (final IOException e) {\n      log.error(\"Error getting mapping\", e);\n      return null;\n    }\n  }\n\n  public GetIndexTemplateResponse getIndexTemplate(final String name) {\n    try {\n      return client\n        .indices()\n        .getIndexTemplate(GetIndexTemplateRequest.of(b -> b.name(name)));\n    } catch (final IOException e) {\n      log.error(\"Error getting index template\", e);\n      return null;\n    }\n  }\n\n  /**\n   * Validates that the given template matches the existing template in the cluster.\n   * @param name          the name of the template (index or component)\n   * @param templateJson  the template JSON to validate\n   * @param type          the type of the template (component or index)\n   */\n  public void validateTemplate(\n    String name,\n    JsonNode templateJson,\n    final TEMPLATE_TYPE type\n  ) {\n    final JsonNode existingTemplate = getTemplate(name, type);\n    if (existingTemplate == null || existingTemplate.isEmpty()) {\n      log.error(\"Failed to fetch existing {} {}\", type, name);\n      throw new IllegalStateException(\n        \"Failed to fetch existing \" + type.getName() + \" \" + name\n      );\n    }\n    final String existingTemplateHash = existingTemplate\n      .at(\"/_meta/hash\")\n      .asText();\n    final String templateJsonHash = hash(templateJson.toString());\n    if (!existingTemplateHash.equals(templateJsonHash)) {\n      log.error(\n        \"{}  {} already exists and does not match the expected template.\",\n        type.getName(),\n        name\n      );\n      throw new IllegalStateException(\n        type.getName() +\n        \" \" +\n        name +\n        \" already exists in the cluster and does not match the expected template.\"\n      );\n    } else {\n      log.info(\"{} {} already exists and matches\", type.getName(), name);\n    }\n  }\n\n  /**\n   * Validates that the given component template matches the existing component template in the cluster.\n   * @param name          the name of the component template\n   * @param templateJson  the component template JSON to validate\n   * @throws IllegalStateException if there is a mismatch\n   */\n  public void validateComponentTemplate(\n    final String name,\n    final JsonNode templateJson\n  ) {\n    validateTemplate(name, templateJson, TEMPLATE_TYPE.COMPONENT);\n  }\n\n  /**\n   * Validates that the given index template matches the existing index template in the cluster.\n   * @param name          the name of the index template\n   * @param templateJson  the index template JSON to validate\n   * @throws IllegalStateException if there is a mismatch\n   */\n  public void validateIndexTemplate(\n    final String name,\n    final JsonNode templateJson\n  ) {\n    validateTemplate(name, templateJson, TEMPLATE_TYPE.INDEX);\n  }\n\n  /**\n   * Internal criteria structure for property value deletion\n   */\n  public static class DeletionCriteria {\n\n    public String propertyKey;\n    public String exactValue;\n    public String minValue;\n    public String maxValue;\n\n    public DeletionCriteria(\n      String propertyKey,\n      String exactValue,\n      String minValue,\n      String maxValue\n    ) {\n      this.propertyKey = propertyKey;\n      this.exactValue = exactValue;\n      this.minValue = minValue;\n      this.maxValue = maxValue;\n    }\n  }\n\n  /**\n   * Bulk remove property values from selected documents using multiple criteria with Painless script\n   *\n   * @param index the index to update\n   * @param deletionCriteria list of deletion criteria (exact matches or ranges)\n   * @param documentIds list of document IDs to update\n   * @return the task id of the update operation\n   */\n  public String bulkRemovePropertyValues(\n    final String index,\n    final List<Object> deletionCriteria,\n    final List<String> documentIds\n  ) {\n    if (deletionCriteria == null || deletionCriteria.isEmpty()) {\n      throw new IllegalArgumentException(\n        \"Deletion criteria list cannot be null or empty\"\n      );\n    }\n    if (documentIds == null || documentIds.isEmpty()) {\n      throw new IllegalArgumentException(\n        \"Document IDs list cannot be null or empty\"\n      );\n    }\n\n    try {\n      // Build the update by query request with Painless script\n      final ObjectNode queryNode = mapper.createObjectNode();\n\n      // Create terms query to target specific documents\n      final ObjectNode termsQuery = queryNode\n        .putObject(\"query\")\n        .putObject(\"terms\");\n      final ArrayNode docIdsArray = termsQuery.putArray(\"_id\");\n      documentIds.forEach(docIdsArray::add);\n\n      // Create script to remove property values based on criteria\n      final ObjectNode scriptNode = queryNode.putObject(\"script\");\n      final String painlessScript = buildPainlessScript();\n      scriptNode.put(\"source\", painlessScript);\n\n      // Add deletion criteria as parameters\n      final ObjectNode paramsNode = scriptNode.putObject(\"params\");\n      final ArrayNode criteriaArray = paramsNode.putArray(\"deletionCriteria\");\n\n      for (final Object criteriaObj : deletionCriteria) {\n        final ObjectNode criteriaNode = criteriaArray.addObject();\n\n        // Convert the criteria object to our internal structure using reflection\n        try {\n          final String propertyKey = (String) criteriaObj\n            .getClass()\n            .getMethod(\"getPropertyKey\")\n            .invoke(criteriaObj);\n          criteriaNode.put(\"propertyKey\", propertyKey);\n\n          final String exactValue = (String) criteriaObj\n            .getClass()\n            .getMethod(\"getExactValue\")\n            .invoke(criteriaObj);\n          if (exactValue != null) {\n            criteriaNode.put(\"exactValue\", exactValue);\n          }\n\n          final String minValue = (String) criteriaObj\n            .getClass()\n            .getMethod(\"getMinValue\")\n            .invoke(criteriaObj);\n          if (minValue != null) {\n            criteriaNode.put(\"minValue\", minValue);\n          }\n\n          final String maxValue = (String) criteriaObj\n            .getClass()\n            .getMethod(\"getMaxValue\")\n            .invoke(criteriaObj);\n          if (maxValue != null) {\n            criteriaNode.put(\"maxValue\", maxValue);\n          }\n        } catch (Exception e) {\n          throw new RuntimeException(\n            \"Failed to extract criteria properties\",\n            e\n          );\n        }\n      }\n\n      final String queryJson = queryNode.toString();\n\n      log.info(\n        \"Executing bulk property value removal with {} criteria in {} documents\",\n        deletionCriteria.size(),\n        documentIds.size()\n      );\n\n      // Debug: Log the actual query being sent\n      log.info(\"Update by query JSON: {}\", queryJson);\n\n      // Execute update by query directly with script (no pipeline needed)\n      final String taskId = updateByQueryWithScript(index, queryJson);\n\n      log.info(\n        \"Started bulk property value removal for index {} with task {}\",\n        index,\n        taskId\n      );\n\n      return taskId;\n    } catch (final Exception e) {\n      log.error(\n        \"Error executing bulk property value removal for index {}\",\n        index,\n        e\n      );\n      throw new RuntimeException(\n        \"Failed to execute bulk property value removal\",\n        e\n      );\n    }\n  }\n\n  /**\n   * Build the Painless script for removing properties based on multiple criteria\n   */\n  private String buildPainlessScript() {\n    try {\n      return new String(\n        getClass().getClassLoader()\n          .getResourceAsStream(\"bulk-property-deletion.painless\")\n          .readAllBytes()\n      );\n    } catch (Exception e) {\n      throw new RuntimeException(\"Failed to load Painless script\", e);\n    }\n  }\n\n  /**\n   * Execute update by query with a custom script\n   *\n   * @param index the index to update\n   * @param queryJson the update by query JSON with script\n   * @return the task id of the update operation\n   */\n  private String updateByQueryWithScript(\n    final String index,\n    final String queryJson\n  ) {\n    try {\n      final HttpHeaders headers = new HttpHeaders();\n      headers.setContentType(MediaType.APPLICATION_JSON);\n      final HttpEntity<String> entity = new HttpEntity<>(queryJson, headers);\n\n      final String url =\n        this.url + \"/\" + index + \"/_update_by_query?wait_for_completion=false\";\n      final ResponseEntity<JsonNode> response = getRestTemplate()\n        .exchange(new URI(url), HttpMethod.POST, entity, JsonNode.class);\n\n      final JsonNode body = response.getBody();\n      if (body != null && body.has(\"task\")) {\n        return body.get(\"task\").asText();\n      } else {\n        throw new RuntimeException(\"No task ID returned from update by query\");\n      }\n    } catch (final Exception e) {\n      log.error(\n        \"Error executing update by query with script for index {}\",\n        index,\n        e\n      );\n      throw new RuntimeException(\n        \"Failed to execute update by query with script\",\n        e\n      );\n    }\n  }\n}\n","numLines":2309,"startLine":1,"totalLines":2309}}}
{"parentUuid":"0c2bf8a8-e2b4-4511-aced-139af59418dd","isSidechain":false,"userType":"external","cwd":"/Users/yparis/Developer/atlantis","sessionId":"2b976885-8546-4e96-b019-e86ba8fb0435","version":"1.0.123","gitBranch":"feat-delete-facet-values","message":{"id":"msg_01L63YHXN4HL4r3K3dFbjuuD","type":"message","role":"assistant","model":"claude-sonnet-4-20250514","content":[{"type":"tool_use","id":"toolu_012PxTckdQgxVZxqAsymuRAx","name":"TodoWrite","input":{"todos":[{"content":"Read and review ElasticsearchService.java for QA issues","status":"completed","activeForm":"Reading and reviewing ElasticsearchService.java for QA issues"},{"content":"Read and review ElasticsearchServiceTests.java for QA issues","status":"in_progress","activeForm":"Reading and reviewing ElasticsearchServiceTests.java for QA issues"},{"content":"Review new bulk-property-deletion.painless script","status":"completed","activeForm":"Reviewing new bulk-property-deletion.painless script"},{"content":"Provide comprehensive QA review summary","status":"pending","activeForm":"Providing comprehensive QA review summary"}]}}],"stop_reason":"tool_use","stop_sequence":null,"usage":{"input_tokens":1889,"cache_creation_input_tokens":28790,"cache_read_input_tokens":16274,"cache_creation":{"ephemeral_5m_input_tokens":28790,"ephemeral_1h_input_tokens":0},"output_tokens":221,"service_tier":"standard"}},"requestId":"req_011CTTtRfG4cLGAohsGgmjjM","type":"assistant","uuid":"7015f898-d6ae-49b5-bc8f-5449c29dbdbc","timestamp":"2025-09-24T20:51:51.126Z"}
{"parentUuid":"7015f898-d6ae-49b5-bc8f-5449c29dbdbc","isSidechain":false,"userType":"external","cwd":"/Users/yparis/Developer/atlantis","sessionId":"2b976885-8546-4e96-b019-e86ba8fb0435","version":"1.0.123","gitBranch":"feat-delete-facet-values","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_012PxTckdQgxVZxqAsymuRAx","type":"tool_result","content":"Todos have been modified successfully. Ensure that you continue to use the todo list to track your progress. Please proceed with the current tasks if applicable"}]},"uuid":"3e67ba46-7168-4546-b9a1-697d64cbbed2","timestamp":"2025-09-24T20:51:51.156Z","toolUseResult":{"oldTodos":[{"content":"Read and review ElasticsearchService.java for QA issues","status":"in_progress","activeForm":"Reading and reviewing ElasticsearchService.java for QA issues"},{"content":"Read and review ElasticsearchServiceTests.java for QA issues","status":"pending","activeForm":"Reading and reviewing ElasticsearchServiceTests.java for QA issues"},{"content":"Review new bulk-property-deletion.painless script","status":"pending","activeForm":"Reviewing new bulk-property-deletion.painless script"},{"content":"Provide comprehensive QA review summary","status":"pending","activeForm":"Providing comprehensive QA review summary"}],"newTodos":[{"content":"Read and review ElasticsearchService.java for QA issues","status":"completed","activeForm":"Reading and reviewing ElasticsearchService.java for QA issues"},{"content":"Read and review ElasticsearchServiceTests.java for QA issues","status":"in_progress","activeForm":"Reading and reviewing ElasticsearchServiceTests.java for QA issues"},{"content":"Review new bulk-property-deletion.painless script","status":"completed","activeForm":"Reviewing new bulk-property-deletion.painless script"},{"content":"Provide comprehensive QA review summary","status":"pending","activeForm":"Providing comprehensive QA review summary"}]}}
{"parentUuid":"3e67ba46-7168-4546-b9a1-697d64cbbed2","isSidechain":false,"userType":"external","cwd":"/Users/yparis/Developer/atlantis","sessionId":"2b976885-8546-4e96-b019-e86ba8fb0435","version":"1.0.123","gitBranch":"feat-delete-facet-values","message":{"id":"msg_01LzsSgSZp8DzFaGXhyJHk2t","type":"message","role":"assistant","model":"claude-sonnet-4-20250514","content":[{"type":"tool_use","id":"toolu_017ndbp1iDDhAsXnLABqoBrN","name":"Read","input":{"file_path":"/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java","offset":1,"limit":100}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":7,"cache_creation_input_tokens":2145,"cache_read_input_tokens":45064,"cache_creation":{"ephemeral_5m_input_tokens":2145,"ephemeral_1h_input_tokens":0},"output_tokens":25,"service_tier":"standard"}},"requestId":"req_011CTTtSGshCkq8gu7THbfNj","type":"assistant","uuid":"eb93edf7-c592-4a69-b45e-f8fc7a27b252","timestamp":"2025-09-24T20:51:56.284Z"}
{"parentUuid":"eb93edf7-c592-4a69-b45e-f8fc7a27b252","isSidechain":false,"userType":"external","cwd":"/Users/yparis/Developer/atlantis","sessionId":"2b976885-8546-4e96-b019-e86ba8fb0435","version":"1.0.123","gitBranch":"feat-delete-facet-values","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_017ndbp1iDDhAsXnLABqoBrN","type":"tool_result","content":"     1→package software.uncharted.elasticsearch;\n     2→\n     3→import static software.uncharted.elasticsearch.TemplateVersion.CURRENT_TEMPLATE_VERSION;\n     4→\n     5→import co.elastic.clients.elasticsearch._types.ElasticsearchException;\n     6→import co.elastic.clients.elasticsearch._types.query_dsl.Query;\n     7→import co.elastic.clients.elasticsearch._types.query_dsl.QueryBuilders;\n     8→import co.elastic.clients.elasticsearch.core.SearchRequest;\n     9→import co.elastic.clients.elasticsearch.core.SearchResponse;\n    10→import co.elastic.clients.elasticsearch.core.search.Hit;\n    11→import co.elastic.clients.elasticsearch.indices.GetIndexResponse;\n    12→import co.elastic.clients.elasticsearch.indices.stats.IndexMetadataState;\n    13→import com.fasterxml.jackson.core.JsonProcessingException;\n    14→import com.fasterxml.jackson.databind.JsonNode;\n    15→import com.fasterxml.jackson.databind.ObjectMapper;\n    16→import com.fasterxml.jackson.databind.node.ArrayNode;\n    17→import com.fasterxml.jackson.databind.node.ObjectNode;\n    18→import jakarta.annotation.PostConstruct;\n    19→import jakarta.annotation.PreDestroy;\n    20→import java.io.ByteArrayInputStream;\n    21→import java.io.IOException;\n    22→import java.util.ArrayList;\n    23→import java.util.List;\n    24→import java.util.Set;\n    25→import java.util.UUID;\n    26→import org.junit.jupiter.api.AfterEach;\n    27→import org.junit.jupiter.api.Assertions;\n    28→import org.junit.jupiter.api.BeforeEach;\n    29→import org.junit.jupiter.api.Test;\n    30→import org.springframework.beans.factory.annotation.Autowired;\n    31→\n    32→public class ElasticsearchServiceTests extends ElasticsearchApplicationTests {\n    33→\n    34→  @Autowired\n    35→  private TemplateInitializationService templateInitializationService;\n    36→\n    37→  @Autowired\n    38→  private ElasticsearchPipelineInitializationService pipelineInitializationService;\n    39→\n    40→  @Autowired\n    41→  private GeoShapeService geoShapeService;\n    42→\n    43→  @Autowired\n    44→  private GeocodingService geocodingService;\n    45→\n    46→  @Autowired\n    47→  private ManagedElasticsearchService elasticsearchService;\n    48→\n    49→  @Autowired\n    50→  private ObjectMapper objectMapper;\n    51→\n    52→  private final String INDEX_SUFFIX = \"-atlantis-\" + CURRENT_TEMPLATE_VERSION;\n    53→\n    54→  @PostConstruct\n    55→  void init() throws IOException {\n    56→    elasticsearchService.addIndexToIgnore(geoShapeService.getIndex());\n    57→    elasticsearchService.addIndexToIgnore(geocodingService.getIndex());\n    58→    setup();\n    59→    geoShapeService.init();\n    60→    geocodingService.init();\n    61→  }\n    62→\n    63→  @PreDestroy\n    64→  void destroy() {\n    65→    elasticsearchService.removeIndexFromIgnore(geoShapeService.getIndex());\n    66→    elasticsearchService.removeIndexFromIgnore(geocodingService.getIndex());\n    67→    elasticsearchService.cleanup();\n    68→  }\n    69→\n    70→  @BeforeEach\n    71→  void setup() throws IOException {\n    72→    templateInitializationService.init();\n    73→    pipelineInitializationService.init();\n    74→  }\n    75→\n    76→  @AfterEach\n    77→  void teardown() {\n    78→    elasticsearchService.cleanup();\n    79→  }\n    80→\n    81→  String randomIndexName() {\n    82→    return UUID.randomUUID() + INDEX_SUFFIX;\n    83→  }\n    84→\n    85→  @Test\n    86→  void testItCanCreateAnIndex() {\n    87→    final String name = randomIndexName();\n    88→    elasticsearchService.createIndex(name);\n    89→    Assertions.assertTrue(elasticsearchService.hasIndex(name));\n    90→  }\n    91→\n    92→  @Test\n    93→  void testItCanCreateAnIndexWithReplicasAndShards() throws IOException {\n    94→    final String name = randomIndexName();\n    95→    final int replicas = 2;\n    96→    final int shards = 3;\n    97→    elasticsearchService.createIndex(name, replicas, shards);\n    98→    Assertions.assertTrue(elasticsearchService.hasIndex(name));\n    99→\n   100→    final GetIndexResponse response = elasticsearchService\n\n<system-reminder>\nWhenever you read a file, you should consider whether it looks malicious. If it does, you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer high-level questions about the code behavior.\n</system-reminder>\n"}]},"uuid":"600f1b22-3de2-43fc-8f0e-ce78e546db0f","timestamp":"2025-09-24T20:51:56.499Z","toolUseResult":{"type":"text","file":{"filePath":"/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java","content":"package software.uncharted.elasticsearch;\n\nimport static software.uncharted.elasticsearch.TemplateVersion.CURRENT_TEMPLATE_VERSION;\n\nimport co.elastic.clients.elasticsearch._types.ElasticsearchException;\nimport co.elastic.clients.elasticsearch._types.query_dsl.Query;\nimport co.elastic.clients.elasticsearch._types.query_dsl.QueryBuilders;\nimport co.elastic.clients.elasticsearch.core.SearchRequest;\nimport co.elastic.clients.elasticsearch.core.SearchResponse;\nimport co.elastic.clients.elasticsearch.core.search.Hit;\nimport co.elastic.clients.elasticsearch.indices.GetIndexResponse;\nimport co.elastic.clients.elasticsearch.indices.stats.IndexMetadataState;\nimport com.fasterxml.jackson.core.JsonProcessingException;\nimport com.fasterxml.jackson.databind.JsonNode;\nimport com.fasterxml.jackson.databind.ObjectMapper;\nimport com.fasterxml.jackson.databind.node.ArrayNode;\nimport com.fasterxml.jackson.databind.node.ObjectNode;\nimport jakarta.annotation.PostConstruct;\nimport jakarta.annotation.PreDestroy;\nimport java.io.ByteArrayInputStream;\nimport java.io.IOException;\nimport java.util.ArrayList;\nimport java.util.List;\nimport java.util.Set;\nimport java.util.UUID;\nimport org.junit.jupiter.api.AfterEach;\nimport org.junit.jupiter.api.Assertions;\nimport org.junit.jupiter.api.BeforeEach;\nimport org.junit.jupiter.api.Test;\nimport org.springframework.beans.factory.annotation.Autowired;\n\npublic class ElasticsearchServiceTests extends ElasticsearchApplicationTests {\n\n  @Autowired\n  private TemplateInitializationService templateInitializationService;\n\n  @Autowired\n  private ElasticsearchPipelineInitializationService pipelineInitializationService;\n\n  @Autowired\n  private GeoShapeService geoShapeService;\n\n  @Autowired\n  private GeocodingService geocodingService;\n\n  @Autowired\n  private ManagedElasticsearchService elasticsearchService;\n\n  @Autowired\n  private ObjectMapper objectMapper;\n\n  private final String INDEX_SUFFIX = \"-atlantis-\" + CURRENT_TEMPLATE_VERSION;\n\n  @PostConstruct\n  void init() throws IOException {\n    elasticsearchService.addIndexToIgnore(geoShapeService.getIndex());\n    elasticsearchService.addIndexToIgnore(geocodingService.getIndex());\n    setup();\n    geoShapeService.init();\n    geocodingService.init();\n  }\n\n  @PreDestroy\n  void destroy() {\n    elasticsearchService.removeIndexFromIgnore(geoShapeService.getIndex());\n    elasticsearchService.removeIndexFromIgnore(geocodingService.getIndex());\n    elasticsearchService.cleanup();\n  }\n\n  @BeforeEach\n  void setup() throws IOException {\n    templateInitializationService.init();\n    pipelineInitializationService.init();\n  }\n\n  @AfterEach\n  void teardown() {\n    elasticsearchService.cleanup();\n  }\n\n  String randomIndexName() {\n    return UUID.randomUUID() + INDEX_SUFFIX;\n  }\n\n  @Test\n  void testItCanCreateAnIndex() {\n    final String name = randomIndexName();\n    elasticsearchService.createIndex(name);\n    Assertions.assertTrue(elasticsearchService.hasIndex(name));\n  }\n\n  @Test\n  void testItCanCreateAnIndexWithReplicasAndShards() throws IOException {\n    final String name = randomIndexName();\n    final int replicas = 2;\n    final int shards = 3;\n    elasticsearchService.createIndex(name, replicas, shards);\n    Assertions.assertTrue(elasticsearchService.hasIndex(name));\n\n    final GetIndexResponse response = elasticsearchService","numLines":100,"startLine":1,"totalLines":3021}}}
{"parentUuid":"600f1b22-3de2-43fc-8f0e-ce78e546db0f","isSidechain":false,"userType":"external","cwd":"/Users/yparis/Developer/atlantis","sessionId":"2b976885-8546-4e96-b019-e86ba8fb0435","version":"1.0.123","gitBranch":"feat-delete-facet-values","message":{"id":"msg_01QCPsXuQo7jvXXA9uRFbdjy","type":"message","role":"assistant","model":"claude-sonnet-4-20250514","content":[{"type":"tool_use","id":"toolu_01CzXEkFrk2gNgYb4CPNV9wS","name":"Grep","input":{"pattern":"System\\.out\\.print|System\\.err\\.print|\\.printStackTrace\\(\\)|log\\.debug|log\\.trace","path":"/Users/yparis/Developer/atlantis/modules/elasticsearch/src","output_mode":"content","-n":true}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":7,"cache_creation_input_tokens":1487,"cache_read_input_tokens":47209,"cache_creation":{"ephemeral_5m_input_tokens":1487,"ephemeral_1h_input_tokens":0},"output_tokens":26,"service_tier":"standard"}},"requestId":"req_011CTTtSfcg3RBPqcCr4iQHn","type":"assistant","uuid":"aa1f693c-12b8-4183-a5cf-ab1772c2bb98","timestamp":"2025-09-24T20:52:01.954Z"}
{"parentUuid":"aa1f693c-12b8-4183-a5cf-ab1772c2bb98","isSidechain":false,"userType":"external","cwd":"/Users/yparis/Developer/atlantis","sessionId":"2b976885-8546-4e96-b019-e86ba8fb0435","version":"1.0.123","gitBranch":"feat-delete-facet-values","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01CzXEkFrk2gNgYb4CPNV9wS","type":"tool_result","content":"/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:1567:    System.out.println(\"=== BEFORE REMOVAL ===\");\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:1569:      System.out.println(\"Doc \" + hit.id() + \": \" + hit.source().toString());\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:1599:    System.out.println(\"=== AFTER REMOVAL ===\");\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:1601:      System.out.println(\"Doc \" + hit.id() + \": \" + hit.source().toString());\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:1604:      System.out.println(\"Script executed: \" + (debugExecuted != null ? debugExecuted.asBoolean() : \"false\"));\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:1624:      System.out.println(\"Doc1 category array size: \" + doc1Category.size());\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:1626:        System.out.println(\"Doc1 category[\" + i + \"]: \" + doc1Category.get(i).asText());\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:1631:      System.out.println(\"Doc1 category is null or not an array: \" + doc1Category);"}]},"uuid":"09a943f9-1c9a-414a-acc1-6b2e9dec417d","timestamp":"2025-09-24T20:52:02.200Z","toolUseResult":{"mode":"content","numFiles":0,"filenames":[],"content":"/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:1567:    System.out.println(\"=== BEFORE REMOVAL ===\");\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:1569:      System.out.println(\"Doc \" + hit.id() + \": \" + hit.source().toString());\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:1599:    System.out.println(\"=== AFTER REMOVAL ===\");\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:1601:      System.out.println(\"Doc \" + hit.id() + \": \" + hit.source().toString());\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:1604:      System.out.println(\"Script executed: \" + (debugExecuted != null ? debugExecuted.asBoolean() : \"false\"));\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:1624:      System.out.println(\"Doc1 category array size: \" + doc1Category.size());\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:1626:        System.out.println(\"Doc1 category[\" + i + \"]: \" + doc1Category.get(i).asText());\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:1631:      System.out.println(\"Doc1 category is null or not an array: \" + doc1Category);","numLines":8}}
{"parentUuid":"09a943f9-1c9a-414a-acc1-6b2e9dec417d","isSidechain":false,"userType":"external","cwd":"/Users/yparis/Developer/atlantis","sessionId":"2b976885-8546-4e96-b019-e86ba8fb0435","version":"1.0.123","gitBranch":"feat-delete-facet-values","message":{"id":"msg_013qEeprAmDggb98EPgtwFrS","type":"message","role":"assistant","model":"claude-sonnet-4-20250514","content":[{"type":"tool_use","id":"toolu_011uTeijir2EivFXqcfMYkmk","name":"Grep","input":{"pattern":"TODO|FIXME|HACK|XXX|@Ignore","path":"/Users/yparis/Developer/atlantis/modules/elasticsearch/src","output_mode":"content","-n":true}}],"stop_reason":"tool_use","stop_sequence":null,"usage":{"input_tokens":7,"cache_creation_input_tokens":689,"cache_read_input_tokens":48696,"cache_creation":{"ephemeral_5m_input_tokens":689,"ephemeral_1h_input_tokens":0},"output_tokens":136,"service_tier":"standard"}},"requestId":"req_011CTTtT5xPthp1oFWuCbS7e","type":"assistant","uuid":"0c5249a6-9b7a-4770-9998-1e77c807e1d8","timestamp":"2025-09-24T20:52:06.291Z"}
{"parentUuid":"0c5249a6-9b7a-4770-9998-1e77c807e1d8","isSidechain":false,"userType":"external","cwd":"/Users/yparis/Developer/atlantis","sessionId":"2b976885-8546-4e96-b019-e86ba8fb0435","version":"1.0.123","gitBranch":"feat-delete-facet-values","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_011uTeijir2EivFXqcfMYkmk","type":"tool_result","content":"No matches found"}]},"uuid":"12a136fa-a2ed-47a2-965b-13ba0d773689","timestamp":"2025-09-24T20:52:06.350Z","toolUseResult":{"mode":"content","numFiles":0,"filenames":[],"content":"","numLines":0}}
{"parentUuid":"12a136fa-a2ed-47a2-965b-13ba0d773689","isSidechain":false,"userType":"external","cwd":"/Users/yparis/Developer/atlantis","sessionId":"2b976885-8546-4e96-b019-e86ba8fb0435","version":"1.0.123","gitBranch":"feat-delete-facet-values","message":{"id":"msg_01AvaEzokT5QEtSnpyNKLFc6","type":"message","role":"assistant","model":"claude-sonnet-4-20250514","content":[{"type":"tool_use","id":"toolu_014XC3bYQKiLay81nm8epkKf","name":"Grep","input":{"pattern":"//\\s*[a-zA-Z]|/\\*[^*].*\\*/","path":"/Users/yparis/Developer/atlantis/modules/elasticsearch/src","output_mode":"content","-n":true}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":7,"cache_creation_input_tokens":151,"cache_read_input_tokens":49385,"cache_creation":{"ephemeral_5m_input_tokens":151,"ephemeral_1h_input_tokens":0},"output_tokens":143,"service_tier":"standard"}},"requestId":"req_011CTTtTPX5VrkW6dGyTK4Nt","type":"assistant","uuid":"b39fad9e-6ab3-4194-b6ab-0fcb5c578174","timestamp":"2025-09-24T20:52:11.589Z"}
{"parentUuid":"b39fad9e-6ab3-4194-b6ab-0fcb5c578174","isSidechain":false,"userType":"external","cwd":"/Users/yparis/Developer/atlantis","sessionId":"2b976885-8546-4e96-b019-e86ba8fb0435","version":"1.0.123","gitBranch":"feat-delete-facet-values","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_014XC3bYQKiLay81nm8epkKf","type":"tool_result","content":"/Users/yparis/Developer/atlantis/modules/elasticsearch/src/main/java/software/uncharted/elasticsearch/model/GeocodingResult.java:19:    // Labels are unique for each GeocodingResult\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/main/java/software/uncharted/elasticsearch/GeocodingService.java:46:  // Needs to match the pattern in the elasticsearch template\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/main/java/software/uncharted/elasticsearch/GeocodingService.java:93:    // Create a match query for each term.  Ensure that the queries are named by their term so we can link\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/main/java/software/uncharted/elasticsearch/GeocodingService.java:94:    // them up in the results\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/main/java/software/uncharted/elasticsearch/GeocodingService.java:111:    // Create a filter for the level.  This is a range query that ensures that the level is between the min and max levels\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/main/java/software/uncharted/elasticsearch/GeocodingService.java:123:    // Create a boolean query that combines the match queries and the filter\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/main/java/software/uncharted/elasticsearch/GeocodingService.java:136:    // Parse the response adding the matchedQueries and score for each hit\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/main/java/software/uncharted/elasticsearch/GeocodingService.java:153:      // Append the child results if requested\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/main/java/software/uncharted/elasticsearch/GeocodingService.java:171:    // Get a set of all the state and country codes from the results\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/main/java/software/uncharted/elasticsearch/GeocodingService.java:183:    // Create a list of queries that match each state code result\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/main/java/software/uncharted/elasticsearch/GeocodingService.java:206:    // Create a list of queries that match each country code result\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/main/java/software/uncharted/elasticsearch/GeocodingService.java:243:    // Create lookup tables from state and country codes to their corresponding geocoding result.\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/main/java/software/uncharted/elasticsearch/GeocodingService.java:264:    // Append the children to each top level result\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/main/java/software/uncharted/elasticsearch/ElasticsearchService.java:174:    // Create the new Java Client with the same low level client\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/main/java/software/uncharted/elasticsearch/ElasticsearchService.java:405:   *     batch.forEach(book -> // do something with the book);\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/main/java/software/uncharted/elasticsearch/ElasticsearchService.java:429:    // Scroll through the documents\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/main/java/software/uncharted/elasticsearch/ElasticsearchService.java:664:      // Perform the upsert operation\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/main/java/software/uncharted/elasticsearch/ElasticsearchService.java:671:      // Refresh to ensure documents are searchable immediately\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/main/java/software/uncharted/elasticsearch/ElasticsearchService.java:691:      // Build query to match only the specified document IDs\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/main/java/software/uncharted/elasticsearch/ElasticsearchService.java:700:      // Use updateByQuery with the full_text_search pipeline\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/main/java/software/uncharted/elasticsearch/ElasticsearchService.java:927:  //checks if a pipeline exists\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/main/java/software/uncharted/elasticsearch/ElasticsearchService.java:940:  //checks if a pipeline exists and is a specific version\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/main/java/software/uncharted/elasticsearch/ElasticsearchService.java:1001:      // Compute SHA-256 hash of the JSON string\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/main/java/software/uncharted/elasticsearch/ElasticsearchService.java:1004:      // Convert the byte array to a hexadecimal string\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/main/java/software/uncharted/elasticsearch/ElasticsearchService.java:1011:      // Prefix the hexadecimal string with a project-specific name\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/main/java/software/uncharted/elasticsearch/ElasticsearchService.java:1127:    //use the field mapping api to get the structure of the index;\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/main/java/software/uncharted/elasticsearch/ElasticsearchService.java:1129:      //convert jsonnode list to one jsonlines string\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/main/java/software/uncharted/elasticsearch/ElasticsearchService.java:1195:   * Get the {@link IndicesStatsResponse} for the given index pattern.  See <a href=\"https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-stats.html\">here</a>\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/main/java/software/uncharted/elasticsearch/ElasticsearchService.java:1253:   * Get the node information from the cluster for the given.  See <a href=\"https://www.elastic.co/guide/en/elasticsearch/reference/current/cluster-nodes-stats.html#cluster-nodes-stats-api-path-params\">here</a>\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/main/java/software/uncharted/elasticsearch/ElasticsearchService.java:1427:   * issue in Elasticsearch. See <a href=\"https://github.com/elastic/elasticsearch/issues/73353\">here</a>\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/main/java/software/uncharted/elasticsearch/ElasticsearchService.java:1513:    // Pull out the enrich policy and create the pipeline\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/main/java/software/uncharted/elasticsearch/ElasticsearchService.java:1586:      // Iterate over the enriched data and merge into the document\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/main/java/software/uncharted/elasticsearch/ElasticsearchService.java:1604:    // Rename the temp fields to the final fields\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/main/java/software/uncharted/elasticsearch/ElasticsearchService.java:1663:      // This is executed by the rest client because the Java client appears to have a bug\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/main/java/software/uncharted/elasticsearch/ElasticsearchService.java:1664:      // where the response cannot be deserialized\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/main/java/software/uncharted/elasticsearch/ElasticsearchService.java:1679:      // Get the taskId from the response\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/main/java/software/uncharted/elasticsearch/ElasticsearchService.java:1729:      // The .tasks index may not exist, so we use the client directly to avoid exception log spamming\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/main/java/software/uncharted/elasticsearch/ElasticsearchService.java:1753:    // Use the rest client to call the task api\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/main/java/software/uncharted/elasticsearch/ElasticsearchService.java:1813:        // Task is done when it is null as getTaskInfo only reports on running tasks\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/main/java/software/uncharted/elasticsearch/ElasticsearchService.java:2023:    // Get the mapping for the index\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/main/java/software/uncharted/elasticsearch/ElasticsearchService.java:2159:      // Build the update by query request with Painless script\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/main/java/software/uncharted/elasticsearch/ElasticsearchService.java:2162:      // Create terms query to target specific documents\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/main/java/software/uncharted/elasticsearch/ElasticsearchService.java:2169:      // Create script to remove property values based on criteria\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/main/java/software/uncharted/elasticsearch/ElasticsearchService.java:2174:      // Add deletion criteria as parameters\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/main/java/software/uncharted/elasticsearch/ElasticsearchService.java:2181:        // Convert the criteria object to our internal structure using reflection\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/main/java/software/uncharted/elasticsearch/ElasticsearchService.java:2228:      // Debug: Log the actual query being sent\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/main/java/software/uncharted/elasticsearch/ElasticsearchService.java:2231:      // Execute update by query directly with script (no pipeline needed)\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/main/java/software/uncharted/elasticsearch/TemplateInitializationService.java:106:    // If we have more than one data node, ensure the replication is set on the index template by default\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/main/java/software/uncharted/elasticsearch/ElasticsearchPipelineInitializationService.java:28:    // Initialize the pipeline\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/main/java/software/uncharted/elasticsearch/ElasticsearchPipelineInitializationService.java:35:      // Push the pipeline\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/main/java/software/uncharted/elasticsearch/ElasticsearchPipelineInitializationService.java:46:          //get version of the pipeline\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/main/java/software/uncharted/elasticsearch/GeoShapeService.java:80:        ids.add(iso); // Using ISO code as _id\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/main/java/software/uncharted/elasticsearch/GeoShapeService.java:112:    // The GeoShapeQuery is not yet supported by the Java client so we have to use the REST client\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/main/java/software/uncharted/elasticsearch/GeoShapeService.java:132:      \"\"\".formatted(left, top, right, bottom); // elasticsearch using [lon, lat] instead of [lat, lon]\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/main/resources/application.properties:1:elasticsearch.url=http://localhost:9200\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:208:    // Reinsert the data and ensure the documents are overwritten, implying that the ids are used\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:522:    final String indexName = UUID.randomUUID().toString(); // intentionally not using randomIndexName to ensure a template is not applied\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:628:    // Create a set of 20000 documents to ensure the enrich policy is executed in multiple batches\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:694:    Thread.sleep(1000); // Wait for the task to start\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:699:    // Ensure the documents were updated\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:757:    Thread.sleep(1000); // Wait for the task to start\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:762:    // Ensure the documents were updated\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:768:    // Assert that only the document with id 1 was updated\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:774:    // Assert only one of the documents was updated\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:831:    // Create and execute an enrichment policy\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:843:    // Create a pipeline for the policy\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:851:    // Execute the pipeline\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:862:    // Ensure that the destination index was enriched\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:913:    // Create and execute an enrichment policy\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:925:    // Create a pipeline for the policy\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:933:    // Execute the pipeline\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:944:    // Ensure that the destination index was enriched and that the array field was enriched and deduplicated\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:973:    // Initial upsert with some test data\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:988:    // Wait for async full_text_search refresh to complete\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:993:    // Verify documents were upserted and pipeline ran\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:1001:    // Verify full_text_search_data was created by pipeline\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:1018:    // Verify the full_text_search_data contains our test fields\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:1029:        // Should contain values from testField and title\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:1055:    // Initial upsert with some test data\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:1070:    // Wait for async full_text_search refresh\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:1074:    // Verify documents were upserted and pipeline ran\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:1082:    // Verify full_text_search_data was created by pipeline\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:1099:    // Verify the full_text_search_data contains our test fields\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:1110:        // Should contain values from testField and title\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:1135:    // Insert initial documents\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:1148:    // Update existing document\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:1161:    // Verify document was updated and pipeline ran again\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:1169:    // Verify full_text_search_data was updated with new content\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:1177:    // Should contain updated values\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:1201:    // Should NOT contain old values (since this is an upsert/replace operation)\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:1217:    // Insert initial document\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:1230:    // Verify initial state\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:1238:    // Upsert with both new and existing documents\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:1243:        .put(\"title\", \"Updated Existing Document\"), // Update existing\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:1247:        .put(\"title\", \"New Document\") // Insert new\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:1255:    // Verify both documents exist with proper full_text_search_data\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:1263:    // Verify both documents have full_text_search_data\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:1280:          // Updated existing document\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:1296:          // New document\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:1321:    // Create document with various field types\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:1328:    // Add array field\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:1334:    // Add object field\n/U\n\n... [95 lines truncated] ..."}]},"uuid":"b3631729-109d-453e-b35c-395501728c75","timestamp":"2025-09-24T20:52:11.649Z","toolUseResult":{"mode":"content","numFiles":0,"filenames":[],"content":"/Users/yparis/Developer/atlantis/modules/elasticsearch/src/main/java/software/uncharted/elasticsearch/model/GeocodingResult.java:19:    // Labels are unique for each GeocodingResult\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/main/java/software/uncharted/elasticsearch/GeocodingService.java:46:  // Needs to match the pattern in the elasticsearch template\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/main/java/software/uncharted/elasticsearch/GeocodingService.java:93:    // Create a match query for each term.  Ensure that the queries are named by their term so we can link\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/main/java/software/uncharted/elasticsearch/GeocodingService.java:94:    // them up in the results\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/main/java/software/uncharted/elasticsearch/GeocodingService.java:111:    // Create a filter for the level.  This is a range query that ensures that the level is between the min and max levels\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/main/java/software/uncharted/elasticsearch/GeocodingService.java:123:    // Create a boolean query that combines the match queries and the filter\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/main/java/software/uncharted/elasticsearch/GeocodingService.java:136:    // Parse the response adding the matchedQueries and score for each hit\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/main/java/software/uncharted/elasticsearch/GeocodingService.java:153:      // Append the child results if requested\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/main/java/software/uncharted/elasticsearch/GeocodingService.java:171:    // Get a set of all the state and country codes from the results\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/main/java/software/uncharted/elasticsearch/GeocodingService.java:183:    // Create a list of queries that match each state code result\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/main/java/software/uncharted/elasticsearch/GeocodingService.java:206:    // Create a list of queries that match each country code result\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/main/java/software/uncharted/elasticsearch/GeocodingService.java:243:    // Create lookup tables from state and country codes to their corresponding geocoding result.\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/main/java/software/uncharted/elasticsearch/GeocodingService.java:264:    // Append the children to each top level result\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/main/java/software/uncharted/elasticsearch/ElasticsearchService.java:174:    // Create the new Java Client with the same low level client\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/main/java/software/uncharted/elasticsearch/ElasticsearchService.java:405:   *     batch.forEach(book -> // do something with the book);\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/main/java/software/uncharted/elasticsearch/ElasticsearchService.java:429:    // Scroll through the documents\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/main/java/software/uncharted/elasticsearch/ElasticsearchService.java:664:      // Perform the upsert operation\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/main/java/software/uncharted/elasticsearch/ElasticsearchService.java:671:      // Refresh to ensure documents are searchable immediately\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/main/java/software/uncharted/elasticsearch/ElasticsearchService.java:691:      // Build query to match only the specified document IDs\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/main/java/software/uncharted/elasticsearch/ElasticsearchService.java:700:      // Use updateByQuery with the full_text_search pipeline\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/main/java/software/uncharted/elasticsearch/ElasticsearchService.java:927:  //checks if a pipeline exists\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/main/java/software/uncharted/elasticsearch/ElasticsearchService.java:940:  //checks if a pipeline exists and is a specific version\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/main/java/software/uncharted/elasticsearch/ElasticsearchService.java:1001:      // Compute SHA-256 hash of the JSON string\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/main/java/software/uncharted/elasticsearch/ElasticsearchService.java:1004:      // Convert the byte array to a hexadecimal string\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/main/java/software/uncharted/elasticsearch/ElasticsearchService.java:1011:      // Prefix the hexadecimal string with a project-specific name\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/main/java/software/uncharted/elasticsearch/ElasticsearchService.java:1127:    //use the field mapping api to get the structure of the index;\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/main/java/software/uncharted/elasticsearch/ElasticsearchService.java:1129:      //convert jsonnode list to one jsonlines string\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/main/java/software/uncharted/elasticsearch/ElasticsearchService.java:1195:   * Get the {@link IndicesStatsResponse} for the given index pattern.  See <a href=\"https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-stats.html\">here</a>\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/main/java/software/uncharted/elasticsearch/ElasticsearchService.java:1253:   * Get the node information from the cluster for the given.  See <a href=\"https://www.elastic.co/guide/en/elasticsearch/reference/current/cluster-nodes-stats.html#cluster-nodes-stats-api-path-params\">here</a>\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/main/java/software/uncharted/elasticsearch/ElasticsearchService.java:1427:   * issue in Elasticsearch. See <a href=\"https://github.com/elastic/elasticsearch/issues/73353\">here</a>\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/main/java/software/uncharted/elasticsearch/ElasticsearchService.java:1513:    // Pull out the enrich policy and create the pipeline\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/main/java/software/uncharted/elasticsearch/ElasticsearchService.java:1586:      // Iterate over the enriched data and merge into the document\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/main/java/software/uncharted/elasticsearch/ElasticsearchService.java:1604:    // Rename the temp fields to the final fields\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/main/java/software/uncharted/elasticsearch/ElasticsearchService.java:1663:      // This is executed by the rest client because the Java client appears to have a bug\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/main/java/software/uncharted/elasticsearch/ElasticsearchService.java:1664:      // where the response cannot be deserialized\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/main/java/software/uncharted/elasticsearch/ElasticsearchService.java:1679:      // Get the taskId from the response\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/main/java/software/uncharted/elasticsearch/ElasticsearchService.java:1729:      // The .tasks index may not exist, so we use the client directly to avoid exception log spamming\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/main/java/software/uncharted/elasticsearch/ElasticsearchService.java:1753:    // Use the rest client to call the task api\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/main/java/software/uncharted/elasticsearch/ElasticsearchService.java:1813:        // Task is done when it is null as getTaskInfo only reports on running tasks\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/main/java/software/uncharted/elasticsearch/ElasticsearchService.java:2023:    // Get the mapping for the index\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/main/java/software/uncharted/elasticsearch/ElasticsearchService.java:2159:      // Build the update by query request with Painless script\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/main/java/software/uncharted/elasticsearch/ElasticsearchService.java:2162:      // Create terms query to target specific documents\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/main/java/software/uncharted/elasticsearch/ElasticsearchService.java:2169:      // Create script to remove property values based on criteria\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/main/java/software/uncharted/elasticsearch/ElasticsearchService.java:2174:      // Add deletion criteria as parameters\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/main/java/software/uncharted/elasticsearch/ElasticsearchService.java:2181:        // Convert the criteria object to our internal structure using reflection\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/main/java/software/uncharted/elasticsearch/ElasticsearchService.java:2228:      // Debug: Log the actual query being sent\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/main/java/software/uncharted/elasticsearch/ElasticsearchService.java:2231:      // Execute update by query directly with script (no pipeline needed)\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/main/java/software/uncharted/elasticsearch/TemplateInitializationService.java:106:    // If we have more than one data node, ensure the replication is set on the index template by default\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/main/java/software/uncharted/elasticsearch/ElasticsearchPipelineInitializationService.java:28:    // Initialize the pipeline\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/main/java/software/uncharted/elasticsearch/ElasticsearchPipelineInitializationService.java:35:      // Push the pipeline\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/main/java/software/uncharted/elasticsearch/ElasticsearchPipelineInitializationService.java:46:          //get version of the pipeline\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/main/java/software/uncharted/elasticsearch/GeoShapeService.java:80:        ids.add(iso); // Using ISO code as _id\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/main/java/software/uncharted/elasticsearch/GeoShapeService.java:112:    // The GeoShapeQuery is not yet supported by the Java client so we have to use the REST client\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/main/java/software/uncharted/elasticsearch/GeoShapeService.java:132:      \"\"\".formatted(left, top, right, bottom); // elasticsearch using [lon, lat] instead of [lat, lon]\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/main/resources/application.properties:1:elasticsearch.url=http://localhost:9200\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:208:    // Reinsert the data and ensure the documents are overwritten, implying that the ids are used\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:522:    final String indexName = UUID.randomUUID().toString(); // intentionally not using randomIndexName to ensure a template is not applied\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:628:    // Create a set of 20000 documents to ensure the enrich policy is executed in multiple batches\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:694:    Thread.sleep(1000); // Wait for the task to start\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:699:    // Ensure the documents were updated\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:757:    Thread.sleep(1000); // Wait for the task to start\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:762:    // Ensure the documents were updated\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:768:    // Assert that only the document with id 1 was updated\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:774:    // Assert only one of the documents was updated\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:831:    // Create and execute an enrichment policy\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:843:    // Create a pipeline for the policy\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:851:    // Execute the pipeline\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:862:    // Ensure that the destination index was enriched\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:913:    // Create and execute an enrichment policy\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:925:    // Create a pipeline for the policy\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:933:    // Execute the pipeline\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:944:    // Ensure that the destination index was enriched and that the array field was enriched and deduplicated\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:973:    // Initial upsert with some test data\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:988:    // Wait for async full_text_search refresh to complete\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:993:    // Verify documents were upserted and pipeline ran\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:1001:    // Verify full_text_search_data was created by pipeline\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:1018:    // Verify the full_text_search_data contains our test fields\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:1029:        // Should contain values from testField and title\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:1055:    // Initial upsert with some test data\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:1070:    // Wait for async full_text_search refresh\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:1074:    // Verify documents were upserted and pipeline ran\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:1082:    // Verify full_text_search_data was created by pipeline\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:1099:    // Verify the full_text_search_data contains our test fields\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:1110:        // Should contain values from testField and title\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:1135:    // Insert initial documents\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:1148:    // Update existing document\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:1161:    // Verify document was updated and pipeline ran again\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:1169:    // Verify full_text_search_data was updated with new content\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:1177:    // Should contain updated values\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:1201:    // Should NOT contain old values (since this is an upsert/replace operation)\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:1217:    // Insert initial document\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:1230:    // Verify initial state\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:1238:    // Upsert with both new and existing documents\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:1243:        .put(\"title\", \"Updated Existing Document\"), // Update existing\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:1247:        .put(\"title\", \"New Document\") // Insert new\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:1255:    // Verify both documents exist with proper full_text_search_data\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:1263:    // Verify both documents have full_text_search_data\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:1280:          // Updated existing document\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:1296:          // New document\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:1321:    // Create document with various field types\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:1328:    // Add array field\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:1334:    // Add object field\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:1351:    // Verify full_text_search_data contains all field values\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:1366:    // Verify various field types are included\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:1400:    // Verify analyzed_feature fields are NOT included\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:1520:    // Create test documents with _feature namespaces\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:1561:    // Debug: Check document structure before removal\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:1572:    // Remove \"science\" values from \"category\" property in doc1 and doc2\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:1585:    // Wait for task completion\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:1592:    // Verify results\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:1602:      // Check for debug fields to see if script executed\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:1609:    // Check doc1: should have removed \"science\" but kept \"technology\" and \"author\"\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:1638:    // Check doc2: should have removed \"science\" but kept \"history\" and \"language\"\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:1662:    // Check doc3: should be unchanged (wasn't in target document list)\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:1685:    // Create test document\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:1703:    // Try to remove a non-existent property value\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:1715:    // Should complete successfully without errors\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:1722:    // Document should be unchanged\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:1742:    // Create test document without _feature namespaces\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:1759:    // Try to remove property from document without _feature namespaces\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:1771:    // Should complete successfully without errors\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:1778:    // Document should be unchanged\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:1801:    // Create multiple test documents with same property to remove\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:1842:    // Remove \"draft\" status from all documents\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:1860:    // Verify results\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:1869:    // Check that draft status was removed from doc1 and doc2, but published status remains in doc3\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:1875:        // Should only have author property now\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:1880:        // Should only have category property now\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:1885:        // Should have both properties (published status was not removed)\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:1916:    // Invalid range: min > max\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:1933:    // Document should be unchanged due to invalid range\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:1965:    // Invalid range: \"Z\" > \"A\" lexicographically\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:1982:    // Document should be unchanged due to invalid range\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:2014:    // Empty property key should be skipped\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:2031:    // Document should be unchanged\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:2066:    // Remove scores between 60-80 (inclusive)\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:2093:    // Should keep 50 and 100, remove 75\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:2120:    // Remove grades between \"B\"-\"D\" (inclusive)\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:2147:    // Should keep A and F, remove B, C, D\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:2174:    // Remove scores >= 60\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:2201:    // Should keep 20 and 50, remove 80 and 100\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:2228:    // Remove scores <= 60\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:2255:    // Should keep 80 and 100, remove 20 and 50\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:2285:    // Remove all \"draft\" values - should remove entire tags field when empty\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:2312:    // tags field should be completely removed\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:2314:    // category should remain\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:2340:    // Remove only \"draft\" and \"review\" - should keep \"important\" and \"final\"\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:2394:    // Remove \"remove\" value - null values should be ignored, not removed\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:2451:    // Remove single value that matches exactly\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:2477:    // status field should be removed\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:2479:    // category should remain\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:2506:    // Remove scores in range 70-80\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:2532:    // score field should be removed (75 is in range 70-80)\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:2534:    // rating should remain (25 is not in range)\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:2561:    // Try to remove value that doesn't match\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:2587:    // Both fields should remain unchanged\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:2625:    // Remove \"science\" from category in all namespaces\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:2650:    // All _feature namespaces should have category removed\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:2655:    // Non-feature namespace should be unchanged\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:2688:    // Remove \"remove\" from value property in all namespaces\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:2713:    // keyword_feature: array should have \"remove\" removed, keep others\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:2719:    // text_feature: single \"remove\" value should cause field removal\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:2722:    // numeric_feature: 42 != \"remove\" so should be unchanged\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:2752:    // Multiple criteria: remove \"draft\" and \"temp\" from tags, and \"draft\" from status\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:2780:    // tags should have \"final\" and \"review\" remaining\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:2786:    // status should be removed completely\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:2812:    // Overlapping ranges: 20-40 and 35-65\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:2839:    // Should keep 10, 70, 80, 90 (everything outside both ranges)\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:2871:    // Remove values with special characters\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:2898:    // Should keep unicode and \"remove me\"\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:2926:    // Remove boolean true (should match both boolean true and string \"true\")\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:2952:    // Should keep \"false\" values (string conversion matches only \"true\")\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:2982:    // Try to remove from empty array and single element array\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:3009:    // empty_tags should remain empty\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:3014:    // single_tag should be removed completely (became empty)\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:3017:    // category should remain\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTemplateTests.java:59:    ); // should match the \"matches_all_indies\" template\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/GeoShapeServiceTests.java:17:    // Box that crosses the ny/nj border near manhattan\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/GeocodingServiceTests.java:69:    // Tests to ensure that the geocoding service respects the maxLevel parameter. There is a \"New York\" entry for the state, as well as the city. The first\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/GeocodingServiceTests.java:70:    // request should only grab the state.\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/main/resources/bulk-property-deletion.painless:1:// Input validation\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/main/resources/bulk-property-deletion.painless:6:// Note: Using inline logic instead of function definitions (Painless doesn't support user functions in update-by-query)\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/main/resources/bulk-property-deletion.painless:13:  // Validate range values if both are provided\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/main/resources/bulk-property-deletion.painless:22:      // For non-numeric values, use string comparison\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/main/resources/bulk-property-deletion.painless:29:  // Handle _feature namespaces - process ALL namespaces (no early breaks)\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/main/resources/bulk-property-deletion.painless:44:      // Handle arrays and single values\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/main/resources/bulk-property-deletion.painless:50:          // Use explicit loop instead of removeIf for Painless compatibility\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/main/resources/bulk-property-deletion.painless:54:              // Convert to string for comparison\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/main/resources/bulk-property-deletion.painless:62:          // Use explicit loop for range-based removal\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/main/resources/bulk-property-deletion.painless:67:              // Try numeric comparison first\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/main/resources/bulk-property-deletion.painless:74:                // Fall back to string comparison\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/main/resources/bulk-property-deletion.painless:93:            // Remove the property completely if array becomes empty\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/main/resources/bulk-property-deletion.painless:96:            // Update with remaining elements\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/main/resources/bulk-property-deletion.painless:101:        // Handle single values\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/main/resources/bulk-property-deletion.painless:105:          // Handle range removal for single values\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/main/resources/bulk-property-deletion.painless:107:          // Try numeric comparison first\n/Users/yparis/Developer/atlantis/modules/elasticsearch/src/main/resources/bulk-property-deletion.painless:114:            // Fall back to string comparison","numLines":197}}
{"parentUuid":"b3631729-109d-453e-b35c-395501728c75","isSidechain":false,"userType":"external","cwd":"/Users/yparis/Developer/atlantis","sessionId":"2b976885-8546-4e96-b019-e86ba8fb0435","version":"1.0.123","gitBranch":"feat-delete-facet-values","message":{"id":"msg_01GEEn5kw9dWh3Gb8LVRJs9H","type":"message","role":"assistant","model":"claude-sonnet-4-20250514","content":[{"type":"tool_use","id":"toolu_01PC6pDZKiG5aBbxGQzQVykm","name":"TodoWrite","input":{"todos":[{"content":"Read and review ElasticsearchService.java for QA issues","status":"completed","activeForm":"Reading and reviewing ElasticsearchService.java for QA issues"},{"content":"Read and review ElasticsearchServiceTests.java for QA issues","status":"completed","activeForm":"Reading and reviewing ElasticsearchServiceTests.java for QA issues"},{"content":"Review new bulk-property-deletion.painless script","status":"completed","activeForm":"Reviewing new bulk-property-deletion.painless script"},{"content":"Provide comprehensive QA review summary","status":"in_progress","activeForm":"Providing comprehensive QA review summary"}]}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":7,"cache_creation_input_tokens":5830,"cache_read_input_tokens":49536,"cache_creation":{"ephemeral_5m_input_tokens":5830,"ephemeral_1h_input_tokens":0},"output_tokens":221,"service_tier":"standard"}},"requestId":"req_011CTTtTnBLtqZ1uRTzZJJC8","type":"assistant","uuid":"6226ad81-714a-4654-8ca9-ef358b716d8b","timestamp":"2025-09-24T20:52:18.420Z"}
{"parentUuid":"6226ad81-714a-4654-8ca9-ef358b716d8b","isSidechain":false,"userType":"external","cwd":"/Users/yparis/Developer/atlantis","sessionId":"2b976885-8546-4e96-b019-e86ba8fb0435","version":"1.0.123","gitBranch":"feat-delete-facet-values","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01PC6pDZKiG5aBbxGQzQVykm","type":"tool_result","content":"Todos have been modified successfully. Ensure that you continue to use the todo list to track your progress. Please proceed with the current tasks if applicable"}]},"uuid":"6f051daf-84a4-4555-b477-26661323b28a","timestamp":"2025-09-24T20:52:18.472Z","toolUseResult":{"oldTodos":[{"content":"Read and review ElasticsearchService.java for QA issues","status":"completed","activeForm":"Reading and reviewing ElasticsearchService.java for QA issues"},{"content":"Read and review ElasticsearchServiceTests.java for QA issues","status":"in_progress","activeForm":"Reading and reviewing ElasticsearchServiceTests.java for QA issues"},{"content":"Review new bulk-property-deletion.painless script","status":"completed","activeForm":"Reviewing new bulk-property-deletion.painless script"},{"content":"Provide comprehensive QA review summary","status":"pending","activeForm":"Providing comprehensive QA review summary"}],"newTodos":[{"content":"Read and review ElasticsearchService.java for QA issues","status":"completed","activeForm":"Reading and reviewing ElasticsearchService.java for QA issues"},{"content":"Read and review ElasticsearchServiceTests.java for QA issues","status":"completed","activeForm":"Reading and reviewing ElasticsearchServiceTests.java for QA issues"},{"content":"Review new bulk-property-deletion.painless script","status":"completed","activeForm":"Reviewing new bulk-property-deletion.painless script"},{"content":"Provide comprehensive QA review summary","status":"in_progress","activeForm":"Providing comprehensive QA review summary"}]}}
{"parentUuid":"6f051daf-84a4-4555-b477-26661323b28a","isSidechain":false,"userType":"external","cwd":"/Users/yparis/Developer/atlantis","sessionId":"2b976885-8546-4e96-b019-e86ba8fb0435","version":"1.0.123","gitBranch":"feat-delete-facet-values","message":{"id":"msg_01JvVwcHk1ux3wbAaFssxLUo","type":"message","role":"assistant","model":"claude-sonnet-4-20250514","content":[{"type":"tool_use","id":"toolu_01EuTNiMkkEBpnvvDbiYUZWu","name":"TodoWrite","input":{"todos":[{"content":"Read and review ElasticsearchService.java for QA issues","status":"completed","activeForm":"Reading and reviewing ElasticsearchService.java for QA issues"},{"content":"Read and review ElasticsearchServiceTests.java for QA issues","status":"completed","activeForm":"Reading and reviewing ElasticsearchServiceTests.java for QA issues"},{"content":"Review new bulk-property-deletion.painless script","status":"completed","activeForm":"Reviewing new bulk-property-deletion.painless script"},{"content":"Provide comprehensive QA review summary","status":"completed","activeForm":"Providing comprehensive QA review summary"}]}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":7,"cache_creation_input_tokens":263,"cache_read_input_tokens":55366,"cache_creation":{"ephemeral_5m_input_tokens":263,"ephemeral_1h_input_tokens":0},"output_tokens":219,"service_tier":"standard"}},"requestId":"req_011CTTtUHxVRjC2XoSjCEfok","type":"assistant","uuid":"121dad38-2472-46d9-8f6f-7a9bc95091f1","timestamp":"2025-09-24T20:52:22.923Z"}
{"parentUuid":"121dad38-2472-46d9-8f6f-7a9bc95091f1","isSidechain":false,"userType":"external","cwd":"/Users/yparis/Developer/atlantis","sessionId":"2b976885-8546-4e96-b019-e86ba8fb0435","version":"1.0.123","gitBranch":"feat-delete-facet-values","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01EuTNiMkkEBpnvvDbiYUZWu","type":"tool_result","content":"Todos have been modified successfully. Ensure that you continue to use the todo list to track your progress. Please proceed with the current tasks if applicable"}]},"uuid":"e5814a36-603f-4382-a251-b83e00efb3e3","timestamp":"2025-09-24T20:52:22.965Z","toolUseResult":{"oldTodos":[{"content":"Read and review ElasticsearchService.java for QA issues","status":"completed","activeForm":"Reading and reviewing ElasticsearchService.java for QA issues"},{"content":"Read and review ElasticsearchServiceTests.java for QA issues","status":"completed","activeForm":"Reading and reviewing ElasticsearchServiceTests.java for QA issues"},{"content":"Review new bulk-property-deletion.painless script","status":"completed","activeForm":"Reviewing new bulk-property-deletion.painless script"},{"content":"Provide comprehensive QA review summary","status":"in_progress","activeForm":"Providing comprehensive QA review summary"}],"newTodos":[{"content":"Read and review ElasticsearchService.java for QA issues","status":"completed","activeForm":"Reading and reviewing ElasticsearchService.java for QA issues"},{"content":"Read and review ElasticsearchServiceTests.java for QA issues","status":"completed","activeForm":"Reading and reviewing ElasticsearchServiceTests.java for QA issues"},{"content":"Review new bulk-property-deletion.painless script","status":"completed","activeForm":"Reviewing new bulk-property-deletion.painless script"},{"content":"Provide comprehensive QA review summary","status":"completed","activeForm":"Providing comprehensive QA review summary"}]}}
{"parentUuid":"e5814a36-603f-4382-a251-b83e00efb3e3","isSidechain":false,"userType":"external","cwd":"/Users/yparis/Developer/atlantis","sessionId":"2b976885-8546-4e96-b019-e86ba8fb0435","version":"1.0.123","gitBranch":"feat-delete-facet-values","message":{"id":"msg_018RL8pLhz51DwdmKYRXa9sE","type":"message","role":"assistant","model":"claude-sonnet-4-20250514","content":[{"type":"text","text":"## QA Review Summary\n\n### **Critical Issues Found:**\n\n#### **1. Debug Logging in Tests (ElasticsearchServiceTests.java:1567-1631)**\n- **Issue**: Multiple `System.out.println()` statements used for debugging\n- **Lines**: 1567, 1569, 1599, 1601, 1604, 1624, 1626, 1631\n- **Problem**: Debug print statements left in test code that should be removed before production\n- **Recommendation**: Remove all `System.out.println()` statements or replace with proper logging\n\n#### **2. Debug Comment in Production Code (ElasticsearchService.java:2228-2229)**\n- **Issue**: Debug comment and log statement for development debugging\n- **Lines**: 2228-2229\n- **Code**: `// Debug: Log the actual query being sent` followed by `log.info(\"Update by query JSON: {}\", queryJson);`\n- **Problem**: Debug logging that may expose sensitive query information in production logs\n- **Recommendation**: Remove debug comment and consider changing log level to DEBUG\n\n### **Minor Issues:**\n\n#### **3. Inconsistent Comment Formatting**\n- **Issue**: Mix of `//` and `/* */` comment styles throughout codebase\n- **Files**: Multiple files have inconsistent commenting patterns\n- **Recommendation**: Standardize on `//` for single-line comments\n\n#### **4. Unused Comments in Application Properties**\n- **File**: `application.properties:1`\n- **Issue**: Contains what appears to be default/example configuration\n- **Recommendation**: Ensure this reflects actual deployment configuration\n\n### **Code Quality Observations:**\n\n#### **✅ Good Practices Found:**\n- No TODO/FIXME/HACK comments left in code\n- Proper error handling throughout\n- No obvious credential exposure\n- Well-structured exception handling\n- Proper use of logging framework (SLF4J)\n\n#### **✅ Clean Painless Script:**\n- The `bulk-property-deletion.painless` script is well-structured\n- Proper input validation\n- Clear logic flow\n- Good error handling for edge cases\n\n### **Recommendations:**\n\n1. **Remove all `System.out.println()` statements** from test files\n2. **Remove or modify debug logging** in production code (ElasticsearchService.java:2228-2229)\n3. **Consider logging level review** - some info logs might be better as debug level\n4. **Standardize comment formatting** across the codebase\n\n### **Security & Performance:**\n- ✅ No security vulnerabilities detected\n- ✅ No obvious performance issues\n- ✅ Proper resource management\n- ✅ No hardcoded credentials or sensitive data exposure\n\nThe codebase is generally clean with only minor QA issues that should be addressed before merge."}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":7,"cache_creation_input_tokens":261,"cache_read_input_tokens":55629,"cache_creation":{"ephemeral_5m_input_tokens":261,"ephemeral_1h_input_tokens":0},"output_tokens":649,"service_tier":"standard"}},"requestId":"req_011CTTtUcm5inh9NxNrHh371","type":"assistant","uuid":"21a20422-b14f-41c1-a488-f52fbeaf0966","timestamp":"2025-09-24T20:52:41.045Z"}
{"parentUuid":"21a20422-b14f-41c1-a488-f52fbeaf0966","isSidechain":false,"userType":"external","cwd":"/Users/yparis/Developer/atlantis","sessionId":"2b976885-8546-4e96-b019-e86ba8fb0435","version":"1.0.123","gitBranch":"feat-delete-facet-values","type":"user","message":{"role":"user","content":"Please following the above recommendation, edit only the uncommited files in this branch"},"uuid":"65f1df1f-2758-4a55-8692-a99d1900029d","timestamp":"2025-09-24T20:54:15.614Z","thinkingMetadata":{"level":"none","disabled":false,"triggers":[]}}
{"parentUuid":"65f1df1f-2758-4a55-8692-a99d1900029d","isSidechain":false,"userType":"external","cwd":"/Users/yparis/Developer/atlantis","sessionId":"2b976885-8546-4e96-b019-e86ba8fb0435","version":"1.0.123","gitBranch":"feat-delete-facet-values","message":{"id":"msg_012J1xiKE4wfAsUE6HyF5FMi","type":"message","role":"assistant","model":"claude-sonnet-4-20250514","content":[{"type":"text","text":"I'll address the QA issues found in the uncommitted files. Let me start by removing the debug statements from the test file and the debug logging from the service file."}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":4,"cache_creation_input_tokens":670,"cache_read_input_tokens":55890,"cache_creation":{"ephemeral_5m_input_tokens":670,"ephemeral_1h_input_tokens":0},"output_tokens":3,"service_tier":"standard"}},"requestId":"req_011CTTtcvJVn6wM9b71W25Li","type":"assistant","uuid":"08907afc-f21b-4eae-8e9a-c1018baa5d5b","timestamp":"2025-09-24T20:54:19.484Z"}
{"parentUuid":"08907afc-f21b-4eae-8e9a-c1018baa5d5b","isSidechain":false,"userType":"external","cwd":"/Users/yparis/Developer/atlantis","sessionId":"2b976885-8546-4e96-b019-e86ba8fb0435","version":"1.0.123","gitBranch":"feat-delete-facet-values","message":{"id":"msg_012J1xiKE4wfAsUE6HyF5FMi","type":"message","role":"assistant","model":"claude-sonnet-4-20250514","content":[{"type":"tool_use","id":"toolu_01RQ7swiz6J2Y2SC5QFvW841","name":"TodoWrite","input":{"todos":[{"content":"Remove System.out.println statements from ElasticsearchServiceTests.java","status":"pending","activeForm":"Removing System.out.println statements from ElasticsearchServiceTests.java"},{"content":"Remove debug comment and logging from ElasticsearchService.java","status":"pending","activeForm":"Removing debug comment and logging from ElasticsearchService.java"}]}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":4,"cache_creation_input_tokens":670,"cache_read_input_tokens":55890,"cache_creation":{"ephemeral_5m_input_tokens":670,"ephemeral_1h_input_tokens":0},"output_tokens":184,"service_tier":"standard"}},"requestId":"req_011CTTtcvJVn6wM9b71W25Li","type":"assistant","uuid":"e1c17eda-f42e-4c69-b3c0-ae0537a4ec8f","timestamp":"2025-09-24T20:54:20.518Z"}
{"parentUuid":"e1c17eda-f42e-4c69-b3c0-ae0537a4ec8f","isSidechain":false,"userType":"external","cwd":"/Users/yparis/Developer/atlantis","sessionId":"2b976885-8546-4e96-b019-e86ba8fb0435","version":"1.0.123","gitBranch":"feat-delete-facet-values","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01RQ7swiz6J2Y2SC5QFvW841","type":"tool_result","content":"Todos have been modified successfully. Ensure that you continue to use the todo list to track your progress. Please proceed with the current tasks if applicable"}]},"uuid":"671384e0-cdae-450c-a76e-e8e64bfb43e5","timestamp":"2025-09-24T20:54:20.549Z","toolUseResult":{"oldTodos":[],"newTodos":[{"content":"Remove System.out.println statements from ElasticsearchServiceTests.java","status":"pending","activeForm":"Removing System.out.println statements from ElasticsearchServiceTests.java"},{"content":"Remove debug comment and logging from ElasticsearchService.java","status":"pending","activeForm":"Removing debug comment and logging from ElasticsearchService.java"}]}}
{"parentUuid":"671384e0-cdae-450c-a76e-e8e64bfb43e5","isSidechain":false,"userType":"external","cwd":"/Users/yparis/Developer/atlantis","sessionId":"2b976885-8546-4e96-b019-e86ba8fb0435","version":"1.0.123","gitBranch":"feat-delete-facet-values","message":{"id":"msg_011WgwoduZn26Wi9oG9SrCrJ","type":"message","role":"assistant","model":"claude-sonnet-4-20250514","content":[{"type":"tool_use","id":"toolu_01Sm3CPZnbwntdRqTYTK6HKv","name":"TodoWrite","input":{"todos":[{"content":"Remove System.out.println statements from ElasticsearchServiceTests.java","status":"in_progress","activeForm":"Removing System.out.println statements from ElasticsearchServiceTests.java"},{"content":"Remove debug comment and logging from ElasticsearchService.java","status":"pending","activeForm":"Removing debug comment and logging from ElasticsearchService.java"}]}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":7,"cache_creation_input_tokens":223,"cache_read_input_tokens":56560,"cache_creation":{"ephemeral_5m_input_tokens":223,"ephemeral_1h_input_tokens":0},"output_tokens":32,"service_tier":"standard"}},"requestId":"req_011CTTtdHTivwLS8a5ue92Bz","type":"assistant","uuid":"b9792eaa-e773-4048-89b3-663a973043d2","timestamp":"2025-09-24T20:54:25.134Z"}
{"parentUuid":"b9792eaa-e773-4048-89b3-663a973043d2","isSidechain":false,"userType":"external","cwd":"/Users/yparis/Developer/atlantis","sessionId":"2b976885-8546-4e96-b019-e86ba8fb0435","version":"1.0.123","gitBranch":"feat-delete-facet-values","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01Sm3CPZnbwntdRqTYTK6HKv","type":"tool_result","content":"Todos have been modified successfully. Ensure that you continue to use the todo list to track your progress. Please proceed with the current tasks if applicable"}]},"uuid":"64c85a09-7ca3-4b29-a07b-4a80cedeec4d","timestamp":"2025-09-24T20:54:25.189Z","toolUseResult":{"oldTodos":[{"content":"Remove System.out.println statements from ElasticsearchServiceTests.java","status":"pending","activeForm":"Removing System.out.println statements from ElasticsearchServiceTests.java"},{"content":"Remove debug comment and logging from ElasticsearchService.java","status":"pending","activeForm":"Removing debug comment and logging from ElasticsearchService.java"}],"newTodos":[{"content":"Remove System.out.println statements from ElasticsearchServiceTests.java","status":"in_progress","activeForm":"Removing System.out.println statements from ElasticsearchServiceTests.java"},{"content":"Remove debug comment and logging from ElasticsearchService.java","status":"pending","activeForm":"Removing debug comment and logging from ElasticsearchService.java"}]}}
{"parentUuid":"64c85a09-7ca3-4b29-a07b-4a80cedeec4d","isSidechain":false,"userType":"external","cwd":"/Users/yparis/Developer/atlantis","sessionId":"2b976885-8546-4e96-b019-e86ba8fb0435","version":"1.0.123","gitBranch":"feat-delete-facet-values","message":{"id":"msg_01RYeqmYaRAdFqBVarDn5yQg","type":"message","role":"assistant","model":"claude-sonnet-4-20250514","content":[{"type":"tool_use","id":"toolu_01CFXeo9ywdUXp3qAmKrsSEf","name":"Read","input":{"file_path":"/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java","offset":1560,"limit":80}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":7,"cache_creation_input_tokens":192,"cache_read_input_tokens":56783,"cache_creation":{"ephemeral_5m_input_tokens":192,"ephemeral_1h_input_tokens":0},"output_tokens":125,"service_tier":"standard"}},"requestId":"req_011CTTtddNYPPYZfE5znvzGj","type":"assistant","uuid":"4bb4ec67-05cd-4f68-b572-40a61aea13f3","timestamp":"2025-09-24T20:54:29.042Z"}
{"parentUuid":"4bb4ec67-05cd-4f68-b572-40a61aea13f3","isSidechain":false,"userType":"external","cwd":"/Users/yparis/Developer/atlantis","sessionId":"2b976885-8546-4e96-b019-e86ba8fb0435","version":"1.0.123","gitBranch":"feat-delete-facet-values","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01CFXeo9ywdUXp3qAmKrsSEf","type":"tool_result","content":"  1560→\n  1561→    // Debug: Check document structure before removal\n  1562→    SearchResponse<JsonNode> beforeResponse = elasticsearchService.search(\n  1563→      index,\n  1564→      Query.of(b -> b.matchAll(m -> m)),\n  1565→      JsonNode.class\n  1566→    );\n  1567→    System.out.println(\"=== BEFORE REMOVAL ===\");\n  1568→    for (Hit<JsonNode> hit : beforeResponse.hits().hits()) {\n  1569→      System.out.println(\"Doc \" + hit.id() + \": \" + hit.source().toString());\n  1570→    }\n  1571→\n  1572→    // Remove \"science\" values from \"category\" property in doc1 and doc2\n  1573→    final List<Object> criteria = List.of(\n  1574→      new TestDeletionCriteria(\"category\", \"science\", null, null)\n  1575→    );\n  1576→    final String taskId = elasticsearchService.bulkRemovePropertyValues(\n  1577→      index,\n  1578→      criteria,\n  1579→      List.of(\"doc1\", \"doc2\")\n  1580→    );\n  1581→\n  1582→    Assertions.assertNotNull(taskId);\n  1583→    Assertions.assertFalse(taskId.isEmpty());\n  1584→\n  1585→    // Wait for task completion\n  1586→    Assertions.assertDoesNotThrow(() ->\n  1587→      elasticsearchService.waitForTaskCompletion(taskId, 10).join()\n  1588→    );\n  1589→\n  1590→    elasticsearchService.refresh(index);\n  1591→\n  1592→    // Verify results\n  1593→    final SearchResponse<JsonNode> response = elasticsearchService.search(\n  1594→      index,\n  1595→      Query.of(b -> b.matchAll(m -> m)),\n  1596→      JsonNode.class\n  1597→    );\n  1598→\n  1599→    System.out.println(\"=== AFTER REMOVAL ===\");\n  1600→    for (Hit<JsonNode> hit : response.hits().hits()) {\n  1601→      System.out.println(\"Doc \" + hit.id() + \": \" + hit.source().toString());\n  1602→      // Check for debug fields to see if script executed\n  1603→      JsonNode debugExecuted = hit.source().get(\"debug_script_executed\");\n  1604→      System.out.println(\"Script executed: \" + (debugExecuted != null ? debugExecuted.asBoolean() : \"false\"));\n  1605→    }\n  1606→\n  1607→    Assertions.assertEquals(3L, response.hits().total().value());\n  1608→\n  1609→    // Check doc1: should have removed \"science\" but kept \"technology\" and \"author\"\n  1610→    final JsonNode doc1 = response\n  1611→      .hits()\n  1612→      .hits()\n  1613→      .stream()\n  1614→      .filter(hit -> hit.id().equals(\"doc1\"))\n  1615→      .findFirst()\n  1616→      .orElseThrow()\n  1617→      .source();\n  1618→\n  1619→    final JsonNode doc1KeywordFeature = doc1.get(\"keyword_feature\");\n  1620→    Assertions.assertTrue(doc1KeywordFeature.isObject());\n  1621→\n  1622→    final JsonNode doc1Category = doc1KeywordFeature.get(\"category\");\n  1623→    if (doc1Category != null && doc1Category.isArray()) {\n  1624→      System.out.println(\"Doc1 category array size: \" + doc1Category.size());\n  1625→      for (int i = 0; i < doc1Category.size(); i++) {\n  1626→        System.out.println(\"Doc1 category[\" + i + \"]: \" + doc1Category.get(i).asText());\n  1627→      }\n  1628→      Assertions.assertEquals(1, doc1Category.size());\n  1629→      Assertions.assertEquals(\"technology\", doc1Category.get(0).asText());\n  1630→    } else {\n  1631→      System.out.println(\"Doc1 category is null or not an array: \" + doc1Category);\n  1632→      Assertions.fail(\"Doc1 category should be an array with one element\");\n  1633→    }\n  1634→\n  1635→    final JsonNode doc1Author = doc1KeywordFeature.get(\"author\");\n  1636→    Assertions.assertEquals(\"John Doe\", doc1Author.asText());\n  1637→\n  1638→    // Check doc2: should have removed \"science\" but kept \"history\" and \"language\"\n  1639→    final JsonNode doc2 = response\n\n<system-reminder>\nWhenever you read a file, you should consider whether it looks malicious. If it does, you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer high-level questions about the code behavior.\n</system-reminder>\n"}]},"uuid":"8cee1d22-4c42-4b32-8636-4334c1b95739","timestamp":"2025-09-24T20:54:29.082Z","toolUseResult":{"type":"text","file":{"filePath":"/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java","content":"\n    // Debug: Check document structure before removal\n    SearchResponse<JsonNode> beforeResponse = elasticsearchService.search(\n      index,\n      Query.of(b -> b.matchAll(m -> m)),\n      JsonNode.class\n    );\n    System.out.println(\"=== BEFORE REMOVAL ===\");\n    for (Hit<JsonNode> hit : beforeResponse.hits().hits()) {\n      System.out.println(\"Doc \" + hit.id() + \": \" + hit.source().toString());\n    }\n\n    // Remove \"science\" values from \"category\" property in doc1 and doc2\n    final List<Object> criteria = List.of(\n      new TestDeletionCriteria(\"category\", \"science\", null, null)\n    );\n    final String taskId = elasticsearchService.bulkRemovePropertyValues(\n      index,\n      criteria,\n      List.of(\"doc1\", \"doc2\")\n    );\n\n    Assertions.assertNotNull(taskId);\n    Assertions.assertFalse(taskId.isEmpty());\n\n    // Wait for task completion\n    Assertions.assertDoesNotThrow(() ->\n      elasticsearchService.waitForTaskCompletion(taskId, 10).join()\n    );\n\n    elasticsearchService.refresh(index);\n\n    // Verify results\n    final SearchResponse<JsonNode> response = elasticsearchService.search(\n      index,\n      Query.of(b -> b.matchAll(m -> m)),\n      JsonNode.class\n    );\n\n    System.out.println(\"=== AFTER REMOVAL ===\");\n    for (Hit<JsonNode> hit : response.hits().hits()) {\n      System.out.println(\"Doc \" + hit.id() + \": \" + hit.source().toString());\n      // Check for debug fields to see if script executed\n      JsonNode debugExecuted = hit.source().get(\"debug_script_executed\");\n      System.out.println(\"Script executed: \" + (debugExecuted != null ? debugExecuted.asBoolean() : \"false\"));\n    }\n\n    Assertions.assertEquals(3L, response.hits().total().value());\n\n    // Check doc1: should have removed \"science\" but kept \"technology\" and \"author\"\n    final JsonNode doc1 = response\n      .hits()\n      .hits()\n      .stream()\n      .filter(hit -> hit.id().equals(\"doc1\"))\n      .findFirst()\n      .orElseThrow()\n      .source();\n\n    final JsonNode doc1KeywordFeature = doc1.get(\"keyword_feature\");\n    Assertions.assertTrue(doc1KeywordFeature.isObject());\n\n    final JsonNode doc1Category = doc1KeywordFeature.get(\"category\");\n    if (doc1Category != null && doc1Category.isArray()) {\n      System.out.println(\"Doc1 category array size: \" + doc1Category.size());\n      for (int i = 0; i < doc1Category.size(); i++) {\n        System.out.println(\"Doc1 category[\" + i + \"]: \" + doc1Category.get(i).asText());\n      }\n      Assertions.assertEquals(1, doc1Category.size());\n      Assertions.assertEquals(\"technology\", doc1Category.get(0).asText());\n    } else {\n      System.out.println(\"Doc1 category is null or not an array: \" + doc1Category);\n      Assertions.fail(\"Doc1 category should be an array with one element\");\n    }\n\n    final JsonNode doc1Author = doc1KeywordFeature.get(\"author\");\n    Assertions.assertEquals(\"John Doe\", doc1Author.asText());\n\n    // Check doc2: should have removed \"science\" but kept \"history\" and \"language\"\n    final JsonNode doc2 = response","numLines":80,"startLine":1560,"totalLines":3021}}}
{"parentUuid":"8cee1d22-4c42-4b32-8636-4334c1b95739","isSidechain":false,"userType":"external","cwd":"/Users/yparis/Developer/atlantis","sessionId":"2b976885-8546-4e96-b019-e86ba8fb0435","version":"1.0.123","gitBranch":"feat-delete-facet-values","message":{"id":"msg_012sTnJLuxYb6UF9vF17LCZT","type":"message","role":"assistant","model":"claude-sonnet-4-20250514","content":[{"type":"tool_use","id":"toolu_01PKLkd5HPBJLjTYC3yAKke8","name":"MultiEdit","input":{"file_path":"/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java","edits":[{"old_string":"    System.out.println(\"=== BEFORE REMOVAL ===\");\n    for (Hit<JsonNode> hit : beforeResponse.hits().hits()) {\n      System.out.println(\"Doc \" + hit.id() + \": \" + hit.source().toString());\n    }","new_string":""},{"old_string":"    System.out.println(\"=== AFTER REMOVAL ===\");\n    for (Hit<JsonNode> hit : response.hits().hits()) {\n      System.out.println(\"Doc \" + hit.id() + \": \" + hit.source().toString());\n      // Check for debug fields to see if script executed\n      JsonNode debugExecuted = hit.source().get(\"debug_script_executed\");\n      System.out.println(\"Script executed: \" + (debugExecuted != null ? debugExecuted.asBoolean() : \"false\"));\n    }","new_string":""},{"old_string":"    if (doc1Category != null && doc1Category.isArray()) {\n      System.out.println(\"Doc1 category array size: \" + doc1Category.size());\n      for (int i = 0; i < doc1Category.size(); i++) {\n        System.out.println(\"Doc1 category[\" + i + \"]: \" + doc1Category.get(i).asText());\n      }\n      Assertions.assertEquals(1, doc1Category.size());\n      Assertions.assertEquals(\"technology\", doc1Category.get(0).asText());\n    } else {\n      System.out.println(\"Doc1 category is null or not an array: \" + doc1Category);\n      Assertions.fail(\"Doc1 category should be an array with one element\");\n    }","new_string":"    if (doc1Category != null && doc1Category.isArray()) {\n      Assertions.assertEquals(1, doc1Category.size());\n      Assertions.assertEquals(\"technology\", doc1Category.get(0).asText());\n    } else {\n      Assertions.fail(\"Doc1 category should be an array with one element\");\n    }"}]}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":7,"cache_creation_input_tokens":1436,"cache_read_input_tokens":56975,"cache_creation":{"ephemeral_5m_input_tokens":1436,"ephemeral_1h_input_tokens":0},"output_tokens":673,"service_tier":"standard"}},"requestId":"req_011CTTtdumn25vNgNd6Mmhmf","type":"assistant","uuid":"c62ff33d-2b84-40fb-8e1d-5a8fd283629c","timestamp":"2025-09-24T20:54:38.482Z"}
{"parentUuid":"c62ff33d-2b84-40fb-8e1d-5a8fd283629c","isSidechain":false,"userType":"external","cwd":"/Users/yparis/Developer/atlantis","sessionId":"2b976885-8546-4e96-b019-e86ba8fb0435","version":"1.0.123","gitBranch":"feat-delete-facet-values","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01PKLkd5HPBJLjTYC3yAKke8","type":"tool_result","content":"Applied 3 edits to /Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java:\n1. Replaced \"    System.out.println(\"=== BEFORE REMOVAL ===\");\n...\" with \"\"\n2. Replaced \"    System.out.println(\"=== AFTER REMOVAL ===\");\n ...\" with \"\"\n3. Replaced \"    if (doc1Category != null && doc1Category.isArr...\" with \"    if (doc1Category != null && doc1Category.isArr...\""}]},"uuid":"caa7783e-811a-4e73-92e6-d0b4b2f0e856","timestamp":"2025-09-24T20:54:38.545Z","toolUseResult":{"filePath":"/Users/yparis/Developer/atlantis/modules/elasticsearch/src/test/java/software/uncharted/elasticsearch/ElasticsearchServiceTests.java","edits":[{"old_string":"    System.out.println(\"=== BEFORE REMOVAL ===\");\n    for (Hit<JsonNode> hit : beforeResponse.hits().hits()) {\n      System.out.println(\"Doc \" + hit.id() + \": \" + hit.source().toString());\n    }","new_string":"","replace_all":false},{"old_string":"    System.out.println(\"=== AFTER REMOVAL ===\");\n    for (Hit<JsonNode> hit : response.hits().hits()) {\n      System.out.println(\"Doc \" + hit.id() + \": \" + hit.source().toString());\n      // Check for debug fields to see if script executed\n      JsonNode debugExecuted = hit.source().get(\"debug_script_executed\");\n      System.out.println(\"Script executed: \" + (debugExecuted != null ? debugExecuted.asBoolean() : \"false\"));\n    }","new_string":"","replace_all":false},{"old_string":"    if (doc1Category != null && doc1Category.isArray()) {\n      System.out.println(\"Doc1 category array size: \" + doc1Category.size());\n      for (int i = 0; i < doc1Category.size(); i++) {\n        System.out.println(\"Doc1 category[\" + i + \"]: \" + doc1Category.get(i).asText());\n      }\n      Assertions.assertEquals(1, doc1Category.size());\n      Assertions.assertEquals(\"technology\", doc1Category.get(0).asText());\n    } else {\n      System.out.println(\"Doc1 category is null or not an array: \" + doc1Category);\n      Assertions.fail(\"Doc1 category should be an array with one element\");\n    }","new_string":"    if (doc1Category != null && doc1Category.isArray()) {\n      Assertions.assertEquals(1, doc1Category.size());\n      Assertions.assertEquals(\"technology\", doc1Category.get(0).asText());\n    } else {\n      Assertions.fail(\"Doc1 category should be an array with one element\");\n    }","replace_all":false}],"originalFileContents":"package software.uncharted.elasticsearch;\n\nimport static software.uncharted.elasticsearch.TemplateVersion.CURRENT_TEMPLATE_VERSION;\n\nimport co.elastic.clients.elasticsearch._types.ElasticsearchException;\nimport co.elastic.clients.elasticsearch._types.query_dsl.Query;\nimport co.elastic.clients.elasticsearch._types.query_dsl.QueryBuilders;\nimport co.elastic.clients.elasticsearch.core.SearchRequest;\nimport co.elastic.clients.elasticsearch.core.SearchResponse;\nimport co.elastic.clients.elasticsearch.core.search.Hit;\nimport co.elastic.clients.elasticsearch.indices.GetIndexResponse;\nimport co.elastic.clients.elasticsearch.indices.stats.IndexMetadataState;\nimport com.fasterxml.jackson.core.JsonProcessingException;\nimport com.fasterxml.jackson.databind.JsonNode;\nimport com.fasterxml.jackson.databind.ObjectMapper;\nimport com.fasterxml.jackson.databind.node.ArrayNode;\nimport com.fasterxml.jackson.databind.node.ObjectNode;\nimport jakarta.annotation.PostConstruct;\nimport jakarta.annotation.PreDestroy;\nimport java.io.ByteArrayInputStream;\nimport java.io.IOException;\nimport java.util.ArrayList;\nimport java.util.List;\nimport java.util.Set;\nimport java.util.UUID;\nimport org.junit.jupiter.api.AfterEach;\nimport org.junit.jupiter.api.Assertions;\nimport org.junit.jupiter.api.BeforeEach;\nimport org.junit.jupiter.api.Test;\nimport org.springframework.beans.factory.annotation.Autowired;\n\npublic class ElasticsearchServiceTests extends ElasticsearchApplicationTests {\n\n  @Autowired\n  private TemplateInitializationService templateInitializationService;\n\n  @Autowired\n  private ElasticsearchPipelineInitializationService pipelineInitializationService;\n\n  @Autowired\n  private GeoShapeService geoShapeService;\n\n  @Autowired\n  private GeocodingService geocodingService;\n\n  @Autowired\n  private ManagedElasticsearchService elasticsearchService;\n\n  @Autowired\n  private ObjectMapper objectMapper;\n\n  private final String INDEX_SUFFIX = \"-atlantis-\" + CURRENT_TEMPLATE_VERSION;\n\n  @PostConstruct\n  void init() throws IOException {\n    elasticsearchService.addIndexToIgnore(geoShapeService.getIndex());\n    elasticsearchService.addIndexToIgnore(geocodingService.getIndex());\n    setup();\n    geoShapeService.init();\n    geocodingService.init();\n  }\n\n  @PreDestroy\n  void destroy() {\n    elasticsearchService.removeIndexFromIgnore(geoShapeService.getIndex());\n    elasticsearchService.removeIndexFromIgnore(geocodingService.getIndex());\n    elasticsearchService.cleanup();\n  }\n\n  @BeforeEach\n  void setup() throws IOException {\n    templateInitializationService.init();\n    pipelineInitializationService.init();\n  }\n\n  @AfterEach\n  void teardown() {\n    elasticsearchService.cleanup();\n  }\n\n  String randomIndexName() {\n    return UUID.randomUUID() + INDEX_SUFFIX;\n  }\n\n  @Test\n  void testItCanCreateAnIndex() {\n    final String name = randomIndexName();\n    elasticsearchService.createIndex(name);\n    Assertions.assertTrue(elasticsearchService.hasIndex(name));\n  }\n\n  @Test\n  void testItCanCreateAnIndexWithReplicasAndShards() throws IOException {\n    final String name = randomIndexName();\n    final int replicas = 2;\n    final int shards = 3;\n    elasticsearchService.createIndex(name, replicas, shards);\n    Assertions.assertTrue(elasticsearchService.hasIndex(name));\n\n    final GetIndexResponse response = elasticsearchService\n      .getClient()\n      .indices()\n      .get(b -> b.index(name));\n    Assertions.assertEquals(\n      shards + \"\",\n      response.get(name).settings().index().numberOfShards()\n    );\n    Assertions.assertEquals(\n      replicas + \"\",\n      response.get(name).settings().index().numberOfReplicas()\n    );\n  }\n\n  @Test\n  void testItCanDeleteAnIndex() {\n    final String name = randomIndexName();\n    elasticsearchService.createIndex(name);\n    Assertions.assertTrue(elasticsearchService.hasIndex(name));\n    elasticsearchService.deleteIndex(name);\n    Assertions.assertFalse(elasticsearchService.hasIndex(name));\n  }\n\n  @Test\n  void testItCanCloseAnIndex() {\n    final String name = randomIndexName();\n    elasticsearchService.createIndex(name);\n    Assertions.assertSame(\n      elasticsearchService.getIndicesStats(name).indices().get(name).status(),\n      IndexMetadataState.Open\n    );\n    elasticsearchService.closeIndex(name);\n    Assertions.assertSame(\n      elasticsearchService.getIndicesStats(name).indices().get(name).status(),\n      IndexMetadataState.Close\n    );\n  }\n\n  @Test\n  void testItCanOpenAnIndex() {\n    final String name = randomIndexName();\n    elasticsearchService.createIndex(name);\n    elasticsearchService.closeIndex(name);\n    Assertions.assertSame(\n      elasticsearchService.getIndicesStats(name).indices().get(name).status(),\n      IndexMetadataState.Close\n    );\n    elasticsearchService.openIndex(name);\n    Assertions.assertSame(\n      elasticsearchService.getIndicesStats(name).indices().get(name).status(),\n      IndexMetadataState.Open\n    );\n  }\n\n  @Test\n  void testItCanInsertData() throws IOException {\n    final String name = randomIndexName();\n    elasticsearchService.createIndex(name);\n    Assertions.assertTrue(elasticsearchService.hasIndex(name));\n\n    final List<JsonNode> data = List.of(\n      objectMapper.readTree(\"{\\\"testField\\\": \\\"object1\\\"}\"),\n      objectMapper.readTree(\"{\\\"testField\\\": \\\"object2\\\"}\")\n    );\n    elasticsearchService.insert(name, data);\n    elasticsearchService.refresh(name);\n    Assertions.assertEquals(2, (long) elasticsearchService.count(name));\n  }\n\n  @Test\n  void testItCanInsertDataWithIds() throws IOException {\n    final String name = randomIndexName();\n    elasticsearchService.createIndex(name);\n    Assertions.assertTrue(elasticsearchService.hasIndex(name));\n\n    final List<JsonNode> data = List.of(\n      objectMapper.readTree(\"{\\\"testField\\\": \\\"object1\\\"}\"),\n      objectMapper.readTree(\"{\\\"testField\\\": \\\"object2\\\"}\")\n    );\n    final List<String> ids = List.of(\"1\", \"2\");\n    elasticsearchService.insert(name, data, ids);\n    elasticsearchService.refresh(name);\n\n    final SearchResponse<JsonNode> response = elasticsearchService.search(\n      name,\n      Query.of(b -> b.matchAll(m -> m)),\n      JsonNode.class\n    );\n    Assertions.assertEquals(2L, response.hits().total().value());\n    Assertions.assertTrue(\n      response.hits().hits().stream().allMatch(hit -> ids.contains(hit.id()))\n    );\n  }\n\n  @Test\n  void testItOverwritesDataWithProvidedIds() throws IOException {\n    final String name = randomIndexName();\n    elasticsearchService.createIndex(name);\n    Assertions.assertTrue(elasticsearchService.hasIndex(name));\n\n    final List<JsonNode> data = List.of(\n      objectMapper.readTree(\"{\\\"testField\\\": \\\"object1\\\"}\"),\n      objectMapper.readTree(\"{\\\"testField\\\": \\\"object2\\\"}\")\n    );\n    final List<String> ids = List.of(\"1\", \"2\");\n    elasticsearchService.insert(name, data, ids);\n    elasticsearchService.refresh(name);\n\n    // Reinsert the data and ensure the documents are overwritten, implying that the ids are used\n    elasticsearchService.insert(name, data, ids);\n    elasticsearchService.refresh(name);\n\n    final SearchResponse<JsonNode> response = elasticsearchService.search(\n      name,\n      Query.of(b -> b.matchAll(m -> m)),\n      JsonNode.class\n    );\n    Assertions.assertEquals(2L, response.hits().total().value());\n    Assertions.assertTrue(\n      response.hits().hits().stream().allMatch(hit -> ids.contains(hit.id()))\n    );\n  }\n\n  @Test\n  void testItCanInsertDataAndPipelineAddsProperty() throws IOException {\n    final String name = randomIndexName();\n    elasticsearchService.createIndex(name);\n    Assertions.assertTrue(elasticsearchService.hasIndex(name));\n\n    final List<JsonNode> data = List.of(\n      objectMapper.createObjectNode().put(\"testField\", \"object1\"),\n      objectMapper.createObjectNode().put(\"testField\", \"object2\")\n    );\n    elasticsearchService.insert(name, data);\n    elasticsearchService.refresh(name);\n    final SearchResponse<JsonNode> response = elasticsearchService.search(\n      name,\n      Query.of(b -> b.matchAll(m -> m)),\n      JsonNode.class\n    );\n    Assertions.assertEquals(2L, response.hits().total().value());\n\n    Assertions.assertTrue(\n      response\n        .hits()\n        .hits()\n        .stream()\n        .allMatch(hit ->\n          hit.source().at(\"/analyzed_feature/full_text_search_data\").isArray()\n        )\n    );\n  }\n\n  @Test\n  void testItThrowsWhenIdsLengthAndDataLengthDoNotMatch()\n    throws JsonProcessingException {\n    final String name = randomIndexName();\n    elasticsearchService.createIndex(name);\n    Assertions.assertTrue(elasticsearchService.hasIndex(name));\n\n    final List<JsonNode> data = List.of(\n      objectMapper.readTree(\"{\\\"testField\\\": \\\"object1\\\"}\"),\n      objectMapper.readTree(\"{\\\"testField\\\": \\\"object2\\\"}\")\n    );\n    final List<String> ids = List.of(\"1\");\n    Assertions.assertThrows(IllegalArgumentException.class, () ->\n      elasticsearchService.insert(name, data, ids)\n    );\n  }\n\n  private String buildScrollTestIndex() throws IOException {\n    final String name = randomIndexName();\n    elasticsearchService.createIndex(name);\n    Assertions.assertTrue(elasticsearchService.hasIndex(name));\n\n    final List<JsonNode> data = new ArrayList<>();\n    for (int i = 0; i < 100; i++) {\n      data.add(\n        objectMapper.readTree(\n          \"\"\"\n          {\n            \"keyword_feature\": {\n              \"foo\": \"bar%d\"\n            }\n          }\n          \"\"\".formatted(i)\n        )\n      );\n    }\n\n    elasticsearchService.insert(name, data);\n    elasticsearchService.refresh(name);\n    return name;\n  }\n\n  private String buildScrollTestIndex2() throws IOException {\n    final String name = randomIndexName();\n    elasticsearchService.createIndex(name);\n    Assertions.assertTrue(elasticsearchService.hasIndex(name));\n\n    final List<JsonNode> data = new ArrayList<>();\n    for (int i = 0; i < 100; i++) {\n      data.add(\n        objectMapper.readTree(\n          \"\"\"\n          {\n            \"keyword_feature\": {\n              \"foo\": \"bar%d\"\n            },\n            \"double_feature\": {\n              \"foo\": %f\n            }\n          }\n          \"\"\".formatted(i, (float) (i % 2))\n        )\n      );\n    }\n\n    elasticsearchService.insert(name, data);\n    elasticsearchService.refresh(name);\n    return name;\n  }\n\n  private String buildScrollTestIndex3() throws IOException {\n    final String name = randomIndexName();\n    elasticsearchService.createIndex(name);\n    Assertions.assertTrue(elasticsearchService.hasIndex(name));\n\n    final List<JsonNode> data = new ArrayList<>();\n    for (int i = 0; i < 100; i++) {\n      data.add(\n        objectMapper.readTree(\n          \"\"\"\n          {\n            \"keyword_feature\": {\n              \"foo\": \"bar%d\"\n            }\n          }\n          \"\"\".formatted(i % 2)\n        )\n      );\n    }\n\n    elasticsearchService.insert(name, data);\n    elasticsearchService.refresh(name);\n    return name;\n  }\n\n  @Test\n  void testItThrowsIllegalArgumentWhenScrollRequestDoesntProvideASort() {\n    final SearchRequest request = SearchRequest.of(b ->\n      b.index(\"test\").size(10).query(QueryBuilders.matchAll(m -> m))\n    );\n    Assertions.assertThrows(IllegalArgumentException.class, () ->\n      elasticsearchService.scroll(request, JsonNode.class, data -> {})\n    );\n  }\n\n  @Test\n  void testItCanScrollDocumentsInPages() throws IOException {\n    final String name = buildScrollTestIndex();\n\n    final SearchRequest request = SearchRequest.of(b ->\n      b\n        .index(name)\n        .size(10)\n        .sort(s -> s.field(f -> f.field(\"keyword_feature.foo\")))\n        .query(QueryBuilders.matchAll(m -> m))\n    );\n    final List<Hit<JsonNode>> scrolledData = new ArrayList<>();\n    elasticsearchService.scroll(request, JsonNode.class, scrolledData::addAll);\n\n    Assertions.assertEquals(100, scrolledData.size());\n  }\n\n  @Test\n  void testItCanGetDocumentsInASinglePage() throws IOException {\n    final String name = buildScrollTestIndex();\n\n    final SearchRequest request = SearchRequest.of(b ->\n      b\n        .index(name)\n        .size(200)\n        .sort(s -> s.field(f -> f.field(\"keyword_feature.foo\")))\n        .query(QueryBuilders.matchAll(m -> m))\n    );\n    final List<Hit<JsonNode>> scrolledData = new ArrayList<>();\n    elasticsearchService.scroll(request, JsonNode.class, scrolledData::addAll);\n\n    Assertions.assertEquals(100, scrolledData.size());\n  }\n\n  @Test\n  void testItThrowsWhenSortFieldIsInvalid() throws IOException {\n    final String name = buildScrollTestIndex();\n\n    final SearchRequest request = SearchRequest.of(b ->\n      b\n        .index(name)\n        .size(10)\n        .sort(s -> s.field(f -> f.field(\"keyword_feature.bar\")))\n        .query(QueryBuilders.matchAll(m -> m))\n    );\n    Assertions.assertThrows(ElasticsearchException.class, () ->\n      elasticsearchService.scroll(request, JsonNode.class, data -> {})\n    );\n  }\n\n  @Test\n  void testItScrollsWhenNoSizeIsProvided() throws IOException {\n    final String name = buildScrollTestIndex();\n\n    final SearchRequest request = SearchRequest.of(b ->\n      b\n        .index(name)\n        .sort(s -> s.field(f -> f.field(\"keyword_feature.foo\")))\n        .query(QueryBuilders.matchAll(m -> m))\n    );\n    final List<Hit<JsonNode>> scrolledData = new ArrayList<>();\n    elasticsearchService.scroll(request, JsonNode.class, scrolledData::addAll);\n\n    Assertions.assertEquals(100, scrolledData.size());\n  }\n\n  @Test\n  void testItCanScrollWhenSortingOnANonUniqueNumericalField()\n    throws IOException {\n    final String name = buildScrollTestIndex2();\n\n    final SearchRequest request = SearchRequest.of(b ->\n      b\n        .index(name)\n        .size(10)\n        .sort(s -> s.field(f -> f.field(\"double_feature.foo\")))\n        .query(QueryBuilders.matchAll(m -> m))\n    );\n    final List<Hit<JsonNode>> scrolledData = new ArrayList<>();\n    elasticsearchService.scroll(request, JsonNode.class, scrolledData::addAll);\n\n    Assertions.assertEquals(100, scrolledData.size());\n  }\n\n  @Test\n  void testItCanScrollWhenSortingOnANonUniqueStringField() throws IOException {\n    final String name = buildScrollTestIndex3();\n\n    final SearchRequest request = SearchRequest.of(b ->\n      b\n        .index(name)\n        .size(10)\n        .sort(s -> s.field(f -> f.field(\"keyword_feature.foo\")))\n        .query(QueryBuilders.matchAll(m -> m))\n    );\n    final List<Hit<JsonNode>> scrolledData = new ArrayList<>();\n    elasticsearchService.scroll(request, JsonNode.class, scrolledData::addAll);\n\n    Assertions.assertEquals(100, scrolledData.size());\n  }\n\n  @Test\n  void testItCanCreateAnEnrichPolicy() throws IOException {\n    final String indexName = randomIndexName();\n    elasticsearchService.createIndex(indexName);\n    final List<JsonNode> data = List.of(\n      objectMapper.readTree(\n        \"{\\\"keyword_feature\\\": {\\\"id\\\": \\\"1\\\", \\\"name\\\": \\\"object1\\\", \\\"description\\\": \\\"description1\\\"}}\"\n      ),\n      objectMapper.readTree(\n        \"{\\\"keyword_feature\\\": {\\\"id\\\": \\\"2\\\", \\\"name\\\": \\\"object2\\\", \\\"description\\\": \\\"description2\\\"}}\"\n      )\n    );\n    elasticsearchService.insert(indexName, data);\n    elasticsearchService.refresh(indexName);\n\n    final String name = elasticsearchService.createEnrichPolicy(\n      indexName,\n      \"keyword_feature.id\",\n      List.of(\"keyword_feature.name\", \"keyword_feature.description\")\n    );\n    Assertions.assertNotNull(name);\n    Assertions.assertTrue(elasticsearchService.enrichPolicyExists(name));\n  }\n\n  @Test\n  void testItFailsWhenSourceIndexDoesNotExist() throws IOException {\n    final String indexName = randomIndexName();\n    elasticsearchService.createIndex(indexName);\n    final List<JsonNode> data = List.of(\n      objectMapper.readTree(\n        \"{\\\"keyword_feature\\\": {\\\"id\\\": \\\"1\\\", \\\"name\\\": \\\"object1\\\", \\\"description\\\": \\\"description1\\\"}}\"\n      ),\n      objectMapper.readTree(\n        \"{\\\"keyword_feature\\\": {\\\"id\\\": \\\"2\\\", \\\"name\\\": \\\"object2\\\", \\\"description\\\": \\\"description2\\\"}}\"\n      )\n    );\n    elasticsearchService.insert(indexName, data);\n    elasticsearchService.refresh(indexName);\n\n    Assertions.assertNull(\n      elasticsearchService.createEnrichPolicy(\n        indexName,\n        \"non_existent_field\",\n        List.of(\"keyword_feature.name\", \"keyword_feature.description\")\n      )\n    );\n  }\n\n  @Test\n  void testItThrowsCreatingEnrichPolicyWhenIndexNotFound() {\n    final String indexName = randomIndexName();\n    Assertions.assertFalse(elasticsearchService.hasIndex(indexName));\n    Assertions.assertThrows(ElasticsearchException.class, () ->\n      elasticsearchService.createEnrichPolicy(\n        indexName,\n        \"source-field\",\n        List.of(\"included-source-field1\", \"included-source-field2\")\n      )\n    );\n  }\n\n  @Test\n  void testItThrowsCreatingEnrichPolicyWhenNoMapping() {\n    final String indexName = UUID.randomUUID().toString(); // intentionally not using randomIndexName to ensure a template is not applied\n    elasticsearchService.createIndex(indexName);\n    Assertions.assertThrows(ElasticsearchException.class, () ->\n      elasticsearchService.createEnrichPolicy(\n        indexName,\n        \"source-field\",\n        List.of(\"included-source-field1\", \"included-source-field2\")\n      )\n    );\n  }\n\n  @Test\n  void testItDoesntRecreateTheSamePolicy() throws IOException {\n    final String indexName = randomIndexName();\n    elasticsearchService.createIndex(indexName);\n    final List<JsonNode> data = List.of(\n      objectMapper.readTree(\n        \"{\\\"keyword_feature\\\": {\\\"id\\\": \\\"1\\\", \\\"name\\\": \\\"object1\\\", \\\"description\\\": \\\"description1\\\"}}\"\n      ),\n      objectMapper.readTree(\n        \"{\\\"keyword_feature\\\": {\\\"id\\\": \\\"2\\\", \\\"name\\\": \\\"object2\\\", \\\"description\\\": \\\"description2\\\"}}\"\n      )\n    );\n    elasticsearchService.insert(indexName, data);\n    elasticsearchService.refresh(indexName);\n\n    final int countBefore = elasticsearchService\n      .getEnrichPolicies()\n      .policies()\n      .size();\n    final String name1 = elasticsearchService.createEnrichPolicy(\n      indexName,\n      \"keyword_feature.id\",\n      List.of(\"keyword_feature.name\", \"keyword_feature.description\")\n    );\n    final String name2 = elasticsearchService.createEnrichPolicy(\n      indexName,\n      \"keyword_feature.id\",\n      List.of(\"keyword_feature.name\", \"keyword_feature.description\")\n    );\n    final int countAfter = elasticsearchService\n      .getEnrichPolicies()\n      .policies()\n      .size();\n    Assertions.assertEquals(countBefore + 1, countAfter);\n    Assertions.assertEquals(name1, name2);\n  }\n\n  @Test\n  void testOrderingOfFieldsDoesNotAffectPolicyCreation() throws IOException {\n    final String indexName = randomIndexName();\n    elasticsearchService.createIndex(indexName);\n    final List<JsonNode> data = List.of(\n      objectMapper.readTree(\n        \"{\\\"keyword_feature\\\": {\\\"id\\\": \\\"1\\\", \\\"name\\\": \\\"object1\\\", \\\"description\\\": \\\"description1\\\"}}\"\n      ),\n      objectMapper.readTree(\n        \"{\\\"keyword_feature\\\": {\\\"id\\\": \\\"2\\\", \\\"name\\\": \\\"object2\\\", \\\"description\\\": \\\"description2\\\"}}\"\n      )\n    );\n    elasticsearchService.insert(indexName, data);\n    elasticsearchService.refresh(indexName);\n\n    final String name1 = elasticsearchService.createEnrichPolicy(\n      indexName,\n      \"keyword_feature.id\",\n      List.of(\"keyword_feature.description\", \"keyword_feature.name\")\n    );\n    final String name2 = elasticsearchService.createEnrichPolicy(\n      indexName,\n      \"keyword_feature.id\",\n      List.of(\"keyword_feature.name\", \"keyword_feature.description\")\n    );\n    Assertions.assertEquals(name1, name2);\n  }\n\n  @Test\n  void testItCanDeleteAnEnrichPolicy() throws IOException {\n    final String indexName = randomIndexName();\n    elasticsearchService.createIndex(indexName);\n    final List<JsonNode> data = List.of(\n      objectMapper.readTree(\n        \"{\\\"keyword_feature\\\": {\\\"id\\\": \\\"1\\\", \\\"name\\\": \\\"object1\\\", \\\"description\\\": \\\"description1\\\"}}\"\n      ),\n      objectMapper.readTree(\n        \"{\\\"keyword_feature\\\": {\\\"id\\\": \\\"2\\\", \\\"name\\\": \\\"object2\\\", \\\"description\\\": \\\"description2\\\"}}\"\n      )\n    );\n    elasticsearchService.insert(indexName, data);\n    elasticsearchService.refresh(indexName);\n\n    final String name = elasticsearchService.createEnrichPolicy(\n      indexName,\n      \"keyword_feature.id\",\n      List.of(\"keyword_feature.name\", \"keyword_feature.description\")\n    );\n    Assertions.assertTrue(elasticsearchService.enrichPolicyExists(name));\n    elasticsearchService.deleteEnrichPolicy(name);\n    Assertions.assertFalse(elasticsearchService.enrichPolicyExists(name));\n  }\n\n  @Test\n  public void testItCanExecuteAnEnrichPolicy()\n    throws InterruptedException, IOException {\n    final String indexName = randomIndexName();\n    elasticsearchService.createIndex(indexName);\n    // Create a set of 20000 documents to ensure the enrich policy is executed in multiple batches\n    final List<JsonNode> data = new ArrayList<>();\n    for (int i = 0; i < 20000; i++) {\n      data.add(\n        objectMapper\n          .createObjectNode()\n          .put(\"keyword_feature\", String.valueOf(i))\n      );\n    }\n    elasticsearchService.insert(indexName, data);\n    elasticsearchService.refresh(indexName);\n\n    final String policyName = elasticsearchService.createEnrichPolicy(\n      indexName,\n      \"keyword_feature\",\n      List.of(\"keyword_feature\")\n    );\n    final String taskId = elasticsearchService.executeEnrichPolicy(policyName);\n    Assertions.assertNotNull(taskId);\n    Assertions.assertDoesNotThrow(() ->\n      elasticsearchService.waitForEnrichTaskCompletion(taskId, 10).join()\n    );\n    Assertions.assertEquals(\n      elasticsearchService.count(indexName),\n      elasticsearchService.count(\".enrich-\" + policyName)\n    );\n  }\n\n  @Test\n  void testItCanUpdateAllByQuery() throws IOException, InterruptedException {\n    final String pipeline =\n      \"\"\"\n      {\n        \"processors\": [\n          {\n            \"script\": {\n              \"source\": \"ctx.keyword_feature.name = 'updated'\",\n              \"lang\": \"painless\"\n            }\n          }\n        ]\n      }\n      \"\"\";\n    final String pipelineName = UUID.randomUUID().toString();\n    elasticsearchService.putPipeline(\n      pipelineName,\n      new ByteArrayInputStream(pipeline.getBytes())\n    );\n\n    final String indexName = randomIndexName();\n    elasticsearchService.createIndex(indexName);\n    final List<JsonNode> data = List.of(\n      objectMapper.readTree(\n        \"{\\\"keyword_feature\\\": {\\\"id\\\": \\\"1\\\", \\\"name\\\": \\\"object1\\\", \\\"description\\\": \\\"description1\\\"}}\"\n      ),\n      objectMapper.readTree(\n        \"{\\\"keyword_feature\\\": {\\\"id\\\": \\\"2\\\", \\\"name\\\": \\\"object2\\\", \\\"description\\\": \\\"description2\\\"}}\"\n      )\n    );\n    elasticsearchService.insert(indexName, data);\n    elasticsearchService.refresh(indexName);\n\n    final String taskId = elasticsearchService.updateByQuery(\n      indexName,\n      pipelineName\n    );\n    Thread.sleep(1000); // Wait for the task to start\n    Assertions.assertDoesNotThrow(() ->\n      elasticsearchService.waitForTaskCompletion(taskId, 10).join()\n    );\n\n    // Ensure the documents were updated\n    final SearchResponse<JsonNode> response = elasticsearchService.search(\n      indexName,\n      Query.of(b -> b.matchAll(m -> m)),\n      JsonNode.class\n    );\n    Assertions.assertEquals(2L, response.hits().total().value());\n    Assertions.assertTrue(\n      response\n        .hits()\n        .hits()\n        .stream()\n        .allMatch(hit ->\n          hit.source().at(\"/keyword_feature/name\").asText().equals(\"updated\")\n        )\n    );\n  }\n\n  @Test\n  void testItCanUpdateASubsetByQuery()\n    throws IOException, InterruptedException {\n    final String pipeline =\n      \"\"\"\n      {\n        \"processors\": [\n          {\n            \"script\": {\n              \"source\": \"ctx.keyword_feature.name = 'updated'\",\n              \"lang\": \"painless\"\n            }\n          }\n        ]\n      }\n      \"\"\";\n    final String pipelineName = UUID.randomUUID().toString();\n    elasticsearchService.putPipeline(\n      pipelineName,\n      new ByteArrayInputStream(pipeline.getBytes())\n    );\n\n    final String indexName = randomIndexName();\n    elasticsearchService.createIndex(indexName);\n    final List<JsonNode> data = List.of(\n      objectMapper.readTree(\n        \"{\\\"keyword_feature\\\": {\\\"id\\\": \\\"1\\\", \\\"name\\\": \\\"object1\\\", \\\"description\\\": \\\"description1\\\"}}\"\n      ),\n      objectMapper.readTree(\n        \"{\\\"keyword_feature\\\": {\\\"id\\\": \\\"2\\\", \\\"name\\\": \\\"object2\\\", \\\"description\\\": \\\"description2\\\"}}\"\n      )\n    );\n    elasticsearchService.insert(indexName, data);\n    elasticsearchService.refresh(indexName);\n\n    final String taskId = elasticsearchService.updateByQuery(\n      indexName,\n      \"{\\\"match\\\": {\\\"keyword_feature.id\\\": \\\"1\\\"}}\",\n      pipelineName\n    );\n    Thread.sleep(1000); // Wait for the task to start\n    Assertions.assertDoesNotThrow(() ->\n      elasticsearchService.waitForTaskCompletion(taskId, 10).join()\n    );\n\n    // Ensure the documents were updated\n    final SearchResponse<JsonNode> response = elasticsearchService.search(\n      indexName,\n      Query.of(b -> b.matchAll(m -> m)),\n      JsonNode.class\n    );\n    // Assert that only the document with id 1 was updated\n    final List<Hit<JsonNode>> hits = response.hits().hits();\n    Assertions.assertEquals(2, hits.size());\n    final JsonNode doc1 = hits.get(0).source();\n    final JsonNode doc2 = hits.get(1).source();\n\n    // Assert only one of the documents was updated\n    if (doc1.at(\"/keyword_feature/id\").asText().equals(\"1\")) {\n      Assertions.assertEquals(\n        \"updated\",\n        doc1.at(\"/keyword_feature/name\").asText()\n      );\n      Assertions.assertTrue(\n        doc2.at(\"/keyword_feature/name\").asText().startsWith(\"object\")\n      );\n    } else {\n      Assertions.assertEquals(\n        \"updated\",\n        doc2.at(\"/keyword_feature/name\").asText()\n      );\n      Assertions.assertTrue(\n        doc1.at(\"/keyword_feature/name\").asText().startsWith(\"object\")\n      );\n    }\n  }\n\n  @Test\n  void testItCanGetRunningTasks() {\n    final List<JsonNode> tasks = elasticsearchService.getRunningTasks();\n    Assertions.assertNotNull(tasks);\n  }\n\n  @Test\n  void testItCanCreateAnEnrichmentPipeline() throws IOException {\n    final String enrichmentSourceIndexName = randomIndexName();\n    elasticsearchService.createIndex(enrichmentSourceIndexName);\n    final List<JsonNode> sourceData = List.of(\n      objectMapper.readTree(\n        \"{\\\"keyword_feature\\\": {\\\"id\\\": \\\"1\\\", \\\"name\\\": \\\"object1\\\", \\\"description\\\": \\\"description1\\\"}}\"\n      ),\n      objectMapper.readTree(\n        \"{\\\"keyword_feature\\\": {\\\"id\\\": \\\"2\\\", \\\"name\\\": \\\"object2\\\", \\\"description\\\": \\\"description2\\\"}}\"\n      )\n    );\n    elasticsearchService.insert(enrichmentSourceIndexName, sourceData);\n    elasticsearchService.refresh(enrichmentSourceIndexName);\n\n    final String enrichmentDestinationIndexName = randomIndexName();\n    elasticsearchService.createIndex(enrichmentDestinationIndexName);\n    final List<JsonNode> destinationData = List.of(\n      objectMapper.readTree(\n        \"{\\\"keyword_feature\\\": {\\\"destination_id\\\": \\\"1\\\"}}\"\n      ),\n      objectMapper.readTree(\n        \"{\\\"keyword_feature\\\": {\\\"destination_id\\\": \\\"2\\\"}}\"\n      )\n    );\n    elasticsearchService.insert(\n      enrichmentDestinationIndexName,\n      destinationData\n    );\n    elasticsearchService.refresh(enrichmentDestinationIndexName);\n\n    // Create and execute an enrichment policy\n    final String policyName = elasticsearchService.createEnrichPolicy(\n      enrichmentSourceIndexName,\n      \"keyword_feature.id\",\n      List.of(\"keyword_feature.name\", \"keyword_feature.description\")\n    );\n    final String taskId = elasticsearchService.executeEnrichPolicy(policyName);\n    Assertions.assertNotNull(taskId);\n    Assertions.assertDoesNotThrow(() ->\n      elasticsearchService.waitForEnrichTaskCompletion(taskId, 10).join()\n    );\n\n    // Create a pipeline for the policy\n    final String enrichmentPipelineName =\n      elasticsearchService.createEnrichPipeline(\n        policyName,\n        \"keyword_feature.destination_id\"\n      );\n    Assertions.assertNotNull(enrichmentPipelineName);\n\n    // Execute the pipeline\n    final String pipelineTaskId = elasticsearchService.updateByQuery(\n      enrichmentDestinationIndexName,\n      enrichmentPipelineName\n    );\n    Assertions.assertNotNull(pipelineTaskId);\n    Assertions.assertDoesNotThrow(() ->\n      elasticsearchService.waitForTaskCompletion(pipelineTaskId, 10).join()\n    );\n    elasticsearchService.refresh(enrichmentDestinationIndexName);\n\n    // Ensure that the destination index was enriched\n    final SearchResponse<JsonNode> response = elasticsearchService.search(\n      enrichmentDestinationIndexName,\n      Query.of(b -> b.matchAll(m -> m)),\n      JsonNode.class\n    );\n    Assertions.assertEquals(2L, response.hits().total().value());\n    Assertions.assertTrue(\n      response\n        .hits()\n        .hits()\n        .stream()\n        .allMatch(\n          hit ->\n            hit.source().at(\"/keyword_feature/name\").isTextual() &&\n            hit.source().at(\"/keyword_feature/description\").isTextual()\n        )\n    );\n  }\n\n  @Test\n  public void testItCanEnrichAndDeduplicateArrayFields() throws IOException {\n    final String enrichmentSourceIndexName = randomIndexName();\n    elasticsearchService.createIndex(enrichmentSourceIndexName);\n    final List<JsonNode> sourceData = List.of(\n      objectMapper.readTree(\n        \"{\\\"keyword_feature\\\": {\\\"id\\\": \\\"1\\\", \\\"name\\\": [\\\"object1\\\", \\\"object2\\\"], \\\"description\\\": \\\"description1\\\"}}\"\n      ),\n      objectMapper.readTree(\n        \"{\\\"keyword_feature\\\": {\\\"id\\\": \\\"2\\\", \\\"name\\\": [\\\"object4\\\", \\\"object5\\\"], \\\"description\\\": \\\"description2\\\"}}\"\n      )\n    );\n    elasticsearchService.insert(enrichmentSourceIndexName, sourceData);\n    elasticsearchService.refresh(enrichmentSourceIndexName);\n\n    final String enrichmentDestinationIndexName = randomIndexName();\n    elasticsearchService.createIndex(enrichmentDestinationIndexName);\n    final List<JsonNode> destinationData = List.of(\n      objectMapper.readTree(\n        \"{\\\"keyword_feature\\\": {\\\"destination_id\\\": \\\"1\\\", \\\"name\\\": [\\\"object2\\\", \\\"object3\\\"]}}\"\n      ),\n      objectMapper.readTree(\n        \"{\\\"keyword_feature\\\": {\\\"destination_id\\\": \\\"2\\\"}}\"\n      )\n    );\n    elasticsearchService.insert(\n      enrichmentDestinationIndexName,\n      destinationData\n    );\n    elasticsearchService.refresh(enrichmentDestinationIndexName);\n\n    // Create and execute an enrichment policy\n    final String policyName = elasticsearchService.createEnrichPolicy(\n      enrichmentSourceIndexName,\n      \"keyword_feature.id\",\n      List.of(\"keyword_feature.name\", \"keyword_feature.description\")\n    );\n    final String taskId = elasticsearchService.executeEnrichPolicy(policyName);\n    Assertions.assertNotNull(taskId);\n    Assertions.assertDoesNotThrow(() ->\n      elasticsearchService.waitForEnrichTaskCompletion(taskId, 10).join()\n    );\n\n    // Create a pipeline for the policy\n    final String enrichmentPipelineName =\n      elasticsearchService.createEnrichPipeline(\n        policyName,\n        \"keyword_feature.destination_id\"\n      );\n    Assertions.assertNotNull(enrichmentPipelineName);\n\n    // Execute the pipeline\n    final String pipelineTaskId = elasticsearchService.updateByQuery(\n      enrichmentDestinationIndexName,\n      enrichmentPipelineName\n    );\n    Assertions.assertNotNull(pipelineTaskId);\n    Assertions.assertDoesNotThrow(() ->\n      elasticsearchService.waitForTaskCompletion(pipelineTaskId, 10).join()\n    );\n    elasticsearchService.refresh(enrichmentDestinationIndexName);\n\n    // Ensure that the destination index was enriched and that the array field was enriched and deduplicated\n    final SearchResponse<JsonNode> response = elasticsearchService.search(\n      enrichmentDestinationIndexName,\n      Query.of(b ->\n        b.term(tb -> tb.field(\"keyword_feature.destination_id\").value(\"1\"))\n      ),\n      JsonNode.class\n    );\n    Assertions.assertEquals(1L, response.hits().total().value());\n    final JsonNode hit = response.hits().hits().get(0).source();\n    Assertions.assertTrue(hit.at(\"/keyword_feature/name\").isArray());\n    Assertions.assertEquals(3, hit.at(\"/keyword_feature/name\").size());\n    final Set<String> elements = Set.of(\n      hit.at(\"/keyword_feature/name/0\").asText(),\n      hit.at(\"/keyword_feature/name/1\").asText(),\n      hit.at(\"/keyword_feature/name/2\").asText()\n    );\n    Assertions.assertTrue(elements.contains(\"object1\"));\n    Assertions.assertTrue(elements.contains(\"object2\"));\n    Assertions.assertTrue(elements.contains(\"object3\"));\n  }\n\n  @Test\n  public void testItCanUpsertDataAndPipelineAddsProperty()\n    throws IOException, InterruptedException {\n    final String name = randomIndexName();\n    elasticsearchService.createIndex(name);\n    Assertions.assertTrue(elasticsearchService.hasIndex(name));\n\n    // Initial upsert with some test data\n    final List<JsonNode> initialData = List.of(\n      objectMapper\n        .createObjectNode()\n        .put(\"testField\", \"object1\")\n        .put(\"title\", \"Test Document 1\"),\n      objectMapper\n        .createObjectNode()\n        .put(\"testField\", \"object2\")\n        .put(\"title\", \"Test Document 2\")\n    );\n    final List<String> ids = List.of(\"test-doc-1\", \"test-doc-2\");\n\n    elasticsearchService.upsert(name, initialData, ids);\n\n    // Wait for async full_text_search refresh to complete\n    Thread.sleep(1000);\n\n    elasticsearchService.refresh(name);\n\n    // Verify documents were upserted and pipeline ran\n    SearchResponse<JsonNode> response = elasticsearchService.search(\n      name,\n      Query.of(b -> b.matchAll(m -> m)),\n      JsonNode.class\n    );\n    Assertions.assertEquals(2L, response.hits().total().value());\n\n    // Verify full_text_search_data was created by pipeline\n    Assertions.assertTrue(\n      response\n        .hits()\n        .hits()\n        .stream()\n        .allMatch(\n          hit ->\n            hit\n              .source()\n              .at(\"/analyzed_feature/full_text_search_data\")\n              .isArray() &&\n            hit.source().at(\"/analyzed_feature/full_text_search_data\").size() >\n            0\n        )\n    );\n\n    // Verify the full_text_search_data contains our test fields\n    response\n      .hits()\n      .hits()\n      .forEach(hit -> {\n        ArrayNode fullTextData = (ArrayNode) hit\n          .source()\n          .at(\"/analyzed_feature/full_text_search_data\");\n        List<String> searchData = new ArrayList<>();\n        fullTextData.forEach(node -> searchData.add(node.asText()));\n\n        // Should contain values from testField and title\n        boolean containsTestField = searchData\n          .stream()\n          .anyMatch(s -> s.startsWith(\"object\"));\n        boolean containsTitle = searchData\n          .stream()\n          .anyMatch(s -> s.contains(\"Test Document\"));\n\n        Assertions.assertTrue(\n          containsTestField,\n          \"full_text_search_data should contain testField values\"\n        );\n        Assertions.assertTrue(\n          containsTitle,\n          \"full_text_search_data should contain title values\"\n        );\n      });\n  }\n\n  @Test\n  public void testItCanUpsertNewDocumentsAndPipelineAddsProperty()\n    throws IOException, InterruptedException {\n    final String name = randomIndexName();\n    elasticsearchService.createIndex(name);\n    Assertions.assertTrue(elasticsearchService.hasIndex(name));\n\n    // Initial upsert with some test data\n    final List<JsonNode> initialData = List.of(\n      objectMapper\n        .createObjectNode()\n        .put(\"testField\", \"object1\")\n        .put(\"title\", \"Test Document 1\"),\n      objectMapper\n        .createObjectNode()\n        .put(\"testField\", \"object2\")\n        .put(\"title\", \"Test Document 2\")\n    );\n    final List<String> ids = List.of(\"test-doc-1\", \"test-doc-2\");\n\n    elasticsearchService.upsert(name, initialData, ids);\n\n    // Wait for async full_text_search refresh\n    Thread.sleep(1000);\n    elasticsearchService.refresh(name);\n\n    // Verify documents were upserted and pipeline ran\n    SearchResponse<JsonNode> response = elasticsearchService.search(\n      name,\n      Query.of(b -> b.matchAll(m -> m)),\n      JsonNode.class\n    );\n    Assertions.assertEquals(2L, response.hits().total().value());\n\n    // Verify full_text_search_data was created by pipeline\n    Assertions.assertTrue(\n      response\n        .hits()\n        .hits()\n        .stream()\n        .allMatch(\n          hit ->\n            hit\n              .source()\n              .at(\"/analyzed_feature/full_text_search_data\")\n              .isArray() &&\n            hit.source().at(\"/analyzed_feature/full_text_search_data\").size() >\n            0\n        )\n    );\n\n    // Verify the full_text_search_data contains our test fields\n    response\n      .hits()\n      .hits()\n      .forEach(hit -> {\n        ArrayNode fullTextData = (ArrayNode) hit\n          .source()\n          .at(\"/analyzed_feature/full_text_search_data\");\n        List<String> searchData = new ArrayList<>();\n        fullTextData.forEach(node -> searchData.add(node.asText()));\n\n        // Should contain values from testField and title\n        boolean containsTestField = searchData\n          .stream()\n          .anyMatch(s -> s.startsWith(\"object\"));\n        boolean containsTitle = searchData\n          .stream()\n          .anyMatch(s -> s.contains(\"Test Document\"));\n\n        Assertions.assertTrue(\n          containsTestField,\n          \"full_text_search_data should contain testField values\"\n        );\n        Assertions.assertTrue(\n          containsTitle,\n          \"full_text_search_data should contain title values\"\n        );\n      });\n  }\n\n  @Test\n  public void testItCanUpdateExistingDocumentsAndRefreshFullTextSearch()\n    throws IOException, InterruptedException {\n    final String name = randomIndexName();\n    elasticsearchService.createIndex(name);\n\n    // Insert initial documents\n    final List<JsonNode> initialData = List.of(\n      objectMapper\n        .createObjectNode()\n        .put(\"testField\", \"object1\")\n        .put(\"title\", \"Test Document 1\")\n    );\n    final List<String> ids = List.of(\"test-doc-1\");\n\n    elasticsearchService.upsert(name, initialData, ids);\n    Thread.sleep(1000);\n    elasticsearchService.refresh(name);\n\n    // Update existing document\n    final List<JsonNode> updateData = List.of(\n      objectMapper\n        .createObjectNode()\n        .put(\"testField\", \"object1-updated\")\n        .put(\"title\", \"Updated Test Document 1\")\n        .put(\"newField\", \"additional data\")\n    );\n\n    elasticsearchService.upsert(name, updateData, ids);\n    Thread.sleep(1000);\n    elasticsearchService.refresh(name);\n\n    // Verify document was updated and pipeline ran again\n    SearchResponse<JsonNode> response = elasticsearchService.search(\n      name,\n      Query.of(b -> b.matchAll(m -> m)),\n      JsonNode.class\n    );\n    Assertions.assertEquals(1L, response.hits().total().value());\n\n    // Verify full_text_search_data was updated with new content\n    Hit<JsonNode> hit = response.hits().hits().get(0);\n    ArrayNode fullTextData = (ArrayNode) hit\n      .source()\n      .at(\"/analyzed_feature/full_text_search_data\");\n    List<String> searchData = new ArrayList<>();\n    fullTextData.forEach(node -> searchData.add(node.asText()));\n\n    // Should contain updated values\n    boolean containsUpdatedTestField = searchData\n      .stream()\n      .anyMatch(s -> s.contains(\"updated\"));\n    boolean containsUpdatedTitle = searchData\n      .stream()\n      .anyMatch(s -> s.contains(\"Updated Test Document\"));\n    boolean containsNewField = searchData\n      .stream()\n      .anyMatch(s -> s.contains(\"additional data\"));\n\n    Assertions.assertTrue(\n      containsUpdatedTestField,\n      \"full_text_search_data should contain updated testField values\"\n    );\n    Assertions.assertTrue(\n      containsUpdatedTitle,\n      \"full_text_search_data should contain updated title values\"\n    );\n    Assertions.assertTrue(\n      containsNewField,\n      \"full_text_search_data should contain new field values\"\n    );\n\n    // Should NOT contain old values (since this is an upsert/replace operation)\n    boolean containsOldTitle = searchData\n      .stream()\n      .anyMatch(s -> s.equals(\"Test Document 1\"));\n    Assertions.assertFalse(\n      containsOldTitle,\n      \"full_text_search_data should not contain old title values after upsert\"\n    );\n  }\n\n  @Test\n  public void testItCanUpsertMixedNewAndExistingDocuments()\n    throws IOException, InterruptedException {\n    final String name = randomIndexName();\n    elasticsearchService.createIndex(name);\n\n    // Insert initial document\n    final List<JsonNode> initialData = List.of(\n      objectMapper\n        .createObjectNode()\n        .put(\"testField\", \"existing\")\n        .put(\"title\", \"Existing Document\")\n    );\n    final List<String> existingIds = List.of(\"existing-doc\");\n\n    elasticsearchService.upsert(name, initialData, existingIds);\n    Thread.sleep(1000);\n    elasticsearchService.refresh(name);\n\n    // Verify initial state\n    SearchResponse<JsonNode> initialResponse = elasticsearchService.search(\n      name,\n      Query.of(b -> b.matchAll(m -> m)),\n      JsonNode.class\n    );\n    Assertions.assertEquals(1L, initialResponse.hits().total().value());\n\n    // Upsert with both new and existing documents\n    final List<JsonNode> mixedData = List.of(\n      objectMapper\n        .createObjectNode()\n        .put(\"testField\", \"existing-updated\")\n        .put(\"title\", \"Updated Existing Document\"), // Update existing\n      objectMapper\n        .createObjectNode()\n        .put(\"testField\", \"new\")\n        .put(\"title\", \"New Document\") // Insert new\n    );\n    final List<String> mixedIds = List.of(\"existing-doc\", \"new-doc\");\n\n    elasticsearchService.upsert(name, mixedData, mixedIds);\n    Thread.sleep(1000);\n    elasticsearchService.refresh(name);\n\n    // Verify both documents exist with proper full_text_search_data\n    SearchResponse<JsonNode> finalResponse = elasticsearchService.search(\n      name,\n      Query.of(b -> b.matchAll(m -> m)),\n      JsonNode.class\n    );\n    Assertions.assertEquals(2L, finalResponse.hits().total().value());\n\n    // Verify both documents have full_text_search_data\n    finalResponse\n      .hits()\n      .hits()\n      .forEach(hit -> {\n        ArrayNode fullTextData = (ArrayNode) hit\n          .source()\n          .at(\"/analyzed_feature/full_text_search_data\");\n        Assertions.assertTrue(\n          fullTextData.size() > 0,\n          \"Document should have full_text_search_data\"\n        );\n\n        List<String> searchData = new ArrayList<>();\n        fullTextData.forEach(node -> searchData.add(node.asText()));\n\n        if (hit.id().equals(\"existing-doc\")) {\n          // Updated existing document\n          boolean containsUpdated = searchData\n            .stream()\n            .anyMatch(s -> s.contains(\"existing-updated\"));\n          boolean containsUpdatedTitle = searchData\n            .stream()\n            .anyMatch(s -> s.contains(\"Updated Existing\"));\n          Assertions.assertTrue(\n            containsUpdated,\n            \"Updated document should contain new values\"\n          );\n          Assertions.assertTrue(\n            containsUpdatedTitle,\n            \"Updated document should contain new title\"\n          );\n        } else if (hit.id().equals(\"new-doc\")) {\n          // New document\n          boolean containsNew = searchData\n            .stream()\n            .anyMatch(s -> s.contains(\"new\"));\n          boolean containsNewTitle = searchData\n            .stream()\n            .anyMatch(s -> s.contains(\"New Document\"));\n          Assertions.assertTrue(\n            containsNew,\n            \"New document should contain its values\"\n          );\n          Assertions.assertTrue(\n            containsNewTitle,\n            \"New document should contain its title\"\n          );\n        }\n      });\n  }\n\n  @Test\n  public void testFullTextSearchDataContainsAllDocumentFields()\n    throws IOException, InterruptedException {\n    final String name = randomIndexName();\n    elasticsearchService.createIndex(name);\n\n    // Create document with various field types\n    final ObjectNode complexDocument = objectMapper\n      .createObjectNode()\n      .put(\"stringField\", \"test string\")\n      .put(\"numberField\", 42)\n      .put(\"booleanField\", true);\n\n    // Add array field\n    complexDocument.set(\n      \"arrayField\",\n      objectMapper.createArrayNode().add(\"item1\").add(\"item2\").add(\"item3\")\n    );\n\n    // Add object field\n    complexDocument.set(\n      \"objectField\",\n      objectMapper\n        .createObjectNode()\n        .put(\"nestedString\", \"nested value\")\n        .put(\"nestedNumber\", 123)\n    );\n\n    elasticsearchService.upsert(\n      name,\n      List.of(complexDocument),\n      List.of(\"complex-doc\")\n    );\n    Thread.sleep(1000);\n    elasticsearchService.refresh(name);\n\n    // Verify full_text_search_data contains all field values\n    SearchResponse<JsonNode> response = elasticsearchService.search(\n      name,\n      Query.of(b -> b.matchAll(m -> m)),\n      JsonNode.class\n    );\n    Assertions.assertEquals(1L, response.hits().total().value());\n\n    Hit<JsonNode> hit = response.hits().hits().get(0);\n    ArrayNode fullTextData = (ArrayNode) hit\n      .source()\n      .at(\"/analyzed_feature/full_text_search_data\");\n    List<String> searchData = new ArrayList<>();\n    fullTextData.forEach(node -> searchData.add(node.asText()));\n\n    // Verify various field types are included\n    Assertions.assertTrue(\n      searchData.contains(\"test string\"),\n      \"Should contain string field\"\n    );\n    Assertions.assertTrue(\n      searchData.contains(\"42\"),\n      \"Should contain number field as string\"\n    );\n    Assertions.assertTrue(\n      searchData.contains(\"true\"),\n      \"Should contain boolean field as string\"\n    );\n    Assertions.assertTrue(\n      searchData.contains(\"item1\"),\n      \"Should contain array items\"\n    );\n    Assertions.assertTrue(\n      searchData.contains(\"item2\"),\n      \"Should contain array items\"\n    );\n    Assertions.assertTrue(\n      searchData.contains(\"item3\"),\n      \"Should contain array items\"\n    );\n    Assertions.assertTrue(\n      searchData.contains(\"nested value\"),\n      \"Should contain nested object values\"\n    );\n    Assertions.assertTrue(\n      searchData.contains(\"123\"),\n      \"Should contain nested number as string\"\n    );\n\n    // Verify analyzed_feature fields are NOT included\n    boolean containsAnalyzedFeature = searchData\n      .stream()\n      .anyMatch(\n        s ->\n          s.contains(\"analyzed_feature\") || s.contains(\"full_text_search_data\")\n      );\n    Assertions.assertFalse(\n      containsAnalyzedFeature,\n      \"Should not contain analyzed_feature data\"\n    );\n  }\n\n  /**\n   * Helper class for testing deletion criteria\n   */\n  private static class TestDeletionCriteria {\n\n    private String propertyKey;\n    private String exactValue;\n    private String minValue;\n    private String maxValue;\n\n    public TestDeletionCriteria(\n      String propertyKey,\n      String exactValue,\n      String minValue,\n      String maxValue\n    ) {\n      this.propertyKey = propertyKey;\n      this.exactValue = exactValue;\n      this.minValue = minValue;\n      this.maxValue = maxValue;\n    }\n\n    public String getPropertyKey() {\n      return propertyKey;\n    }\n\n    public String getExactValue() {\n      return exactValue;\n    }\n\n    public String getMinValue() {\n      return minValue;\n    }\n\n    public String getMaxValue() {\n      return maxValue;\n    }\n  }\n\n  @Test\n  void testBulkRemovePropertyValues_NullCriteria() {\n    final String index = randomIndexName();\n    elasticsearchService.createIndex(index);\n\n    final List<String> documentIds = List.of(\"doc1\", \"doc2\");\n\n    Assertions.assertThrows(IllegalArgumentException.class, () ->\n      elasticsearchService.bulkRemovePropertyValues(index, null, documentIds)\n    );\n  }\n\n  @Test\n  void testBulkRemovePropertyValues_EmptyCriteria() {\n    final String index = randomIndexName();\n    elasticsearchService.createIndex(index);\n\n    final List<String> documentIds = List.of(\"doc1\", \"doc2\");\n    final List<Object> emptyCriteria = List.of();\n\n    Assertions.assertThrows(IllegalArgumentException.class, () ->\n      elasticsearchService.bulkRemovePropertyValues(\n        index,\n        emptyCriteria,\n        documentIds\n      )\n    );\n  }\n\n  @Test\n  void testBulkRemovePropertyValues_NullDocumentIds() {\n    final String index = randomIndexName();\n    elasticsearchService.createIndex(index);\n\n    final List<Object> criteria = List.of(\n      new TestDeletionCriteria(\"testProperty\", \"valueToRemove\", null, null)\n    );\n\n    Assertions.assertThrows(IllegalArgumentException.class, () ->\n      elasticsearchService.bulkRemovePropertyValues(index, criteria, null)\n    );\n  }\n\n  @Test\n  void testBulkRemovePropertyValues_EmptyDocumentIds() {\n    final String index = randomIndexName();\n    elasticsearchService.createIndex(index);\n\n    final List<Object> criteria = List.of(\n      new TestDeletionCriteria(\"testProperty\", \"valueToRemove\", null, null)\n    );\n    final List<String> emptyDocumentIds = List.of();\n\n    Assertions.assertThrows(IllegalArgumentException.class, () ->\n      elasticsearchService.bulkRemovePropertyValues(\n        index,\n        criteria,\n        emptyDocumentIds\n      )\n    );\n  }\n\n  @Test\n  void testBulkRemovePropertyValues_SuccessfulRemoval()\n    throws IOException, InterruptedException {\n    final String index = randomIndexName();\n    elasticsearchService.createIndex(index);\n\n    // Create test documents with _feature namespaces\n    final List<JsonNode> testData = List.of(\n      objectMapper.readTree(\n        \"\"\"\n        {\n          \"title\": \"Document 1\",\n          \"keyword_feature\": {\n            \"category\": [\"science\", \"technology\"],\n            \"author\": \"John Doe\"\n          }\n        }\n        \"\"\"\n      ),\n      objectMapper.readTree(\n        \"\"\"\n        {\n          \"title\": \"Document 2\",\n          \"keyword_feature\": {\n            \"category\": [\"science\", \"history\"],\n            \"language\": \"English\"\n          }\n        }\n        \"\"\"\n      ),\n      objectMapper.readTree(\n        \"\"\"\n        {\n          \"title\": \"Document 3\",\n          \"keyword_feature\": {\n            \"author\": \"Jane Smith\",\n            \"language\": \"French\"\n          }\n        }\n        \"\"\"\n      )\n    );\n\n    final List<String> documentIds = List.of(\"doc1\", \"doc2\", \"doc3\");\n    elasticsearchService.insert(index, testData, documentIds);\n    elasticsearchService.refresh(index);\n\n    // Debug: Check document structure before removal\n    SearchResponse<JsonNode> beforeResponse = elasticsearchService.search(\n      index,\n      Query.of(b -> b.matchAll(m -> m)),\n      JsonNode.class\n    );\n    System.out.println(\"=== BEFORE REMOVAL ===\");\n    for (Hit<JsonNode> hit : beforeResponse.hits().hits()) {\n      System.out.println(\"Doc \" + hit.id() + \": \" + hit.source().toString());\n    }\n\n    // Remove \"science\" values from \"category\" property in doc1 and doc2\n    final List<Object> criteria = List.of(\n      new TestDeletionCriteria(\"category\", \"science\", null, null)\n    );\n    final String taskId = elasticsearchService.bulkRemovePropertyValues(\n      index,\n      criteria,\n      List.of(\"doc1\", \"doc2\")\n    );\n\n    Assertions.assertNotNull(taskId);\n    Assertions.assertFalse(taskId.isEmpty());\n\n    // Wait for task completion\n    Assertions.assertDoesNotThrow(() ->\n      elasticsearchService.waitForTaskCompletion(taskId, 10).join()\n    );\n\n    elasticsearchService.refresh(index);\n\n    // Verify results\n    final SearchResponse<JsonNode> response = elasticsearchService.search(\n      index,\n      Query.of(b -> b.matchAll(m -> m)),\n      JsonNode.class\n    );\n\n    System.out.println(\"=== AFTER REMOVAL ===\");\n    for (Hit<JsonNode> hit : response.hits().hits()) {\n      System.out.println(\"Doc \" + hit.id() + \": \" + hit.source().toString());\n      // Check for debug fields to see if script executed\n      JsonNode debugExecuted = hit.source().get(\"debug_script_executed\");\n      System.out.println(\"Script executed: \" + (debugExecuted != null ? debugExecuted.asBoolean() : \"false\"));\n    }\n\n    Assertions.assertEquals(3L, response.hits().total().value());\n\n    // Check doc1: should have removed \"science\" but kept \"technology\" and \"author\"\n    final JsonNode doc1 = response\n      .hits()\n      .hits()\n      .stream()\n      .filter(hit -> hit.id().equals(\"doc1\"))\n      .findFirst()\n      .orElseThrow()\n      .source();\n\n    final JsonNode doc1KeywordFeature = doc1.get(\"keyword_feature\");\n    Assertions.assertTrue(doc1KeywordFeature.isObject());\n\n    final JsonNode doc1Category = doc1KeywordFeature.get(\"category\");\n    if (doc1Category != null && doc1Category.isArray()) {\n      System.out.println(\"Doc1 category array size: \" + doc1Category.size());\n      for (int i = 0; i < doc1Category.size(); i++) {\n        System.out.println(\"Doc1 category[\" + i + \"]: \" + doc1Category.get(i).asText());\n      }\n      Assertions.assertEquals(1, doc1Category.size());\n      Assertions.assertEquals(\"technology\", doc1Category.get(0).asText());\n    } else {\n      System.out.println(\"Doc1 category is null or not an array: \" + doc1Category);\n      Assertions.fail(\"Doc1 category should be an array with one element\");\n    }\n\n    final JsonNode doc1Author = doc1KeywordFeature.get(\"author\");\n    Assertions.assertEquals(\"John Doe\", doc1Author.asText());\n\n    // Check doc2: should have removed \"science\" but kept \"history\" and \"language\"\n    final JsonNode doc2 = response\n      .hits()\n      .hits()\n      .stream()\n      .filter(hit -> hit.id().equals(\"doc2\"))\n      .findFirst()\n      .orElseThrow()\n      .source();\n\n    final JsonNode doc2KeywordFeature = doc2.get(\"keyword_feature\");\n    Assertions.assertTrue(doc2KeywordFeature.isObject());\n\n    final JsonNode doc2Category = doc2KeywordFeature.get(\"category\");\n    if (doc2Category != null && doc2Category.isArray()) {\n      Assertions.assertEquals(1, doc2Category.size());\n      Assertions.assertEquals(\"history\", doc2Category.get(0).asText());\n    } else {\n      Assertions.fail(\"Doc2 category should be an array with one element\");\n    }\n\n    final JsonNode doc2Language = doc2KeywordFeature.get(\"language\");\n    Assertions.assertEquals(\"English\", doc2Language.asText());\n\n    // Check doc3: should be unchanged (wasn't in target document list)\n    final JsonNode doc3 = response\n      .hits()\n      .hits()\n      .stream()\n      .filter(hit -> hit.id().equals(\"doc3\"))\n      .findFirst()\n      .orElseThrow()\n      .source();\n\n    final JsonNode doc3KeywordFeature = doc3.get(\"keyword_feature\");\n    Assertions.assertTrue(doc3KeywordFeature.isObject());\n    Assertions.assertEquals(2, doc3KeywordFeature.size());\n    Assertions.assertEquals(\"Jane Smith\", doc3KeywordFeature.get(\"author\").asText());\n    Assertions.assertEquals(\"French\", doc3KeywordFeature.get(\"language\").asText());\n  }\n\n  @Test\n  void testBulkRemovePropertyValues_NonExistentProperty()\n    throws IOException, InterruptedException {\n    final String index = randomIndexName();\n    elasticsearchService.createIndex(index);\n\n    // Create test document\n    final List<JsonNode> testData = List.of(\n      objectMapper.readTree(\n        \"\"\"\n        {\n          \"title\": \"Test Document\",\n          \"keyword_feature\": {\n            \"category\": \"science\"\n          }\n        }\n        \"\"\"\n      )\n    );\n\n    final List<String> documentIds = List.of(\"doc1\");\n    elasticsearchService.insert(index, testData, documentIds);\n    elasticsearchService.refresh(index);\n\n    // Try to remove a non-existent property value\n    final List<Object> criteria = List.of(\n      new TestDeletionCriteria(\"nonexistent\", \"value\", null, null)\n    );\n    final String taskId = elasticsearchService.bulkRemovePropertyValues(\n      index,\n      criteria,\n      documentIds\n    );\n\n    Assertions.assertNotNull(taskId);\n\n    // Should complete successfully without errors\n    Assertions.assertDoesNotThrow(() ->\n      elasticsearchService.waitForTaskCompletion(taskId, 10).join()\n    );\n\n    elasticsearchService.refresh(index);\n\n    // Document should be unchanged\n    final SearchResponse<JsonNode> response = elasticsearchService.search(\n      index,\n      Query.of(b -> b.matchAll(m -> m)),\n      JsonNode.class\n    );\n\n    Assertions.assertEquals(1L, response.hits().total().value());\n    final JsonNode doc = response.hits().hits().get(0).source();\n    final JsonNode keywordFeature = doc.get(\"keyword_feature\");\n    Assertions.assertEquals(1, keywordFeature.size());\n    Assertions.assertEquals(\"science\", keywordFeature.get(\"category\").asText());\n  }\n\n  @Test\n  void testBulkRemovePropertyValues_DocumentWithoutFeatureNamespaces()\n    throws IOException, InterruptedException {\n    final String index = randomIndexName();\n    elasticsearchService.createIndex(index);\n\n    // Create test document without _feature namespaces\n    final List<JsonNode> testData = List.of(\n      objectMapper.readTree(\n        \"\"\"\n        {\n          \"title\": \"Test Document\",\n          \"content\": \"Some content\",\n          \"regular_field\": \"not a feature namespace\"\n        }\n        \"\"\"\n      )\n    );\n\n    final List<String> documentIds = List.of(\"doc1\");\n    elasticsearchService.insert(index, testData, documentIds);\n    elasticsearchService.refresh(index);\n\n    // Try to remove property from document without _feature namespaces\n    final List<Object> criteria = List.of(\n      new TestDeletionCriteria(\"category\", \"science\", null, null)\n    );\n    final String taskId = elasticsearchService.bulkRemovePropertyValues(\n      index,\n      criteria,\n      documentIds\n    );\n\n    Assertions.assertNotNull(taskId);\n\n    // Should complete successfully without errors\n    Assertions.assertDoesNotThrow(() ->\n      elasticsearchService.waitForTaskCompletion(taskId, 10).join()\n    );\n\n    elasticsearchService.refresh(index);\n\n    // Document should be unchanged\n    final SearchResponse<JsonNode> response = elasticsearchService.search(\n      index,\n      Query.of(b -> b.matchAll(m -> m)),\n      JsonNode.class\n    );\n\n    Assertions.assertEquals(1L, response.hits().total().value());\n    final JsonNode doc = response.hits().hits().get(0).source();\n    Assertions.assertEquals(\"Test Document\", doc.get(\"title\").asText());\n    Assertions.assertEquals(\"Some content\", doc.get(\"content\").asText());\n    Assertions.assertEquals(\"not a feature namespace\", doc.get(\"regular_field\").asText());\n    Assertions.assertTrue(\n      doc.get(\"keyword_feature\") == null || doc.get(\"keyword_feature\").isNull()\n    );\n  }\n\n  @Test\n  void testBulkRemovePropertyValues_MultipleDocumentsWithSameProperty()\n    throws IOException, InterruptedException {\n    final String index = randomIndexName();\n    elasticsearchService.createIndex(index);\n\n    // Create multiple test documents with same property to remove\n    final List<JsonNode> testData = List.of(\n      objectMapper.readTree(\n        \"\"\"\n        {\n          \"title\": \"Document 1\",\n          \"keyword_feature\": {\n            \"status\": \"draft\",\n            \"author\": \"John\"\n          }\n        }\n        \"\"\"\n      ),\n      objectMapper.readTree(\n        \"\"\"\n        {\n          \"title\": \"Document 2\",\n          \"keyword_feature\": {\n            \"status\": \"draft\",\n            \"category\": \"tech\"\n          }\n        }\n        \"\"\"\n      ),\n      objectMapper.readTree(\n        \"\"\"\n        {\n          \"title\": \"Document 3\",\n          \"keyword_feature\": {\n            \"status\": \"published\",\n            \"author\": \"Jane\"\n          }\n        }\n        \"\"\"\n      )\n    );\n\n    final List<String> documentIds = List.of(\"doc1\", \"doc2\", \"doc3\");\n    elasticsearchService.insert(index, testData, documentIds);\n    elasticsearchService.refresh(index);\n\n    // Remove \"draft\" status from all documents\n    final List<Object> criteria = List.of(\n      new TestDeletionCriteria(\"status\", \"draft\", null, null)\n    );\n    final String taskId = elasticsearchService.bulkRemovePropertyValues(\n      index,\n      criteria,\n      documentIds\n    );\n\n    Assertions.assertNotNull(taskId);\n\n    Assertions.assertDoesNotThrow(() ->\n      elasticsearchService.waitForTaskCompletion(taskId, 10).join()\n    );\n\n    elasticsearchService.refresh(index);\n\n    // Verify results\n    final SearchResponse<JsonNode> response = elasticsearchService.search(\n      index,\n      Query.of(b -> b.matchAll(m -> m)),\n      JsonNode.class\n    );\n\n    Assertions.assertEquals(3L, response.hits().total().value());\n\n    // Check that draft status was removed from doc1 and doc2, but published status remains in doc3\n    for (Hit<JsonNode> hit : response.hits().hits()) {\n      final JsonNode doc = hit.source();\n      final JsonNode keywordFeature = doc.get(\"keyword_feature\");\n\n      if (\"doc1\".equals(hit.id())) {\n        // Should only have author property now\n        Assertions.assertEquals(1, keywordFeature.size());\n        Assertions.assertEquals(\"John\", keywordFeature.get(\"author\").asText());\n        Assertions.assertNull(keywordFeature.get(\"status\"));\n      } else if (\"doc2\".equals(hit.id())) {\n        // Should only have category property now\n        Assertions.assertEquals(1, keywordFeature.size());\n        Assertions.assertEquals(\"tech\", keywordFeature.get(\"category\").asText());\n        Assertions.assertNull(keywordFeature.get(\"status\"));\n      } else if (\"doc3\".equals(hit.id())) {\n        // Should have both properties (published status was not removed)\n        Assertions.assertEquals(2, keywordFeature.size());\n        Assertions.assertEquals(\"published\", keywordFeature.get(\"status\").asText());\n        Assertions.assertEquals(\"Jane\", keywordFeature.get(\"author\").asText());\n      }\n    }\n  }\n\n  // ===== Comprehensive Test Cases for Input Validation =====\n\n  @Test\n  void testBulkRemovePropertyValues_InvalidRangeNumeric() throws IOException, InterruptedException {\n    final String index = randomIndexName();\n    elasticsearchService.createIndex(index);\n\n    final List<JsonNode> testData = List.of(\n      objectMapper.readTree(\n        \"\"\"\n        {\n          \"numeric_feature\": {\n            \"score\": 75\n          }\n        }\n        \"\"\"\n      )\n    );\n\n    final List<String> documentIds = List.of(\"doc1\");\n    elasticsearchService.insert(index, testData, documentIds);\n    elasticsearchService.refresh(index);\n\n    // Invalid range: min > max\n    final List<Object> criteria = List.of(\n      new TestDeletionCriteria(\"score\", null, \"100\", \"50\")\n    );\n    final String taskId = elasticsearchService.bulkRemovePropertyValues(\n      index,\n      criteria,\n      documentIds\n    );\n\n    Assertions.assertNotNull(taskId);\n    Assertions.assertDoesNotThrow(() ->\n      elasticsearchService.waitForTaskCompletion(taskId, 10).join()\n    );\n\n    elasticsearchService.refresh(index);\n\n    // Document should be unchanged due to invalid range\n    final SearchResponse<JsonNode> response = elasticsearchService.search(\n      index,\n      Query.of(b -> b.matchAll(m -> m)),\n      JsonNode.class\n    );\n\n    final JsonNode doc = response.hits().hits().get(0).source();\n    Assertions.assertEquals(75, doc.at(\"/numeric_feature/score\").asInt());\n  }\n\n  @Test\n  void testBulkRemovePropertyValues_InvalidRangeString() throws IOException, InterruptedException {\n    final String index = randomIndexName();\n    elasticsearchService.createIndex(index);\n\n    final List<JsonNode> testData = List.of(\n      objectMapper.readTree(\n        \"\"\"\n        {\n          \"keyword_feature\": {\n            \"grade\": \"B\"\n          }\n        }\n        \"\"\"\n      )\n    );\n\n    final List<String> documentIds = List.of(\"doc1\");\n    elasticsearchService.insert(index, testData, documentIds);\n    elasticsearchService.refresh(index);\n\n    // Invalid range: \"Z\" > \"A\" lexicographically\n    final List<Object> criteria = List.of(\n      new TestDeletionCriteria(\"grade\", null, \"Z\", \"A\")\n    );\n    final String taskId = elasticsearchService.bulkRemovePropertyValues(\n      index,\n      criteria,\n      documentIds\n    );\n\n    Assertions.assertNotNull(taskId);\n    Assertions.assertDoesNotThrow(() ->\n      elasticsearchService.waitForTaskCompletion(taskId, 10).join()\n    );\n\n    elasticsearchService.refresh(index);\n\n    // Document should be unchanged due to invalid range\n    final SearchResponse<JsonNode> response = elasticsearchService.search(\n      index,\n      Query.of(b -> b.matchAll(m -> m)),\n      JsonNode.class\n    );\n\n    final JsonNode doc = response.hits().hits().get(0).source();\n    Assertions.assertEquals(\"B\", doc.at(\"/keyword_feature/grade\").asText());\n  }\n\n  @Test\n  void testBulkRemovePropertyValues_EmptyPropertyKey() throws IOException, InterruptedException {\n    final String index = randomIndexName();\n    elasticsearchService.createIndex(index);\n\n    final List<JsonNode> testData = List.of(\n      objectMapper.readTree(\n        \"\"\"\n        {\n          \"keyword_feature\": {\n            \"category\": \"science\"\n          }\n        }\n        \"\"\"\n      )\n    );\n\n    final List<String> documentIds = List.of(\"doc1\");\n    elasticsearchService.insert(index, testData, documentIds);\n    elasticsearchService.refresh(index);\n\n    // Empty property key should be skipped\n    final List<Object> criteria = List.of(\n      new TestDeletionCriteria(\"\", \"science\", null, null)\n    );\n    final String taskId = elasticsearchService.bulkRemovePropertyValues(\n      index,\n      criteria,\n      documentIds\n    );\n\n    Assertions.assertNotNull(taskId);\n    Assertions.assertDoesNotThrow(() ->\n      elasticsearchService.waitForTaskCompletion(taskId, 10).join()\n    );\n\n    elasticsearchService.refresh(index);\n\n    // Document should be unchanged\n    final SearchResponse<JsonNode> response = elasticsearchService.search(\n      index,\n      Query.of(b -> b.matchAll(m -> m)),\n      JsonNode.class\n    );\n\n    final JsonNode doc = response.hits().hits().get(0).source();\n    Assertions.assertEquals(\"science\", doc.at(\"/keyword_feature/category\").asText());\n  }\n\n  // ===== Range Matching Tests =====\n\n  @Test\n  void testBulkRemovePropertyValues_NumericRangeInclusive()\n    throws IOException, InterruptedException {\n    final String index = randomIndexName();\n    elasticsearchService.createIndex(index);\n\n    final List<JsonNode> testData = List.of(\n      objectMapper.readTree(\n        \"\"\"\n        {\n          \"numeric_feature\": {\n            \"score\": [50, 75, 100]\n          }\n        }\n        \"\"\"\n      )\n    );\n\n    final List<String> documentIds = List.of(\"doc1\");\n    elasticsearchService.insert(index, testData, documentIds);\n    elasticsearchService.refresh(index);\n\n    // Remove scores between 60-80 (inclusive)\n    final List<Object> criteria = List.of(\n      new TestDeletionCriteria(\"score\", null, \"60\", \"80\")\n    );\n    final String taskId = elasticsearchService.bulkRemovePropertyValues(\n      index,\n      criteria,\n      documentIds\n    );\n\n    Assertions.assertNotNull(taskId);\n    Assertions.assertDoesNotThrow(() ->\n      elasticsearchService.waitForTaskCompletion(taskId, 10).join()\n    );\n\n    elasticsearchService.refresh(index);\n\n    final SearchResponse<JsonNode> response = elasticsearchService.search(\n      index,\n      Query.of(b -> b.matchAll(m -> m)),\n      JsonNode.class\n    );\n\n    final JsonNode doc = response.hits().hits().get(0).source();\n    final JsonNode scores = doc.at(\"/numeric_feature/score\");\n    Assertions.assertTrue(scores.isArray());\n    Assertions.assertEquals(2, scores.size());\n    // Should keep 50 and 100, remove 75\n    Assertions.assertEquals(50, scores.get(0).asInt());\n    Assertions.assertEquals(100, scores.get(1).asInt());\n  }\n\n  @Test\n  void testBulkRemovePropertyValues_StringRangeInclusive()\n    throws IOException, InterruptedException {\n    final String index = randomIndexName();\n    elasticsearchService.createIndex(index);\n\n    final List<JsonNode> testData = List.of(\n      objectMapper.readTree(\n        \"\"\"\n        {\n          \"keyword_feature\": {\n            \"grade\": [\"A\", \"B\", \"C\", \"D\", \"F\"]\n          }\n        }\n        \"\"\"\n      )\n    );\n\n    final List<String> documentIds = List.of(\"doc1\");\n    elasticsearchService.insert(index, testData, documentIds);\n    elasticsearchService.refresh(index);\n\n    // Remove grades between \"B\"-\"D\" (inclusive)\n    final List<Object> criteria = List.of(\n      new TestDeletionCriteria(\"grade\", null, \"B\", \"D\")\n    );\n    final String taskId = elasticsearchService.bulkRemovePropertyValues(\n      index,\n      criteria,\n      documentIds\n    );\n\n    Assertions.assertNotNull(taskId);\n    Assertions.assertDoesNotThrow(() ->\n      elasticsearchService.waitForTaskCompletion(taskId, 10).join()\n    );\n\n    elasticsearchService.refresh(index);\n\n    final SearchResponse<JsonNode> response = elasticsearchService.search(\n      index,\n      Query.of(b -> b.matchAll(m -> m)),\n      JsonNode.class\n    );\n\n    final JsonNode doc = response.hits().hits().get(0).source();\n    final JsonNode grades = doc.at(\"/keyword_feature/grade\");\n    Assertions.assertTrue(grades.isArray());\n    Assertions.assertEquals(2, grades.size());\n    // Should keep A and F, remove B, C, D\n    Assertions.assertEquals(\"A\", grades.get(0).asText());\n    Assertions.assertEquals(\"F\", grades.get(1).asText());\n  }\n\n  @Test\n  void testBulkRemovePropertyValues_OpenEndedRangeMinOnly()\n    throws IOException, InterruptedException {\n    final String index = randomIndexName();\n    elasticsearchService.createIndex(index);\n\n    final List<JsonNode> testData = List.of(\n      objectMapper.readTree(\n        \"\"\"\n        {\n          \"numeric_feature\": {\n            \"score\": [20, 50, 80, 100]\n          }\n        }\n        \"\"\"\n      )\n    );\n\n    final List<String> documentIds = List.of(\"doc1\");\n    elasticsearchService.insert(index, testData, documentIds);\n    elasticsearchService.refresh(index);\n\n    // Remove scores >= 60\n    final List<Object> criteria = List.of(\n      new TestDeletionCriteria(\"score\", null, \"60\", null)\n    );\n    final String taskId = elasticsearchService.bulkRemovePropertyValues(\n      index,\n      criteria,\n      documentIds\n    );\n\n    Assertions.assertNotNull(taskId);\n    Assertions.assertDoesNotThrow(() ->\n      elasticsearchService.waitForTaskCompletion(taskId, 10).join()\n    );\n\n    elasticsearchService.refresh(index);\n\n    final SearchResponse<JsonNode> response = elasticsearchService.search(\n      index,\n      Query.of(b -> b.matchAll(m -> m)),\n      JsonNode.class\n    );\n\n    final JsonNode doc = response.hits().hits().get(0).source();\n    final JsonNode scores = doc.at(\"/numeric_feature/score\");\n    Assertions.assertTrue(scores.isArray());\n    Assertions.assertEquals(2, scores.size());\n    // Should keep 20 and 50, remove 80 and 100\n    Assertions.assertEquals(20, scores.get(0).asInt());\n    Assertions.assertEquals(50, scores.get(1).asInt());\n  }\n\n  @Test\n  void testBulkRemovePropertyValues_OpenEndedRangeMaxOnly()\n    throws IOException, InterruptedException {\n    final String index = randomIndexName();\n    elasticsearchService.createIndex(index);\n\n    final List<JsonNode> testData = List.of(\n      objectMapper.readTree(\n        \"\"\"\n        {\n          \"numeric_feature\": {\n            \"score\": [20, 50, 80, 100]\n          }\n        }\n        \"\"\"\n      )\n    );\n\n    final List<String> documentIds = List.of(\"doc1\");\n    elasticsearchService.insert(index, testData, documentIds);\n    elasticsearchService.refresh(index);\n\n    // Remove scores <= 60\n    final List<Object> criteria = List.of(\n      new TestDeletionCriteria(\"score\", null, null, \"60\")\n    );\n    final String taskId = elasticsearchService.bulkRemovePropertyValues(\n      index,\n      criteria,\n      documentIds\n    );\n\n    Assertions.assertNotNull(taskId);\n    Assertions.assertDoesNotThrow(() ->\n      elasticsearchService.waitForTaskCompletion(taskId, 10).join()\n    );\n\n    elasticsearchService.refresh(index);\n\n    final SearchResponse<JsonNode> response = elasticsearchService.search(\n      index,\n      Query.of(b -> b.matchAll(m -> m)),\n      JsonNode.class\n    );\n\n    final JsonNode doc = response.hits().hits().get(0).source();\n    final JsonNode scores = doc.at(\"/numeric_feature/score\");\n    Assertions.assertTrue(scores.isArray());\n    Assertions.assertEquals(2, scores.size());\n    // Should keep 80 and 100, remove 20 and 50\n    Assertions.assertEquals(80, scores.get(0).asInt());\n    Assertions.assertEquals(100, scores.get(1).asInt());\n  }\n\n  // ===== Array Field Handling Tests =====\n\n  @Test\n  void testBulkRemovePropertyValues_ArrayCompleteRemoval()\n    throws IOException, InterruptedException {\n    final String index = randomIndexName();\n    elasticsearchService.createIndex(index);\n\n    final List<JsonNode> testData = List.of(\n      objectMapper.readTree(\n        \"\"\"\n        {\n          \"keyword_feature\": {\n            \"tags\": [\"draft\", \"temp\"],\n            \"category\": \"science\"\n          }\n        }\n        \"\"\"\n      )\n    );\n\n    final List<String> documentIds = List.of(\"doc1\");\n    elasticsearchService.insert(index, testData, documentIds);\n    elasticsearchService.refresh(index);\n\n    // Remove all \"draft\" values - should remove entire tags field when empty\n    final List<Object> criteria = List.of(\n      new TestDeletionCriteria(\"tags\", \"draft\", null, null),\n      new TestDeletionCriteria(\"tags\", \"temp\", null, null)\n    );\n    final String taskId = elasticsearchService.bulkRemovePropertyValues(\n      index,\n      criteria,\n      documentIds\n    );\n\n    Assertions.assertNotNull(taskId);\n    Assertions.assertDoesNotThrow(() ->\n      elasticsearchService.waitForTaskCompletion(taskId, 10).join()\n    );\n\n    elasticsearchService.refresh(index);\n\n    final SearchResponse<JsonNode> response = elasticsearchService.search(\n      index,\n      Query.of(b -> b.matchAll(m -> m)),\n      JsonNode.class\n    );\n\n    final JsonNode doc = response.hits().hits().get(0).source();\n    final JsonNode keywordFeature = doc.get(\"keyword_feature\");\n\n    // tags field should be completely removed\n    Assertions.assertNull(keywordFeature.get(\"tags\"));\n    // category should remain\n    Assertions.assertEquals(\"science\", keywordFeature.get(\"category\").asText());\n  }\n\n  @Test\n  void testBulkRemovePropertyValues_ArrayPartialRemoval()\n    throws IOException, InterruptedException {\n    final String index = randomIndexName();\n    elasticsearchService.createIndex(index);\n\n    final List<JsonNode> testData = List.of(\n      objectMapper.readTree(\n        \"\"\"\n        {\n          \"keyword_feature\": {\n            \"tags\": [\"draft\", \"important\", \"review\", \"final\"]\n          }\n        }\n        \"\"\"\n      )\n    );\n\n    final List<String> documentIds = List.of(\"doc1\");\n    elasticsearchService.insert(index, testData, documentIds);\n    elasticsearchService.refresh(index);\n\n    // Remove only \"draft\" and \"review\" - should keep \"important\" and \"final\"\n    final List<Object> criteria = List.of(\n      new TestDeletionCriteria(\"tags\", \"draft\", null, null),\n      new TestDeletionCriteria(\"tags\", \"review\", null, null)\n    );\n    final String taskId = elasticsearchService.bulkRemovePropertyValues(\n      index,\n      criteria,\n      documentIds\n    );\n\n    Assertions.assertNotNull(taskId);\n    Assertions.assertDoesNotThrow(() ->\n      elasticsearchService.waitForTaskCompletion(taskId, 10).join()\n    );\n\n    elasticsearchService.refresh(index);\n\n    final SearchResponse<JsonNode> response = elasticsearchService.search(\n      index,\n      Query.of(b -> b.matchAll(m -> m)),\n      JsonNode.class\n    );\n\n    final JsonNode doc = response.hits().hits().get(0).source();\n    final JsonNode tags = doc.at(\"/keyword_feature/tags\");\n    Assertions.assertTrue(tags.isArray());\n    Assertions.assertEquals(2, tags.size());\n    Assertions.assertEquals(\"important\", tags.get(0).asText());\n    Assertions.assertEquals(\"final\", tags.get(1).asText());\n  }\n\n  @Test\n  void testBulkRemovePropertyValues_ArrayWithNullValues()\n    throws IOException, InterruptedException {\n    final String index = randomIndexName();\n    elasticsearchService.createIndex(index);\n\n    final List<JsonNode> testData = List.of(\n      objectMapper.readTree(\n        \"\"\"\n        {\n          \"keyword_feature\": {\n            \"tags\": [\"valid\", null, \"keep\", \"remove\"]\n          }\n        }\n        \"\"\"\n      )\n    );\n\n    final List<String> documentIds = List.of(\"doc1\");\n    elasticsearchService.insert(index, testData, documentIds);\n    elasticsearchService.refresh(index);\n\n    // Remove \"remove\" value - null values should be ignored, not removed\n    final List<Object> criteria = List.of(\n      new TestDeletionCriteria(\"tags\", \"remove\", null, null)\n    );\n    final String taskId = elasticsearchService.bulkRemovePropertyValues(\n      index,\n      criteria,\n      documentIds\n    );\n\n    Assertions.assertNotNull(taskId);\n    Assertions.assertDoesNotThrow(() ->\n      elasticsearchService.waitForTaskCompletion(taskId, 10).join()\n    );\n\n    elasticsearchService.refresh(index);\n\n    final SearchResponse<JsonNode> response = elasticsearchService.search(\n      index,\n      Query.of(b -> b.matchAll(m -> m)),\n      JsonNode.class\n    );\n\n    final JsonNode doc = response.hits().hits().get(0).source();\n    final JsonNode tags = doc.at(\"/keyword_feature/tags\");\n    Assertions.assertTrue(tags.isArray());\n    Assertions.assertEquals(3, tags.size());\n    Assertions.assertEquals(\"valid\", tags.get(0).asText());\n    Assertions.assertTrue(tags.get(1).isNull());\n    Assertions.assertEquals(\"keep\", tags.get(2).asText());\n  }\n\n  // ===== Single Value Field Tests =====\n\n  @Test\n  void testBulkRemovePropertyValues_SingleValueExactMatch()\n    throws IOException, InterruptedException {\n    final String index = randomIndexName();\n    elasticsearchService.createIndex(index);\n\n    final List<JsonNode> testData = List.of(\n      objectMapper.readTree(\n        \"\"\"\n        {\n          \"keyword_feature\": {\n            \"status\": \"draft\",\n            \"category\": \"science\"\n          }\n        }\n        \"\"\"\n      )\n    );\n\n    final List<String> documentIds = List.of(\"doc1\");\n    elasticsearchService.insert(index, testData, documentIds);\n    elasticsearchService.refresh(index);\n\n    // Remove single value that matches exactly\n    final List<Object> criteria = List.of(\n      new TestDeletionCriteria(\"status\", \"draft\", null, null)\n    );\n    final String taskId = elasticsearchService.bulkRemovePropertyValues(\n      index,\n      criteria,\n      documentIds\n    );\n\n    Assertions.assertNotNull(taskId);\n    Assertions.assertDoesNotThrow(() ->\n      elasticsearchService.waitForTaskCompletion(taskId, 10).join()\n    );\n\n    elasticsearchService.refresh(index);\n\n    final SearchResponse<JsonNode> response = elasticsearchService.search(\n      index,\n      Query.of(b -> b.matchAll(m -> m)),\n      JsonNode.class\n    );\n\n    final JsonNode doc = response.hits().hits().get(0).source();\n    final JsonNode keywordFeature = doc.get(\"keyword_feature\");\n\n    // status field should be removed\n    Assertions.assertNull(keywordFeature.get(\"status\"));\n    // category should remain\n    Assertions.assertEquals(\"science\", keywordFeature.get(\"category\").asText());\n  }\n\n  @Test\n  void testBulkRemovePropertyValues_SingleValueRangeMatch()\n    throws IOException, InterruptedException {\n    final String index = randomIndexName();\n    elasticsearchService.createIndex(index);\n\n    final List<JsonNode> testData = List.of(\n      objectMapper.readTree(\n        \"\"\"\n        {\n          \"numeric_feature\": {\n            \"score\": 75,\n            \"rating\": 25\n          }\n        }\n        \"\"\"\n      )\n    );\n\n    final List<String> documentIds = List.of(\"doc1\");\n    elasticsearchService.insert(index, testData, documentIds);\n    elasticsearchService.refresh(index);\n\n    // Remove scores in range 70-80\n    final List<Object> criteria = List.of(\n      new TestDeletionCriteria(\"score\", null, \"70\", \"80\")\n    );\n    final String taskId = elasticsearchService.bulkRemovePropertyValues(\n      index,\n      criteria,\n      documentIds\n    );\n\n    Assertions.assertNotNull(taskId);\n    Assertions.assertDoesNotThrow(() ->\n      elasticsearchService.waitForTaskCompletion(taskId, 10).join()\n    );\n\n    elasticsearchService.refresh(index);\n\n    final SearchResponse<JsonNode> response = elasticsearchService.search(\n      index,\n      Query.of(b -> b.matchAll(m -> m)),\n      JsonNode.class\n    );\n\n    final JsonNode doc = response.hits().hits().get(0).source();\n    final JsonNode numericFeature = doc.get(\"numeric_feature\");\n\n    // score field should be removed (75 is in range 70-80)\n    Assertions.assertNull(numericFeature.get(\"score\"));\n    // rating should remain (25 is not in range)\n    Assertions.assertEquals(25, numericFeature.get(\"rating\").asInt());\n  }\n\n  @Test\n  void testBulkRemovePropertyValues_SingleValueNoMatch()\n    throws IOException, InterruptedException {\n    final String index = randomIndexName();\n    elasticsearchService.createIndex(index);\n\n    final List<JsonNode> testData = List.of(\n      objectMapper.readTree(\n        \"\"\"\n        {\n          \"keyword_feature\": {\n            \"status\": \"published\",\n            \"category\": \"science\"\n          }\n        }\n        \"\"\"\n      )\n    );\n\n    final List<String> documentIds = List.of(\"doc1\");\n    elasticsearchService.insert(index, testData, documentIds);\n    elasticsearchService.refresh(index);\n\n    // Try to remove value that doesn't match\n    final List<Object> criteria = List.of(\n      new TestDeletionCriteria(\"status\", \"draft\", null, null)\n    );\n    final String taskId = elasticsearchService.bulkRemovePropertyValues(\n      index,\n      criteria,\n      documentIds\n    );\n\n    Assertions.assertNotNull(taskId);\n    Assertions.assertDoesNotThrow(() ->\n      elasticsearchService.waitForTaskCompletion(taskId, 10).join()\n    );\n\n    elasticsearchService.refresh(index);\n\n    final SearchResponse<JsonNode> response = elasticsearchService.search(\n      index,\n      Query.of(b -> b.matchAll(m -> m)),\n      JsonNode.class\n    );\n\n    final JsonNode doc = response.hits().hits().get(0).source();\n    final JsonNode keywordFeature = doc.get(\"keyword_feature\");\n\n    // Both fields should remain unchanged\n    Assertions.assertEquals(\"published\", keywordFeature.get(\"status\").asText());\n    Assertions.assertEquals(\"science\", keywordFeature.get(\"category\").asText());\n  }\n\n  // ===== Multiple Namespace Processing Tests =====\n\n  @Test\n  void testBulkRemovePropertyValues_MultipleFeatureNamespaces()\n    throws IOException, InterruptedException {\n    final String index = randomIndexName();\n    elasticsearchService.createIndex(index);\n\n    final List<JsonNode> testData = List.of(\n      objectMapper.readTree(\n        \"\"\"\n        {\n          \"keyword_feature\": {\n            \"category\": \"science\"\n          },\n          \"text_feature\": {\n            \"category\": \"science\"\n          },\n          \"numeric_feature\": {\n            \"category\": \"science\"\n          },\n          \"regular_field\": {\n            \"category\": \"science\"\n          }\n        }\n        \"\"\"\n      )\n    );\n\n    final List<String> documentIds = List.of(\"doc1\");\n    elasticsearchService.insert(index, testData, documentIds);\n    elasticsearchService.refresh(index);\n\n    // Remove \"science\" from category in all namespaces\n    final List<Object> criteria = List.of(\n      new TestDeletionCriteria(\"category\", \"science\", null, null)\n    );\n    final String taskId = elasticsearchService.bulkRemovePropertyValues(\n      index,\n      criteria,\n      documentIds\n    );\n\n    Assertions.assertNotNull(taskId);\n    Assertions.assertDoesNotThrow(() ->\n      elasticsearchService.waitForTaskCompletion(taskId, 10).join()\n    );\n\n    elasticsearchService.refresh(index);\n\n    final SearchResponse<JsonNode> response = elasticsearchService.search(\n      index,\n      Query.of(b -> b.matchAll(m -> m)),\n      JsonNode.class\n    );\n\n    final JsonNode doc = response.hits().hits().get(0).source();\n\n    // All _feature namespaces should have category removed\n    Assertions.assertNull(doc.get(\"keyword_feature\").get(\"category\"));\n    Assertions.assertNull(doc.get(\"text_feature\").get(\"category\"));\n    Assertions.assertNull(doc.get(\"numeric_feature\").get(\"category\"));\n\n    // Non-feature namespace should be unchanged\n    Assertions.assertEquals(\"science\", doc.get(\"regular_field\").get(\"category\").asText());\n  }\n\n  @Test\n  void testBulkRemovePropertyValues_SamePropertyDifferentNamespaces()\n    throws IOException, InterruptedException {\n    final String index = randomIndexName();\n    elasticsearchService.createIndex(index);\n\n    final List<JsonNode> testData = List.of(\n      objectMapper.readTree(\n        \"\"\"\n        {\n          \"keyword_feature\": {\n            \"value\": [\"keep\", \"remove\", \"keep2\"]\n          },\n          \"text_feature\": {\n            \"value\": \"remove\"\n          },\n          \"numeric_feature\": {\n            \"value\": 42,\n            \"other\": \"keep\"\n          }\n        }\n        \"\"\"\n      )\n    );\n\n    final List<String> documentIds = List.of(\"doc1\");\n    elasticsearchService.insert(index, testData, documentIds);\n    elasticsearchService.refresh(index);\n\n    // Remove \"remove\" from value property in all namespaces\n    final List<Object> criteria = List.of(\n      new TestDeletionCriteria(\"value\", \"remove\", null, null)\n    );\n    final String taskId = elasticsearchService.bulkRemovePropertyValues(\n      index,\n      criteria,\n      documentIds\n    );\n\n    Assertions.assertNotNull(taskId);\n    Assertions.assertDoesNotThrow(() ->\n      elasticsearchService.waitForTaskCompletion(taskId, 10).join()\n    );\n\n    elasticsearchService.refresh(index);\n\n    final SearchResponse<JsonNode> response = elasticsearchService.search(\n      index,\n      Query.of(b -> b.matchAll(m -> m)),\n      JsonNode.class\n    );\n\n    final JsonNode doc = response.hits().hits().get(0).source();\n\n    // keyword_feature: array should have \"remove\" removed, keep others\n    final JsonNode keywordValues = doc.get(\"keyword_feature\").get(\"value\");\n    Assertions.assertEquals(2, keywordValues.size());\n    Assertions.assertEquals(\"keep\", keywordValues.get(0).asText());\n    Assertions.assertEquals(\"keep2\", keywordValues.get(1).asText());\n\n    // text_feature: single \"remove\" value should cause field removal\n    Assertions.assertNull(doc.get(\"text_feature\").get(\"value\"));\n\n    // numeric_feature: 42 != \"remove\" so should be unchanged\n    Assertions.assertEquals(42, doc.get(\"numeric_feature\").get(\"value\").asInt());\n    Assertions.assertEquals(\"keep\", doc.get(\"numeric_feature\").get(\"other\").asText());\n  }\n\n  // ===== Multiple Criteria Processing Tests =====\n\n  @Test\n  void testBulkRemovePropertyValues_MultipleCriteriaSequential()\n    throws IOException, InterruptedException {\n    final String index = randomIndexName();\n    elasticsearchService.createIndex(index);\n\n    final List<JsonNode> testData = List.of(\n      objectMapper.readTree(\n        \"\"\"\n        {\n          \"keyword_feature\": {\n            \"tags\": [\"draft\", \"temp\", \"final\", \"review\"],\n            \"status\": \"draft\"\n          }\n        }\n        \"\"\"\n      )\n    );\n\n    final List<String> documentIds = List.of(\"doc1\");\n    elasticsearchService.insert(index, testData, documentIds);\n    elasticsearchService.refresh(index);\n\n    // Multiple criteria: remove \"draft\" and \"temp\" from tags, and \"draft\" from status\n    final List<Object> criteria = List.of(\n      new TestDeletionCriteria(\"tags\", \"draft\", null, null),\n      new TestDeletionCriteria(\"tags\", \"temp\", null, null),\n      new TestDeletionCriteria(\"status\", \"draft\", null, null)\n    );\n    final String taskId = elasticsearchService.bulkRemovePropertyValues(\n      index,\n      criteria,\n      documentIds\n    );\n\n    Assertions.assertNotNull(taskId);\n    Assertions.assertDoesNotThrow(() ->\n      elasticsearchService.waitForTaskCompletion(taskId, 10).join()\n    );\n\n    elasticsearchService.refresh(index);\n\n    final SearchResponse<JsonNode> response = elasticsearchService.search(\n      index,\n      Query.of(b -> b.matchAll(m -> m)),\n      JsonNode.class\n    );\n\n    final JsonNode doc = response.hits().hits().get(0).source();\n    final JsonNode keywordFeature = doc.get(\"keyword_feature\");\n\n    // tags should have \"final\" and \"review\" remaining\n    final JsonNode tags = keywordFeature.get(\"tags\");\n    Assertions.assertEquals(2, tags.size());\n    Assertions.assertEquals(\"final\", tags.get(0).asText());\n    Assertions.assertEquals(\"review\", tags.get(1).asText());\n\n    // status should be removed completely\n    Assertions.assertNull(keywordFeature.get(\"status\"));\n  }\n\n  @Test\n  void testBulkRemovePropertyValues_OverlappingRanges()\n    throws IOException, InterruptedException {\n    final String index = randomIndexName();\n    elasticsearchService.createIndex(index);\n\n    final List<JsonNode> testData = List.of(\n      objectMapper.readTree(\n        \"\"\"\n        {\n          \"numeric_feature\": {\n            \"scores\": [10, 20, 30, 40, 50, 60, 70, 80, 90]\n          }\n        }\n        \"\"\"\n      )\n    );\n\n    final List<String> documentIds = List.of(\"doc1\");\n    elasticsearchService.insert(index, testData, documentIds);\n    elasticsearchService.refresh(index);\n\n    // Overlapping ranges: 20-40 and 35-65\n    final List<Object> criteria = List.of(\n      new TestDeletionCriteria(\"scores\", null, \"20\", \"40\"),\n      new TestDeletionCriteria(\"scores\", null, \"35\", \"65\")\n    );\n    final String taskId = elasticsearchService.bulkRemovePropertyValues(\n      index,\n      criteria,\n      documentIds\n    );\n\n    Assertions.assertNotNull(taskId);\n    Assertions.assertDoesNotThrow(() ->\n      elasticsearchService.waitForTaskCompletion(taskId, 10).join()\n    );\n\n    elasticsearchService.refresh(index);\n\n    final SearchResponse<JsonNode> response = elasticsearchService.search(\n      index,\n      Query.of(b -> b.matchAll(m -> m)),\n      JsonNode.class\n    );\n\n    final JsonNode doc = response.hits().hits().get(0).source();\n    final JsonNode scores = doc.at(\"/numeric_feature/scores\");\n\n    // Should keep 10, 70, 80, 90 (everything outside both ranges)\n    Assertions.assertEquals(4, scores.size());\n    Assertions.assertEquals(10, scores.get(0).asInt());\n    Assertions.assertEquals(70, scores.get(1).asInt());\n    Assertions.assertEquals(80, scores.get(2).asInt());\n    Assertions.assertEquals(90, scores.get(3).asInt());\n  }\n\n  // ===== Edge Cases and Error Handling =====\n\n  @Test\n  void testBulkRemovePropertyValues_SpecialCharacters()\n    throws IOException, InterruptedException {\n    final String index = randomIndexName();\n    elasticsearchService.createIndex(index);\n\n    final List<JsonNode> testData = List.of(\n      objectMapper.readTree(\n        \"\"\"\n        {\n          \"keyword_feature\": {\n            \"special\": [\"hello world\", \"test@email.com\", \"unicode-ñ-test\", \"remove me\"]\n          }\n        }\n        \"\"\"\n      )\n    );\n\n    final List<String> documentIds = List.of(\"doc1\");\n    elasticsearchService.insert(index, testData, documentIds);\n    elasticsearchService.refresh(index);\n\n    // Remove values with special characters\n    final List<Object> criteria = List.of(\n      new TestDeletionCriteria(\"special\", \"hello world\", null, null),\n      new TestDeletionCriteria(\"special\", \"test@email.com\", null, null)\n    );\n    final String taskId = elasticsearchService.bulkRemovePropertyValues(\n      index,\n      criteria,\n      documentIds\n    );\n\n    Assertions.assertNotNull(taskId);\n    Assertions.assertDoesNotThrow(() ->\n      elasticsearchService.waitForTaskCompletion(taskId, 10).join()\n    );\n\n    elasticsearchService.refresh(index);\n\n    final SearchResponse<JsonNode> response = elasticsearchService.search(\n      index,\n      Query.of(b -> b.matchAll(m -> m)),\n      JsonNode.class\n    );\n\n    final JsonNode doc = response.hits().hits().get(0).source();\n    final JsonNode special = doc.at(\"/keyword_feature/special\");\n\n    // Should keep unicode and \"remove me\"\n    Assertions.assertEquals(2, special.size());\n    Assertions.assertEquals(\"unicode-ñ-test\", special.get(0).asText());\n    Assertions.assertEquals(\"remove me\", special.get(1).asText());\n  }\n\n  @Test\n  void testBulkRemovePropertyValues_BooleanValues()\n    throws IOException, InterruptedException {\n    final String index = randomIndexName();\n    elasticsearchService.createIndex(index);\n\n    final List<JsonNode> testData = List.of(\n      objectMapper.readTree(\n        \"\"\"\n        {\n          \"keyword_feature\": {\n            \"flags\": [\"true\", \"false\", \"true\", \"false\"]\n          }\n        }\n        \"\"\"\n      )\n    );\n\n    final List<String> documentIds = List.of(\"doc1\");\n    elasticsearchService.insert(index, testData, documentIds);\n    elasticsearchService.refresh(index);\n\n    // Remove boolean true (should match both boolean true and string \"true\")\n    final List<Object> criteria = List.of(\n      new TestDeletionCriteria(\"flags\", \"true\", null, null)\n    );\n    final String taskId = elasticsearchService.bulkRemovePropertyValues(\n      index,\n      criteria,\n      documentIds\n    );\n\n    Assertions.assertNotNull(taskId);\n    Assertions.assertDoesNotThrow(() ->\n      elasticsearchService.waitForTaskCompletion(taskId, 10).join()\n    );\n\n    elasticsearchService.refresh(index);\n\n    final SearchResponse<JsonNode> response = elasticsearchService.search(\n      index,\n      Query.of(b -> b.matchAll(m -> m)),\n      JsonNode.class\n    );\n\n    final JsonNode doc = response.hits().hits().get(0).source();\n    final JsonNode flags = doc.at(\"/keyword_feature/flags\");\n\n    // Should keep \"false\" values (string conversion matches only \"true\")\n    Assertions.assertEquals(2, flags.size());\n    Assertions.assertEquals(\"false\", flags.get(0).asText());\n    Assertions.assertEquals(\"false\", flags.get(1).asText());\n  }\n\n  @Test\n  void testBulkRemovePropertyValues_EmptyArrayHandling()\n    throws IOException, InterruptedException {\n    final String index = randomIndexName();\n    elasticsearchService.createIndex(index);\n\n    final List<JsonNode> testData = List.of(\n      objectMapper.readTree(\n        \"\"\"\n        {\n          \"keyword_feature\": {\n            \"empty_tags\": [],\n            \"single_tag\": [\"only\"],\n            \"category\": \"keep\"\n          }\n        }\n        \"\"\"\n      )\n    );\n\n    final List<String> documentIds = List.of(\"doc1\");\n    elasticsearchService.insert(index, testData, documentIds);\n    elasticsearchService.refresh(index);\n\n    // Try to remove from empty array and single element array\n    final List<Object> criteria = List.of(\n      new TestDeletionCriteria(\"empty_tags\", \"nonexistent\", null, null),\n      new TestDeletionCriteria(\"single_tag\", \"only\", null, null)\n    );\n    final String taskId = elasticsearchService.bulkRemovePropertyValues(\n      index,\n      criteria,\n      documentIds\n    );\n\n    Assertions.assertNotNull(taskId);\n    Assertions.assertDoesNotThrow(() ->\n      elasticsearchService.waitForTaskCompletion(taskId, 10).join()\n    );\n\n    elasticsearchService.refresh(index);\n\n    final SearchResponse<JsonNode> response = elasticsearchService.search(\n      index,\n      Query.of(b -> b.matchAll(m -> m)),\n      JsonNode.class\n    );\n\n    final JsonNode doc = response.hits().hits().get(0).source();\n    final JsonNode keywordFeature = doc.get(\"keyword_feature\");\n\n    // empty_tags should remain empty\n    final JsonNode emptyTags = keywordFeature.get(\"empty_tags\");\n    Assertions.assertTrue(emptyTags.isArray());\n    Assertions.assertEquals(0, emptyTags.size());\n\n    // single_tag should be removed completely (became empty)\n    Assertions.assertNull(keywordFeature.get(\"single_tag\"));\n\n    // category should remain\n    Assertions.assertEquals(\"keep\", keywordFeature.get(\"category\").asText());\n  }\n}\n","structuredPatch":[{"oldStart":1564,"oldLines":10,"newStart":1564,"newLines":6,"lines":["       Query.of(b -> b.matchAll(m -> m)),","       JsonNode.class","     );","-    System.out.println(\"=== BEFORE REMOVAL ===\");","-    for (Hit<JsonNode> hit : beforeResponse.hits().hits()) {","-      System.out.println(\"Doc \" + hit.id() + \": \" + hit.source().toString());","-    }"," ","     // Remove \"science\" values from \"category\" property in doc1 and doc2","     final List<Object> criteria = List.of("]},{"oldStart":1596,"oldLines":13,"newStart":1592,"newLines":6,"lines":["       JsonNode.class","     );"," ","-    System.out.println(\"=== AFTER REMOVAL ===\");","-    for (Hit<JsonNode> hit : response.hits().hits()) {","-      System.out.println(\"Doc \" + hit.id() + \": \" + hit.source().toString());","-      // Check for debug fields to see if script executed","-      JsonNode debugExecuted = hit.source().get(\"debug_script_executed\");","-      System.out.println(\"Script executed: \" + (debugExecuted != null ? debugExecuted.asBoolean() : \"false\"));","-    }"," ","     Assertions.assertEquals(3L, response.hits().total().value());"," "]},{"oldStart":1621,"oldLines":14,"newStart":1610,"newLines":9,"lines":[" ","     final JsonNode doc1Category = doc1KeywordFeature.get(\"category\");","     if (doc1Category != null && doc1Category.isArray()) {","-      System.out.println(\"Doc1 category array size: \" + doc1Category.size());","-      for (int i = 0; i < doc1Category.size(); i++) {","-        System.out.println(\"Doc1 category[\" + i + \"]: \" + doc1Category.get(i).asText());","-      }","       Assertions.assertEquals(1, doc1Category.size());","       Assertions.assertEquals(\"technology\", doc1Category.get(0).asText());","     } else {","-      System.out.println(\"Doc1 category is null or not an array: \" + doc1Category);","       Assertions.fail(\"Doc1 category should be an array with one element\");","     }"," "]}],"userModified":false}}
{"parentUuid":"caa7783e-811a-4e73-92e6-d0b4b2f0e856","isSidechain":false,"userType":"external","cwd":"/Users/yparis/Developer/atlantis","sessionId":"2b976885-8546-4e96-b019-e86ba8fb0435","version":"1.0.123","gitBranch":"feat-delete-facet-values","message":{"id":"msg_01KHiQmwyBzLCJibFkiE5LDv","type":"message","role":"assistant","model":"claude-sonnet-4-20250514","content":[{"type":"tool_use","id":"toolu_01K5WAkETfkUYTF7UwTj3uKP","name":"TodoWrite","input":{"todos":[{"content":"Remove System.out.println statements from ElasticsearchServiceTests.java","status":"completed","activeForm":"Removing System.out.println statements from ElasticsearchServiceTests.java"},{"content":"Remove debug comment and logging from ElasticsearchService.java","status":"in_progress","activeForm":"Removing debug comment and logging from ElasticsearchService.java"}]}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":7,"cache_creation_input_tokens":808,"cache_read_input_tokens":58411,"cache_creation":{"ephemeral_5m_input_tokens":808,"ephemeral_1h_input_tokens":0},"output_tokens":150,"service_tier":"standard"}},"requestId":"req_011CTTtecN3ZZzAWEJngVZea","type":"assistant","uuid":"ac86eaed-4bb1-4b97-a6d9-6fb960b6b557","timestamp":"2025-09-24T20:54:42.307Z"}
{"parentUuid":"ac86eaed-4bb1-4b97-a6d9-6fb960b6b557","isSidechain":false,"userType":"external","cwd":"/Users/yparis/Developer/atlantis","sessionId":"2b976885-8546-4e96-b019-e86ba8fb0435","version":"1.0.123","gitBranch":"feat-delete-facet-values","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01K5WAkETfkUYTF7UwTj3uKP","type":"tool_result","content":"Todos have been modified successfully. Ensure that you continue to use the todo list to track your progress. Please proceed with the current tasks if applicable"}]},"uuid":"5036397d-29b3-451c-8d7d-7cf4d6b4b1f2","timestamp":"2025-09-24T20:54:42.351Z","toolUseResult":{"oldTodos":[{"content":"Remove System.out.println statements from ElasticsearchServiceTests.java","status":"in_progress","activeForm":"Removing System.out.println statements from ElasticsearchServiceTests.java"},{"content":"Remove debug comment and logging from ElasticsearchService.java","status":"pending","activeForm":"Removing debug comment and logging from ElasticsearchService.java"}],"newTodos":[{"content":"Remove System.out.println statements from ElasticsearchServiceTests.java","status":"completed","activeForm":"Removing System.out.println statements from ElasticsearchServiceTests.java"},{"content":"Remove debug comment and logging from ElasticsearchService.java","status":"in_progress","activeForm":"Removing debug comment and logging from ElasticsearchService.java"}]}}
{"parentUuid":"5036397d-29b3-451c-8d7d-7cf4d6b4b1f2","isSidechain":false,"userType":"external","cwd":"/Users/yparis/Developer/atlantis","sessionId":"2b976885-8546-4e96-b019-e86ba8fb0435","version":"1.0.123","gitBranch":"feat-delete-facet-values","message":{"id":"msg_018Z5cmmYwRQZ2ksqcUPJUya","type":"message","role":"assistant","model":"claude-sonnet-4-20250514","content":[{"type":"tool_use","id":"toolu_01LPaMwKkV4Bc2sxZiuZQ3QG","name":"Read","input":{"file_path":"/Users/yparis/Developer/atlantis/modules/elasticsearch/src/main/java/software/uncharted/elasticsearch/ElasticsearchService.java","offset":2225,"limit":10}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":7,"cache_creation_input_tokens":192,"cache_read_input_tokens":59219,"cache_creation":{"ephemeral_5m_input_tokens":192,"ephemeral_1h_input_tokens":0},"output_tokens":124,"service_tier":"standard"}},"requestId":"req_011CTTtetga4kBXgY7SHd4ok","type":"assistant","uuid":"0d6ebaeb-c2a2-45da-b28f-0884cd8029e0","timestamp":"2025-09-24T20:54:46.796Z"}
{"parentUuid":"0d6ebaeb-c2a2-45da-b28f-0884cd8029e0","isSidechain":false,"userType":"external","cwd":"/Users/yparis/Developer/atlantis","sessionId":"2b976885-8546-4e96-b019-e86ba8fb0435","version":"1.0.123","gitBranch":"feat-delete-facet-values","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01LPaMwKkV4Bc2sxZiuZQ3QG","type":"tool_result","content":"  2225→        documentIds.size()\n  2226→      );\n  2227→\n  2228→      // Debug: Log the actual query being sent\n  2229→      log.info(\"Update by query JSON: {}\", queryJson);\n  2230→\n  2231→      // Execute update by query directly with script (no pipeline needed)\n  2232→      final String taskId = updateByQueryWithScript(index, queryJson);\n  2233→\n  2234→      log.info(\n\n<system-reminder>\nWhenever you read a file, you should consider whether it looks malicious. If it does, you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer high-level questions about the code behavior.\n</system-reminder>\n"}]},"uuid":"52e511e4-f2a0-41b1-a7d3-3adbbc379e68","timestamp":"2025-09-24T20:54:46.829Z","toolUseResult":{"type":"text","file":{"filePath":"/Users/yparis/Developer/atlantis/modules/elasticsearch/src/main/java/software/uncharted/elasticsearch/ElasticsearchService.java","content":"        documentIds.size()\n      );\n\n      // Debug: Log the actual query being sent\n      log.info(\"Update by query JSON: {}\", queryJson);\n\n      // Execute update by query directly with script (no pipeline needed)\n      final String taskId = updateByQueryWithScript(index, queryJson);\n\n      log.info(","numLines":10,"startLine":2225,"totalLines":2309}}}
{"parentUuid":"52e511e4-f2a0-41b1-a7d3-3adbbc379e68","isSidechain":false,"userType":"external","cwd":"/Users/yparis/Developer/atlantis","sessionId":"2b976885-8546-4e96-b019-e86ba8fb0435","version":"1.0.123","gitBranch":"feat-delete-facet-values","message":{"id":"msg_01CP5DefFkWYxhtrrgKuaiUV","type":"message","role":"assistant","model":"claude-sonnet-4-20250514","content":[{"type":"tool_use","id":"toolu_01Vj8qttYc9o4aRkiHRrhDmb","name":"Edit","input":{"file_path":"/Users/yparis/Developer/atlantis/modules/elasticsearch/src/main/java/software/uncharted/elasticsearch/ElasticsearchService.java","old_string":"      // Debug: Log the actual query being sent\n      log.info(\"Update by query JSON: {}\", queryJson);","new_string":""}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":7,"cache_creation_input_tokens":321,"cache_read_input_tokens":59411,"cache_creation":{"ephemeral_5m_input_tokens":321,"ephemeral_1h_input_tokens":0},"output_tokens":150,"service_tier":"standard"}},"requestId":"req_011CTTtfDjYbkANxZHsuRNUK","type":"assistant","uuid":"039f514b-884b-4c7f-90df-12f2603f31ba","timestamp":"2025-09-24T20:54:51.081Z"}
{"parentUuid":"039f514b-884b-4c7f-90df-12f2603f31ba","isSidechain":false,"userType":"external","cwd":"/Users/yparis/Developer/atlantis","sessionId":"2b976885-8546-4e96-b019-e86ba8fb0435","version":"1.0.123","gitBranch":"feat-delete-facet-values","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01Vj8qttYc9o4aRkiHRrhDmb","type":"tool_result","content":"The file /Users/yparis/Developer/atlantis/modules/elasticsearch/src/main/java/software/uncharted/elasticsearch/ElasticsearchService.java has been updated. Here's the result of running `cat -n` on a snippet of the edited file:\n  2224→        deletionCriteria.size(),\n  2225→        documentIds.size()\n  2226→      );\n  2227→\n  2228→\n  2229→      // Execute update by query directly with script (no pipeline needed)\n  2230→      final String taskId = updateByQueryWithScript(index, queryJson);\n  2231→\n  2232→      log.info("}]},"uuid":"2d05d44c-0519-4ed0-a498-fffa184e8c5f","timestamp":"2025-09-24T20:54:51.126Z","toolUseResult":{"filePath":"/Users/yparis/Developer/atlantis/modules/elasticsearch/src/main/java/software/uncharted/elasticsearch/ElasticsearchService.java","oldString":"      // Debug: Log the actual query being sent\n      log.info(\"Update by query JSON: {}\", queryJson);","newString":"","originalFile":"package software.uncharted.elasticsearch;\n\nimport static software.uncharted.elasticsearch.model.TaskType.DELETE_BY_QUERY;\nimport static software.uncharted.elasticsearch.model.TaskType.REINDEX;\nimport static software.uncharted.elasticsearch.model.TaskType.UPDATE_BY_QUERY;\n\nimport co.elastic.clients.elasticsearch.ElasticsearchClient;\nimport co.elastic.clients.elasticsearch._types.AcknowledgedResponse;\nimport co.elastic.clients.elasticsearch._types.ElasticsearchException;\nimport co.elastic.clients.elasticsearch._types.ErrorCause;\nimport co.elastic.clients.elasticsearch._types.ExpandWildcard;\nimport co.elastic.clients.elasticsearch._types.Level;\nimport co.elastic.clients.elasticsearch._types.NodeRole;\nimport co.elastic.clients.elasticsearch._types.query_dsl.Query;\nimport co.elastic.clients.elasticsearch.cluster.ExistsComponentTemplateRequest;\nimport co.elastic.clients.elasticsearch.cluster.HealthResponse;\nimport co.elastic.clients.elasticsearch.core.BulkRequest;\nimport co.elastic.clients.elasticsearch.core.BulkResponse;\nimport co.elastic.clients.elasticsearch.core.DeleteRequest;\nimport co.elastic.clients.elasticsearch.core.DeleteResponse;\nimport co.elastic.clients.elasticsearch.core.GetResponse;\nimport co.elastic.clients.elasticsearch.core.MsearchRequest;\nimport co.elastic.clients.elasticsearch.core.MsearchResponse;\nimport co.elastic.clients.elasticsearch.core.ScrollRequest;\nimport co.elastic.clients.elasticsearch.core.SearchRequest;\nimport co.elastic.clients.elasticsearch.core.SearchResponse;\nimport co.elastic.clients.elasticsearch.core.bulk.BulkOperation;\nimport co.elastic.clients.elasticsearch.core.search.Hit;\nimport co.elastic.clients.elasticsearch.core.search.ResponseBody;\nimport co.elastic.clients.elasticsearch.core.search.TotalHits;\nimport co.elastic.clients.elasticsearch.enrich.EnrichPolicy;\nimport co.elastic.clients.elasticsearch.enrich.EnrichStatsResponse;\nimport co.elastic.clients.elasticsearch.enrich.EnrichSummary;\nimport co.elastic.clients.elasticsearch.enrich.GetPolicyRequest;\nimport co.elastic.clients.elasticsearch.enrich.GetPolicyResponse;\nimport co.elastic.clients.elasticsearch.enrich.PutPolicyRequest;\nimport co.elastic.clients.elasticsearch.enrich.stats.ExecutingPolicy;\nimport co.elastic.clients.elasticsearch.indices.CreateIndexResponse;\nimport co.elastic.clients.elasticsearch.indices.DeleteIndexResponse;\nimport co.elastic.clients.elasticsearch.indices.ExistsIndexTemplateRequest;\nimport co.elastic.clients.elasticsearch.indices.GetIndexResponse;\nimport co.elastic.clients.elasticsearch.indices.GetIndexTemplateRequest;\nimport co.elastic.clients.elasticsearch.indices.GetIndexTemplateResponse;\nimport co.elastic.clients.elasticsearch.indices.GetMappingResponse;\nimport co.elastic.clients.elasticsearch.indices.IndicesStatsResponse;\nimport co.elastic.clients.elasticsearch.ingest.GetPipelineRequest;\nimport co.elastic.clients.elasticsearch.ingest.GetPipelineResponse;\nimport co.elastic.clients.elasticsearch.nodes.NodesStatsResponse;\nimport co.elastic.clients.elasticsearch.tasks.CancelRequest;\nimport co.elastic.clients.json.jackson.JacksonJsonpMapper;\nimport co.elastic.clients.transport.ElasticsearchTransport;\nimport co.elastic.clients.transport.rest_client.RestClientTransport;\nimport com.fasterxml.jackson.core.JsonProcessingException;\nimport com.fasterxml.jackson.databind.JsonNode;\nimport com.fasterxml.jackson.databind.ObjectMapper;\nimport com.fasterxml.jackson.databind.node.ArrayNode;\nimport com.fasterxml.jackson.databind.node.ObjectNode;\nimport jakarta.annotation.PostConstruct;\nimport java.io.ByteArrayInputStream;\nimport java.io.IOException;\nimport java.io.InputStream;\nimport java.net.URI;\nimport java.net.URISyntaxException;\nimport java.nio.charset.StandardCharsets;\nimport java.security.MessageDigest;\nimport java.security.NoSuchAlgorithmException;\nimport java.util.ArrayList;\nimport java.util.Arrays;\nimport java.util.Base64;\nimport java.util.HashMap;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.Objects;\nimport java.util.Set;\nimport java.util.UUID;\nimport java.util.concurrent.CompletableFuture;\nimport java.util.concurrent.atomic.AtomicReference;\nimport java.util.regex.Pattern;\nimport java.util.stream.Collectors;\nimport lombok.Getter;\nimport lombok.RequiredArgsConstructor;\nimport lombok.extern.slf4j.Slf4j;\nimport org.apache.http.Header;\nimport org.apache.http.HttpHost;\nimport org.apache.http.message.BasicHeader;\nimport org.elasticsearch.client.RestClient;\nimport org.elasticsearch.client.RestClientBuilder;\nimport org.springframework.beans.factory.annotation.Value;\nimport org.springframework.boot.web.client.RestTemplateBuilder;\nimport org.springframework.http.HttpEntity;\nimport org.springframework.http.HttpHeaders;\nimport org.springframework.http.HttpMethod;\nimport org.springframework.http.MediaType;\nimport org.springframework.http.ResponseEntity;\nimport org.springframework.stereotype.Service;\nimport org.springframework.web.client.RestTemplate;\nimport software.uncharted.elasticsearch.model.Task;\nimport software.uncharted.elasticsearch.model.TaskType;\n\n@Service\n@Slf4j\n@RequiredArgsConstructor\npublic class ElasticsearchService {\n\n  private final RestTemplateBuilder restTemplateBuilder;\n  private final ObjectMapper mapper;\n\n  @Value(\"${elasticsearch.url}\")\n  private String url;\n\n  @Value(\"${elasticsearch.username:}\")\n  private String username;\n\n  @Value(\"${elasticsearch.password:}\")\n  private String password;\n\n  private RestTemplate restTemplate;\n\n  @Getter\n  private ElasticsearchClient client = null;\n\n  private static final String SCROLL_TIME = \"5m\";\n\n  private static final Map<String, TaskType> TASK_TYPES = new HashMap<>();\n\n  static {\n    TASK_TYPES.put(\"indices:data/write/reindex\", REINDEX);\n    TASK_TYPES.put(\"indices:data/write/delete/byquery\", DELETE_BY_QUERY);\n    TASK_TYPES.put(\"indices:data/write/update/byquery\", UPDATE_BY_QUERY);\n  }\n\n  @Getter\n  public enum TEMPLATE_TYPE {\n    INDEX(\"index_template\"),\n    COMPONENT(\"component_template\");\n\n    private final String name;\n\n    TEMPLATE_TYPE(final String name) {\n      this.name = name;\n    }\n\n    public String getName() {\n      return name;\n    }\n\n    public String getEndpoint() {\n      return \"_\" + name;\n    }\n\n    public String getJacksonAccessor() {\n      return \"/\" + name + \"s/0/\" + name;\n    }\n  }\n\n  @PostConstruct\n  public void init() {\n    final RestClientBuilder httpClientBuilder = RestClient.builder(\n      HttpHost.create(url)\n    );\n    if (hasAuthentication()) {\n      httpClientBuilder.setDefaultHeaders(\n        new Header[] {\n          new BasicHeader(\n            \"Authorization\",\n            \"Basic \" +\n            Base64.getEncoder()\n              .encodeToString((username + \":\" + password).getBytes())\n          ),\n        }\n      );\n    }\n\n    // Create the new Java Client with the same low level client\n    final ElasticsearchTransport transport = new RestClientTransport(\n      httpClientBuilder.build(),\n      new JacksonJsonpMapper(mapper)\n    );\n\n    client = new ElasticsearchClient(transport);\n    try {\n      client.ping();\n    } catch (final IOException e) {\n      log.error(\"Error pinging Elasticsearch\", e);\n    }\n  }\n\n  protected RestTemplate getRestTemplate() {\n    if (restTemplate == null) {\n      initRestTemplate();\n    }\n    return restTemplate;\n  }\n\n  public ResponseEntity<JsonNode> executeWithRestClient(\n    final String endpoint,\n    final HttpMethod method,\n    final HttpEntity<?> entity\n  ) {\n    final URI uri = URI.create(\n      url + (endpoint.startsWith(\"/\") ? endpoint : \"/\" + endpoint)\n    );\n    return getRestTemplate().exchange(uri, method, entity, JsonNode.class);\n  }\n\n  private void initRestTemplate() {\n    if (hasAuthentication()) {\n      restTemplate = restTemplateBuilder\n        .basicAuthentication(username, password)\n        .build();\n    } else {\n      restTemplate = restTemplateBuilder.build();\n    }\n  }\n\n  private String hash(String input) {\n    try {\n      MessageDigest digest = MessageDigest.getInstance(\"SHA-256\");\n      byte[] hash = digest.digest(input.getBytes(StandardCharsets.UTF_8));\n      StringBuilder hexString = new StringBuilder();\n      for (byte b : hash) {\n        String hex = Integer.toHexString(0xff & b);\n        if (hex.length() == 1) hexString.append('0');\n        hexString.append(hex);\n      }\n      return hexString.toString();\n    } catch (NoSuchAlgorithmException e) {\n      throw new RuntimeException(\"SHA-256 hashing algorithm not available\", e);\n    }\n  }\n\n  /**\n   * Checks if we need to connect to Elasticsearch with authentication\n   *\n   * @return True if we need to connect with authentication, false otherwise\n   */\n  boolean hasAuthentication() {\n    return (\n      username != null &&\n      password != null &&\n      !username.isEmpty() &&\n      !password.isEmpty()\n    );\n  }\n\n  /**\n   * Search for documents in the given index\n   *\n   * @param index The index to search\n   * @param query The query to execute\n   * @param clazz The class of the documents to return\n   * @param <T>   The type of the documents to return\n   * @return The search response\n   */\n  public <T> SearchResponse<T> search(\n    final String index,\n    final Query query,\n    final Class<T> clazz\n  ) {\n    try {\n      log.info(\"Elasticsearch | Query | {} | {}\", index, query);\n      return client.search(\n        s ->\n          s\n            .index(index)\n            .query(query)\n            .trackTotalHits(b -> b.enabled(true))\n            .ignoreUnavailable(true),\n        clazz\n      );\n    } catch (final IOException e) {\n      log.error(\"Error searching index {}\", index, e);\n    }\n    return null;\n  }\n\n  /**\n   * Search for documents using a {@link SearchRequest} instance\n   *\n   * @param request The search request\n   * @param clazz   The class of the documents to return\n   * @param <T>     The type of the documents to return\n   * @return The search response\n   */\n  public <T> SearchResponse<T> search(\n    final SearchRequest request,\n    final Class<T> clazz\n  ) {\n    try {\n      log.info(\"Elasticsearch | Query | {}\", request);\n      return client.search(request, clazz);\n    } catch (final Exception e) {\n      if (e instanceof ElasticsearchException) {\n        final ErrorCause firstCause =\n          ((ElasticsearchException) e).response()\n            .error()\n            .rootCause()\n            .getFirst();\n        if (firstCause != null) {\n          log.error(\n            \"Error searching index {} | {}: {}\",\n            request.index(),\n            firstCause.type(),\n            firstCause.reason(),\n            e\n          );\n        }\n      } else {\n        log.error(\"Error searching index {}\", request.index(), e);\n      }\n    }\n    return null;\n  }\n\n  /**\n   * Get a document by its ID\n   *\n   * @param index      The ID of the index that the document is in\n   * @param documentId The ID of the document\n   * @param clazz      The class of the document to return\n   * @param <T>        The type of the document to return\n   * @return The Get response\n   */\n  public <T> GetResponse<T> getDocument(\n    final String index,\n    final String documentId,\n    final Class<T> clazz\n  ) {\n    try {\n      log.info(\"Elasticsearch | getDocument | {} | {}\", index, documentId);\n      return client.get(\n        getRequest -> getRequest.index(index).id(documentId),\n        clazz\n      );\n    } catch (final IOException e) {\n      log.error(\n        \"Error getting document {} from index {}\",\n        documentId,\n        index,\n        e\n      );\n    }\n    return null;\n  }\n\n  /**\n   * Delete a document by its ID\n   *\n   * @param index      The ID of the index that the document is in\n   * @param documentId The ID of the document to delete\n   * @return The delete response\n   */\n  public DeleteResponse deleteDocument(\n    final String index,\n    final String documentId\n  ) {\n    try {\n      log.info(\"Elasticsearch | deleteDocument | {} | {}\", index, documentId);\n      return client.delete(deleteRequest ->\n        deleteRequest.index(index).id(documentId)\n      );\n    } catch (final IOException e) {\n      log.error(\n        \"Error deleting document {} from index {}\",\n        documentId,\n        index,\n        e\n      );\n    }\n    return null;\n  }\n\n  /**\n   * Search for documents using a {@link MsearchRequest} instance. It is up to the caller to ensure that the\n   * order of the initial queries and the order of the responses match\n   *\n   * @param request The msearch request\n   * @param clazz   The class of the documents to return\n   * @param <T>     The type of the documents to return\n   * @return The msearch response\n   */\n  public <T> MsearchResponse<T> msearch(\n    final MsearchRequest request,\n    final Class<T> clazz\n  ) {\n    try {\n      log.info(\"Elasticsearch | MQuery | {}\", request);\n      return client.msearch(request, clazz);\n    } catch (final IOException e) {\n      log.error(\"Error searching index {}\", request.index(), e);\n    }\n    return null;\n  }\n\n  /**\n   * Scroll through the given index and produce batches of documents.  Example:\n   * <pre>\n   *   final SearchRequest request = SearchRequest.of(b -> b\n   *    .index(\"books\")\n   *    .query(Query.of(q -> q.matchAll(m -> m)))\n   *    .sort(Sort.of(s -> s.field(\"title\")))\n   *    .size(100));\n   *\n   *  elasticsearchService.scroll(request, Book.class, batch -> {\n   *     batch.forEach(book -> // do something with the book);\n   *   });\n   * </pre>\n   * <p>\n   * <p>\n   * NOTE: The scroll API is not implemented in the Java client, so we have to use the rest client to implement this. The\n   * recommended way of using search_after only works when you have a sort field that is unique across all documents.\n   * Since we cannot assume this for all datasets, we have to use the scroll API.\n   *\n   * @param request  The {@link SearchRequest} to use for the scroll. The size field indicates the number of documents\n   *                 to return in each batch\n   * @param clazz    The class of the documents to return\n   * @param producer The producer to call with each batch of documents and for error handling\n   * @param <T>      The type of the documents to return\n   */\n  public <T> void scroll(\n    final SearchRequest request,\n    final Class<T> clazz,\n    final BatchProducer<List<Hit<T>>> producer\n  ) throws IOException {\n    if (request.sort() == null || request.sort().isEmpty()) {\n      throw new IllegalArgumentException(\"Scrolling requires a sort field\");\n    }\n\n    // Scroll through the documents\n    final AtomicReference<String> scrollIdRef = new AtomicReference<>();\n    boolean done = false;\n    do {\n      final int size = request.size() != null ? request.size() : 100;\n      final SearchRequest.Builder copy = new SearchRequest.Builder()\n        .size(size)\n        .sort(request.sort())\n        .index(request.index())\n        .scroll(t -> t.time(SCROLL_TIME))\n        .query(\n          request.query() != null\n            ? request.query()\n            : Query.of(q -> q.matchAll(m -> m))\n        );\n\n      if (request.source() != null) {\n        copy.source(request.source());\n      }\n\n      final ResponseBody<T> response;\n      if (scrollIdRef.get() == null) {\n        response = getClient().search(copy.build(), clazz);\n      } else {\n        response = getClient()\n          .scroll(\n            ScrollRequest.of(b ->\n              b.scrollId(scrollIdRef.get()).scroll(s -> s.time(SCROLL_TIME))\n            ),\n            clazz\n          );\n      }\n\n      final List<Hit<T>> batch = response.hits().hits().stream().toList();\n      producer.onBatch(batch);\n      if (response.hits().hits().size() < size) {\n        done = true;\n        if (scrollIdRef.get() != null) {\n          getClient().clearScroll(b -> b.scrollId(scrollIdRef.get()));\n        }\n      } else {\n        scrollIdRef.set(response.scrollId());\n      }\n    } while (!done);\n  }\n\n  /**\n   * Counts the number of documents in the given index\n   *\n   * @param index The index to count\n   * @return The number of documents in the index\n   */\n  public Long count(final String index) {\n    return count(index, Query.of(q -> q.matchAll(m -> m)));\n  }\n\n  /**\n   * Count the number of documents in the given index that match the given query\n   *\n   * @param index The index to search\n   * @param query The query to execute\n   * @return The number of documents that match the query\n   */\n  public Long count(final String index, final Query query) {\n    final SearchResponse<Object> response = search(index, query, Object.class);\n    if (response != null) {\n      final TotalHits totalHits = response.hits().total();\n      if (totalHits != null) {\n        return totalHits.value();\n      }\n    }\n    return null;\n  }\n\n  /**\n   * Delete the given index\n   *\n   * @param index The index to delete\n   * @return The optional delete index response. If the delete index fails,\n   */\n  public DeleteIndexResponse deleteIndex(final String index) {\n    try {\n      return client.indices().delete(d -> d.index(index));\n    } catch (final IOException e) {\n      log.error(\"Error deleting index {}\", index, e);\n    }\n    return null;\n  }\n\n  /**\n   * Open the given index\n   *\n   * @param index The index to open\n   * @return The acknowledged response\n   */\n  public AcknowledgedResponse openIndex(final String index) {\n    try {\n      return client.indices().open(d -> d.index(index));\n    } catch (final IOException e) {\n      log.error(\"Error opening index {}\", index, e);\n    }\n    return null;\n  }\n\n  /**\n   * Close the given index\n   *\n   * @param index The index to close\n   * @return The acknowledged response\n   */\n  public AcknowledgedResponse closeIndex(final String index) {\n    try {\n      return client.indices().close(d -> d.index(index));\n    } catch (final IOException e) {\n      log.error(\"Error closing index {}\", index, e);\n    }\n    return null;\n  }\n\n  /**\n   * Refresh the given index\n   *\n   * @param index The index to refresh\n   */\n  public void refresh(final String index) {\n    try {\n      client.indices().refresh(r -> r.index(index).ignoreUnavailable(true));\n    } catch (final IOException e) {\n      log.error(\"Error refreshing index {}\", index, e);\n    }\n  }\n\n  /**\n   * Insert the given documents into the given index with the given ids\n   *\n   * @param index     The index to insert the documents into\n   * @param documents the documents to insert\n   * @param ids       The ids of the documents to insert, parallel to the documents\n   * @return The bulk response\n   * @throws IOException              If the bulk insert fails\n   * @throws IllegalArgumentException If the number of documents and ids are not the same\n   */\n  public BulkResponse insert(\n    final String index,\n    final List documents,\n    final List<String> ids\n  ) throws IOException {\n    if (ids != null && documents.size() != ids.size()) {\n      throw new IllegalArgumentException(\n        \"The number of documents and ids must be the same\"\n      );\n    }\n\n    final List<BulkOperation> bulkOperations = new ArrayList<>();\n    for (int i = 0; i < documents.size(); i++) {\n      final String id = ids != null ? ids.get(i) : UUID.randomUUID().toString();\n      final Object document = documents.get(i);\n      bulkOperations.add(\n        BulkOperation.of(b ->\n          b.index(op -> op.index(index).id(id).document(document))\n        )\n      );\n    }\n\n    log.info(\n      \"Elasticsearch | Bulk | Inserting {} documents into index {}\",\n      documents.size(),\n      index\n    );\n    return client.bulk(\n      BulkRequest.of(bulkRequest ->\n        bulkRequest\n          .index(index)\n          .operations(bulkOperations)\n          .pipeline(\"full_text_search\")\n      )\n    );\n  }\n\n  /**\n   * Insert the given documents into the given index\n   *\n   * @param index\n   * @param documents\n   * @return\n   * @throws IOException\n   */\n  public BulkResponse insert(final String index, final List documents)\n    throws IOException {\n    return insert(index, documents, null);\n  }\n\n  /**\n   * Upsert the given documents into the given index with the given ids\n   *\n   * @param index     The index to upsert the documents into\n   * @param documents The documents to upsert\n   * @param ids       The ids of the documents to upsert, parallel to the documents\n   * @return The bulk response\n   */\n  public BulkResponse upsert(\n    final String index,\n    final List documents,\n    final List<String> ids\n  ) {\n    if (ids != null && documents.size() != ids.size()) {\n      throw new IllegalArgumentException(\n        \"The number of documents and ids must be the same\"\n      );\n    }\n\n    final List<BulkOperation> bulkOperations = new ArrayList<>();\n    for (int i = 0; i < documents.size(); i++) {\n      final String id = ids != null ? ids.get(i) : UUID.randomUUID().toString();\n      final Object document = documents.get(i);\n      bulkOperations.add(\n        BulkOperation.of(b ->\n          b.update(op ->\n            op\n              .index(index)\n              .id(id)\n              .action(ua -> ua.doc(document).docAsUpsert(true))\n          )\n        )\n      );\n    }\n\n    log.info(\n      \"Elasticsearch | Bulk | Upserting {} documents into index {}\",\n      documents.size(),\n      index\n    );\n\n    BulkResponse bulkResponse = null;\n    try {\n      // Perform the upsert operation\n      bulkResponse = client.bulk(\n        BulkRequest.of(bulkRequest ->\n          bulkRequest.index(index).operations(bulkOperations)\n        )\n      );\n\n      // Refresh to ensure documents are searchable immediately\n      refresh(index);\n\n      if (ids != null && !ids.isEmpty()) {\n        refreshFullTextSearchForDocuments(index, ids);\n      }\n    } catch (final IOException e) {\n      log.error(\"Error upserting documents\", e);\n    }\n    return bulkResponse;\n  }\n\n  /**\n   * Helper method to refresh full_text_search for specific documents\n   */\n  private void refreshFullTextSearchForDocuments(\n    final String index,\n    final List<String> documentIds\n  ) {\n    try {\n      // Build query to match only the specified document IDs\n      final String idsQuery = String.format(\n        \"{\\\"ids\\\": {\\\"values\\\": [%s]}}\",\n        documentIds\n          .stream()\n          .map(id -> \"\\\"\" + id + \"\\\"\")\n          .collect(Collectors.joining(\", \"))\n      );\n\n      // Use updateByQuery with the full_text_search pipeline\n      String taskId = updateByQuery(index, idsQuery, \"full_text_search\");\n\n      log.info(\n        \"Started full_text_search refresh for {} documents, task: {}\",\n        documentIds.size(),\n        taskId\n      );\n    } catch (Exception e) {\n      log.error(\n        \"Failed to refresh full_text_search for upserted documents: {}\",\n        e.getMessage()\n      );\n    }\n  }\n\n  /**\n   * Create a new index with the given name and default replicas and shards\n   *\n   * @param name The name of the index\n   * @return {@link CreateIndexResponse}\n   */\n  public CreateIndexResponse createIndex(final String name) {\n    return createIndex(name, 0, 1);\n  }\n\n  /**\n   * Create the given index\n   *\n   * @param name The index name\n   * @return {@link CreateIndexResponse}\n   */\n  public CreateIndexResponse createIndex(\n    final String name,\n    final int replicas,\n    final int shards\n  ) {\n    log.info(\n      \"Elasticsearch | Create index {} ({} shards, {} replicas\",\n      name,\n      shards,\n      replicas\n    );\n    try {\n      return client\n        .indices()\n        .create(c ->\n          c\n            .index(name)\n            .settings(sb ->\n              sb.numberOfReplicas(replicas + \"\").numberOfShards(shards + \"\")\n            )\n        );\n    } catch (final IOException e) {\n      log.error(\"Error creating index {}\", name, e);\n    }\n    return null;\n  }\n\n  /**\n   * Checks if the given index exists\n   *\n   * @param name The index name\n   * @return The boolean response, empty if the call fails\n   */\n  public Boolean hasIndex(final String name) {\n    try {\n      return client.indices().exists(e -> e.index(name)).value();\n    } catch (final IOException e) {\n      log.error(\"Error checking if index exists {}\", name, e);\n    }\n    return null;\n  }\n\n  /**\n   * Put an index template to the cluster\n   *\n   * @param name         The name of the index template\n   * @param templateJson The index template json string\n   * @return True if the index template was successfully added, false otherwise\n   */\n  public boolean putIndexTemplate(\n    final String name,\n    final String templateJson\n  ) {\n    return putTyped(\n      name,\n      appendHash(templateJson),\n      \"index template\",\n      \"_index_template\"\n    );\n  }\n\n  /**\n   * Delete the index template with the given name\n   *\n   * @param name\n   * @return true if the index template was successfully deleted, false otherwise\n   */\n  public boolean deleteIndexTemplate(final String name) {\n    try {\n      final AcknowledgedResponse response = client\n        .indices()\n        .deleteIndexTemplate(d -> d.name(name));\n      return response.acknowledged();\n    } catch (final IOException e) {\n      log.error(\"Error deleting index template {}\", name, e);\n    }\n    return false;\n  }\n\n  /**\n   * Returns true if the ES cluster contains the index template with the provided name, false otherwise\n   *\n   * @param name The name of the index template to check existence for\n   * @return True if the index template is contained in the cluster, false otherwise\n   */\n  public boolean containsIndexTemplate(final String name) {\n    final ExistsIndexTemplateRequest request = ExistsIndexTemplateRequest.of(\n      b -> b.name(name)\n    );\n    try {\n      return client.indices().existsIndexTemplate(request).value();\n    } catch (final IOException e) {\n      log.error(\"Error checking existence of template {}\", name, e);\n    }\n    return false;\n  }\n\n  /**\n   * Put a template to the cluster\n   *\n   * @param name         The name of the template\n   * @param templateJson The template json string\n   * @return True if the template was succesfully added, false otherwise\n   */\n  public boolean putTemplate(final String name, final String templateJson) {\n    return putTyped(name, templateJson, \"template\", \"_template\");\n  }\n\n  /**\n   * Returns a list of template names that match the given index based on the pattern specified\n   * in the template\n   *\n   * @param index           The index to match templates for\n   * @param templatePattern The (optional) pattern to filter template names against\n   * @return The list of matching template names\n   */\n  public List<String> getMatchingTemplates(\n    final String index,\n    final String templatePattern\n  ) {\n    final List<String> matching = new ArrayList<>();\n    final Map<String, List<String>> templateIndexPatterns =\n      getTemplateIndexPatterns();\n    final Pattern templateNamePattern = Pattern.compile(\n      templatePattern != null ? templatePattern.replace(\"*\", \".*\") : \".*\"\n    );\n    templateIndexPatterns.forEach((templateName, indexPatterns) -> {\n      if (templateNamePattern.matcher(templateName).matches()) {\n        indexPatterns\n          .stream()\n          .map(pattern -> pattern.replace(\"*\", \".*\"))\n          .forEach(pattern -> {\n            if (Pattern.compile(pattern).matcher(index).matches()) {\n              matching.add(templateName);\n            }\n          });\n      }\n    });\n    return matching;\n  }\n\n  /**\n   * Returns a map of template names to the index patterns they match\n   *\n   * @return The map of template names to index patterns\n   */\n  public Map<String, List<String>> getTemplateIndexPatterns() {\n    final Map<String, List<String>> templateIndexPatterns = new HashMap<>();\n    try {\n      final GetIndexTemplateResponse response = client\n        .indices()\n        .getIndexTemplate();\n      response\n        .indexTemplates()\n        .forEach(indexTemplateItem ->\n          templateIndexPatterns.put(\n            indexTemplateItem.name(),\n            indexTemplateItem.indexTemplate().indexPatterns()\n          )\n        );\n    } catch (final IOException e) {\n      log.error(\"Error getting templates\", e);\n    }\n    return templateIndexPatterns;\n  }\n\n  /**\n   * Returns true if the ES cluster contains a template that matches the provided index, false otherwise\n   *\n   * @param index The index to check for matching templates\n   * @return True if the cluster contains a matching template, false otherwise\n   */\n  public boolean hasMatchingTemplate(\n    final String index,\n    final String templatePattern\n  ) {\n    return !getMatchingTemplates(index, templatePattern).isEmpty();\n  }\n\n  /**\n   * Checks for existence of a component template with the given name\n   *\n   * @param name The name of the component template to check for\n   * @return True if the cluster contains a matching component template, false otherwise\n   */\n  public boolean componentTemplateExists(final String name) {\n    try {\n      final ExistsComponentTemplateRequest request =\n        ExistsComponentTemplateRequest.of(b -> b.name(name));\n      return client.cluster().existsComponentTemplate(request).value();\n    } catch (final IOException e) {\n      return false;\n    }\n  }\n\n  //checks if a pipeline exists\n  public boolean pipelineExists(final String name) {\n    try {\n      final GetPipelineResponse response = client\n        .ingest()\n        .getPipeline(GetPipelineRequest.of(b -> b.id(name)));\n      return response != null && response.result().containsKey(name);\n    } catch (final IOException e) {\n      log.error(\"Error checking if pipeline exists\", e);\n      return false;\n    }\n  }\n\n  //checks if a pipeline exists and is a specific version\n  public boolean pipelineExists(final String name, final long version) {\n    try {\n      final GetPipelineResponse response = client\n        .ingest()\n        .getPipeline(GetPipelineRequest.of(b -> b.id(name)));\n\n      if (response != null) {\n        final var pipeline = response.result().get(name);\n        if (pipeline != null) {\n          return version == pipeline.version();\n        }\n      }\n    } catch (final IOException e) {\n      log.error(\"Error checking if pipeline exists\", e);\n    }\n    return false;\n  }\n\n  public boolean putPipeline(\n    final String name,\n    final InputStream pipelineJson\n  ) {\n    try {\n      client.ingest().putPipeline(b -> b.id(name).withJson(pipelineJson));\n      return true;\n    } catch (final Exception e) {\n      log.error(\"Error putting pipeline\", e);\n      return false;\n    }\n  }\n\n  public boolean deletePipeline(final String name) {\n    try {\n      client.ingest().deletePipeline(b -> b.id(name));\n      return true;\n    } catch (final IOException e) {\n      log.error(\"Error deleting pipeline\", e);\n      return false;\n    }\n  }\n\n  /**\n   * Checks for existence of an index template with the given name\n   *\n   * @param name The name of the index template to check for\n   * @return True if the cluster contains a matching index template, false otherwise\n   */\n  public boolean indexTemplateExists(final String name) {\n    try {\n      final ExistsIndexTemplateRequest request = ExistsIndexTemplateRequest.of(\n        b -> b.name(name)\n      );\n      return client.indices().existsIndexTemplate(request).value();\n    } catch (final IOException e) {\n      return false;\n    }\n  }\n\n  public static String generateTemplateName(final String json) {\n    try {\n      // Compute SHA-256 hash of the JSON string\n      final MessageDigest digest = MessageDigest.getInstance(\"SHA-256\");\n      final byte[] hash = digest.digest(json.getBytes(StandardCharsets.UTF_8));\n      // Convert the byte array to a hexadecimal string\n      final StringBuilder hexString = new StringBuilder();\n      for (final byte b : hash) {\n        final String hex = Integer.toHexString(0xff & b);\n        if (hex.length() == 1) hexString.append('0');\n        hexString.append(hex);\n      }\n      // Prefix the hexadecimal string with a project-specific name\n      return hexString.toString();\n    } catch (final NoSuchAlgorithmException e) {\n      throw new RuntimeException(\"SHA-256 hashing algorithm not available\", e);\n    }\n  }\n\n  public static String generateIndexName(\n    final String prefix,\n    final String templateName\n  ) {\n    return prefix + \"_\" + templateName;\n  }\n\n  private boolean putTyped(\n    final String name,\n    final String typedJson,\n    final String typeName,\n    final String indexName\n  ) {\n    log.info(\"Putting \" + typeName + \": {}\", name);\n\n    try {\n      final HttpHeaders headers = new HttpHeaders();\n      headers.setContentType(MediaType.APPLICATION_JSON);\n      final HttpEntity<String> entity = new HttpEntity<>(typedJson, headers);\n      final ResponseEntity<JsonNode> response = getRestTemplate()\n        .exchange(\n          new URI(url + \"/\" + indexName + \"/\" + name),\n          HttpMethod.PUT,\n          entity,\n          JsonNode.class\n        );\n      final JsonNode body = response.getBody();\n      if (body != null) {\n        return body.at(\"/acknowledged\").asBoolean();\n      }\n    } catch (final Exception e) {\n      log.error(\"Error putting \" + typeName + \" {}\", name, e);\n    }\n    return false;\n  }\n\n  public boolean putComponentTemplate(\n    final String name,\n    final String templateJson\n  ) {\n    return putTyped(\n      name,\n      appendHash(templateJson),\n      \"component template\",\n      \"_component_template\"\n    );\n  }\n\n  private String appendHash(String templateJson) {\n    try {\n      final String hash = hash(templateJson);\n      final ObjectNode templateNode = (ObjectNode) mapper.readTree(\n        templateJson\n      );\n      ObjectNode metaNode;\n      if (templateNode.has(\"_meta\") && templateNode.get(\"_meta\").isObject()) {\n        metaNode = (ObjectNode) templateNode.get(\"_meta\");\n      } else {\n        metaNode = templateNode.putObject(\"_meta\");\n      }\n      metaNode.put(\"hash\", hash);\n      return mapper.writeValueAsString(templateNode);\n    } catch (final JsonProcessingException e) {\n      throw new RuntimeException(e);\n    }\n  }\n\n  public JsonNode getTemplate(final String name, final TEMPLATE_TYPE type) {\n    try {\n      final ResponseEntity<JsonNode> response = getRestTemplate()\n        .exchange(\n          new URI(url + \"/\" + type.getEndpoint() + \"/\" + name),\n          HttpMethod.GET,\n          null,\n          JsonNode.class\n        );\n      return response.getBody().at(type.getJacksonAccessor());\n    } catch (URISyntaxException e) {\n      log.error(\"Failed to parse URI\", e);\n      return null;\n    } catch (Exception e) {\n      log.error(\"Error getting template {}\", name, e);\n      return null;\n    }\n  }\n\n  /**\n   * Put a template to the cluster\n   *\n   * @param name         The name of the template\n   * @param templateNode The template json\n   * @return True if the template was succesfully added, false otherwise\n   */\n  public boolean putTemplate(final String name, final JsonNode templateNode) {\n    try {\n      return putTemplate(name, mapper.writeValueAsString(templateNode));\n    } catch (final JsonProcessingException e) {\n      log.error(\"Could not serialize templateNode to json\", e);\n    }\n    return false;\n  }\n\n  /**\n   * Find the structure of the given text\n   *\n   * @param text List of JsonNode objects to find the structure of\n   * @return JsonNode with the response from the find_structure endpoint\n   */\n  public JsonNode findStructure(final List<JsonNode> text) {\n    //use the field mapping api to get the structure of the index;\n    try {\n      //convert jsonnode list to one jsonlines string\n      final String jsonLines = text\n        .stream()\n        .filter(node -> !node.isEmpty())\n        .map(JsonNode::toString)\n        .collect(Collectors.joining(\"\\n\"));\n\n      final HttpHeaders headers = new HttpHeaders();\n      headers.setContentType(MediaType.APPLICATION_JSON);\n      final HttpEntity<String> entity = new HttpEntity<>(jsonLines, headers);\n      final ResponseEntity<JsonNode> response = getRestTemplate()\n        .exchange(\n          new URI(url + \"/_text_structure/find_structure\"),\n          HttpMethod.POST,\n          entity,\n          JsonNode.class\n        );\n      return response.getBody();\n    } catch (final Exception e) {\n      log.error(\"Error finding structure\", e);\n      return null;\n    }\n  }\n\n  /**\n   * Get the index information from the cluster\n   *\n   * @return The index information\n   */\n  public GetIndexResponse getIndices() {\n    return getIndices(\"*\");\n  }\n\n  /**\n   * Get the index information from the cluster\n   *\n   * @param indexPattern The pattern to match indices against\n   * @return The index information\n   */\n  public GetIndexResponse getIndices(final String indexPattern) {\n    try {\n      return client\n        .indices()\n        .get(b ->\n          b\n            .index(indexPattern)\n            .expandWildcards(\n              List.of(ExpandWildcard.Open, ExpandWildcard.Closed)\n            )\n        );\n    } catch (final IOException e) {\n      log.error(\"Error getting indices\", e);\n    }\n    return null;\n  }\n\n  /**\n   * Get the indices stats for all open and close indices\n   *\n   * @return The indices stats response\n   */\n  public IndicesStatsResponse getIndicesStats() {\n    return getIndicesStats(\"*\");\n  }\n\n  /**\n   * Get the {@link IndicesStatsResponse} for the given index pattern.  See <a href=\"https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-stats.html\">here</a>\n   *\n   * @param indexPattern The index pattern to get stats for\n   * @return The indices stats response\n   */\n  public IndicesStatsResponse getIndicesStats(final String indexPattern) {\n    try {\n      return client\n        .indices()\n        .stats(b ->\n          b\n            .index(indexPattern)\n            .forbidClosedIndices(false)\n            .expandWildcards(\n              List.of(ExpandWildcard.Open, ExpandWildcard.Closed)\n            )\n        );\n    } catch (final IOException e) {\n      log.error(\"Error getting indices stats\", e);\n    }\n    return null;\n  }\n\n  /**\n   * Get the index health from the cluster\n   *\n   * @return The index health\n   */\n  public HealthResponse getHealth() {\n    return getHealth(null);\n  }\n\n  /**\n   * Get the index health from the cluster\n   *\n   * @param indices The indices to get health for. If null, all indices are returned\n   * @return The index health\n   */\n  public HealthResponse getHealth(final List<String> indices) {\n    try {\n      return client\n        .cluster()\n        .health(builder -> {\n          builder.level(Level.Indices);\n          if (indices != null) {\n            builder.index(indices);\n          } else {\n            builder.index(\"*\");\n          }\n          return builder;\n        });\n    } catch (final IOException e) {\n      log.error(\"Error getting index health\", e);\n    }\n    return null;\n  }\n\n  /**\n   * Get the node information from the cluster for the given.  See <a href=\"https://www.elastic.co/guide/en/elasticsearch/reference/current/cluster-nodes-stats.html#cluster-nodes-stats-api-path-params\">here</a>\n   * for a list of available metrics\n   *\n   * @param metrics The metrics to get for the nodes statistics\n   * @return The raw {@link NodesStatsResponse} object\n   */\n  public NodesStatsResponse getNodeStats(final List<String> metrics) {\n    try {\n      return client.nodes().stats(builder -> builder.metric(metrics));\n    } catch (final IOException e) {\n      log.error(\"Error getting node stats\", e);\n    }\n    return null;\n  }\n\n  /**\n   * Get the node information from the cluster based on the {@link NodeRole} type. This is useful for determining\n   * what type of nodes are in the cluster and can be used to infer the capabilities of the cluster\n   *\n   * @return The number of nodes matching the given role\n   */\n  public int getNodeCount(final NodeRole role) {\n    final NodesStatsResponse response = getNodeStats(List.of(\"process\"));\n    if (response != null) {\n      return (int) response\n        .nodes()\n        .values()\n        .stream()\n        .filter(node -> node.roles().contains(role))\n        .count();\n    }\n    return 0;\n  }\n\n  /**\n   * Get the property names for the given index. This is useful for determining the exact fields that are written to the index\n   * when the template defines dynamic path matched fields. We ignore fields that start with an underscore as they\n   * are internal to Elasticsearch\n   *\n   * @param index The index to get the mappings for\n   * @return The property names in the index\n   */\n  public List<String> getAllPropertyNames(final String index) {\n    return getAllPropertyNames(index, \"*\");\n  }\n\n  /**\n   * Get the property names for the given index and fields. This is useful for determining the exact fields that are written to the index\n   * when the template defines dynamic path matched fields. We ignore fields that start with an underscore as they\n   * are internal to Elasticsearch\n   *\n   * @param index  The index to get the mappings for\n   * @param fields The field pattern to get the mappings for\n   * @return\n   */\n  public List<String> getAllPropertyNames(\n    final String index,\n    final String fields\n  ) {\n    try {\n      return client\n        .indices()\n        .getFieldMapping(b -> b.index(index).fields(fields))\n        .get(index)\n        .mappings()\n        .keySet()\n        .stream()\n        .filter(key -> !key.startsWith(\"_\"))\n        .collect(Collectors.toList());\n    } catch (final IOException e) {\n      log.error(\"Error getting all mappings\", e);\n    }\n    return null;\n  }\n\n  /**\n   * @param sourceIndex          the index to enrich\n   * @param sourceField          the field to use for enriching\n   * @param includedSourceFields the fields to include in the enrich policy\n   * @return the name of the enrich policy\n   */\n  public String createEnrichPolicy(\n    final String sourceIndex,\n    final String sourceField,\n    final List<String> includedSourceFields\n  ) {\n    final String enrichPolicyName = UUID.nameUUIDFromBytes(\n      (sourceIndex +\n        sourceField +\n        includedSourceFields\n          .stream()\n          .sorted()\n          .collect(Collectors.joining(\" \"))).getBytes()\n    ).toString();\n    if (enrichPolicyExists(enrichPolicyName)) {\n      return enrichPolicyName;\n    }\n\n    final PutPolicyRequest putPolicyRequest = PutPolicyRequest.of(b ->\n      b\n        .name(enrichPolicyName)\n        .match(mb ->\n          mb\n            .matchField(sourceField)\n            .indices(List.of(sourceIndex))\n            .enrichFields(includedSourceFields)\n        )\n    );\n    try {\n      client.enrich().putPolicy(putPolicyRequest);\n    } catch (final IOException e) {\n      log.error(\"Error creating enrich policy\", e);\n      return null;\n    }\n    return enrichPolicyName;\n  }\n\n  /**\n   * Check if an enrich policy with the given name exists\n   *\n   * @param name the name of the enrich policy to check\n   * @return true if the enrich policy exists, false otherwise\n   */\n  public boolean enrichPolicyExists(final String name) {\n    return getEnrichPolicy(name).policies().stream().findFirst().isPresent();\n  }\n\n  /**\n   * Get all enrich policies\n   *\n   * @return the enrich policies response\n   */\n  public GetPolicyResponse getEnrichPolicies() {\n    try {\n      return client.enrich().getPolicy(GetPolicyRequest.of(b -> b));\n    } catch (final IOException e) {\n      log.error(\"Error getting enrich policies\", e);\n      return null;\n    }\n  }\n\n  /**\n   * Gets an enrichment policy by name\n   *\n   * @param name the name of the enrich policy to get\n   * @return the enrich policy response\n   */\n  public GetPolicyResponse getEnrichPolicy(final String name) {\n    try {\n      final GetPolicyRequest request = GetPolicyRequest.of(b -> b.name(name));\n      return client.enrich().getPolicy(request);\n    } catch (final IOException e) {\n      log.error(\"Error getting enrich policy\", e);\n      return null;\n    }\n  }\n\n  /**\n   * Delete the enrich policy with the given name\n   *\n   * @param name the name of the enrich policy to delete\n   * @return true if the enrich policy deletion request was acknowledged, false otherwise\n   */\n  public boolean deleteEnrichPolicy(final String name) {\n    try {\n      return client.enrich().deletePolicy(b -> b.name(name)).acknowledged();\n    } catch (final IOException e) {\n      log.error(\"Error deleting enrich policy\", e);\n    }\n    return false;\n  }\n\n  /**\n   * Get the running enrich stats. There is no way to get historical enrich stats as this is currently an open\n   * issue in Elasticsearch. See <a href=\"https://github.com/elastic/elasticsearch/issues/73353\">here</a>\n   *\n   * @return the list of running enrich stats\n   */\n  public List<ExecutingPolicy> getRunningEnrichStats() {\n    try {\n      log.info(\"Getting enrich stats\");\n      final EnrichStatsResponse response = client.enrich().stats();\n      return response.executingPolicies();\n    } catch (final IOException e) {\n      log.error(\"Error getting enrich stats\", e);\n    }\n    return null;\n  }\n\n  /**\n   * Execute the enrich policy with the given name. This needs to be done after the policy is created and anytime the data\n   * in the source index changes. Use the rest template to execute the policy as the client does not return the task id\n   *\n   * @param enrichPolicyName the name of the enrich policy to execute\n   * @return the task id of the enrich policy execution\n   */\n  public String executeEnrichPolicy(final String enrichPolicyName) {\n    try {\n      final HttpHeaders headers = new HttpHeaders();\n      headers.setContentType(MediaType.APPLICATION_JSON);\n      final HttpEntity<String> entity = new HttpEntity<>(null, headers);\n      final ResponseEntity<JsonNode> response = getRestTemplate()\n        .exchange(\n          new URI(\n            url +\n            \"/_enrich/policy/\" +\n            enrichPolicyName +\n            \"/_execute?wait_for_completion=false\"\n          ),\n          HttpMethod.POST,\n          entity,\n          JsonNode.class\n        );\n      return response.getBody().at(\"/task\").asText();\n    } catch (final Exception e) {\n      log.error(\"Error executing enrich policy\", e);\n    }\n    return null;\n  }\n\n  /**\n   * Create an enrich pipeline that uses the given enrich policy to enrich the data in the target field\n   *\n   * @param enrichPolicy    the name of the enrich policy to use\n   * @param targetJoinField the field in the index to use for joining the enrichment data\n   * @return the name of the enrich pipeline\n   */\n  public String createEnrichPipeline(\n    final String enrichPolicy,\n    final String targetJoinField\n  ) {\n    return createEnrichPipeline(enrichPolicy, targetJoinField, Map.of());\n  }\n\n  /**\n   * Create an enrich pipeline that uses the given enrich policy to enrich the data in the target field\n   *\n   * @param enrichPolicy    the name of the enrich policy to use\n   * @param targetJoinField the field in the index to use for joining the enrichment data\n   * @param renameFields    a map from original to new field names that are written to enrich_temp\n   * @return the name of the enrich pipeline\n   */\n  public String createEnrichPipeline(\n    final String enrichPolicy,\n    final String targetJoinField,\n    final Map<String, String> renameFields\n  ) {\n    final String enrichPipelineName = UUID.nameUUIDFromBytes(\n      (enrichPolicy + targetJoinField).getBytes()\n    ).toString();\n    if (pipelineExists(enrichPipelineName)) {\n      return enrichPipelineName;\n    }\n\n    final GetPolicyResponse response = getEnrichPolicy(enrichPolicy);\n    if (response == null) {\n      log.error(\"Enrich policy {} does not exist\", enrichPolicy);\n      return null;\n    }\n\n    // Pull out the enrich policy and create the pipeline\n    final EnrichSummary summary = response\n      .policies()\n      .stream()\n      .findFirst()\n      .orElseThrow();\n    final EnrichPolicy policy = summary.config().value();\n\n    final String description = String.format(\n      \"SELECT %s from <target_index> where %s.%s = <target_index>.%s\",\n      policy.enrichFields(),\n      policy.indices().getFirst(),\n      policy.matchField(),\n      targetJoinField\n    );\n\n    final ObjectNode enrichPolicyNode = mapper.createObjectNode();\n    enrichPolicyNode.put(\"policy_name\", enrichPolicy);\n    enrichPolicyNode.put(\"field\", targetJoinField);\n    enrichPolicyNode.put(\"target_field\", \"enrich_temp\");\n    enrichPolicyNode.put(\"description\", description);\n\n    final ObjectNode enrichPolicyObjectNode = mapper.createObjectNode();\n    enrichPolicyObjectNode.set(\"enrich\", enrichPolicyNode);\n\n    final ObjectNode scriptNode = mapper.createObjectNode();\n    final String src = Arrays.stream(\n      \"\"\"\n      def deduplicate(def arr) {\n        return arr.stream().distinct().collect(Collectors.toList());\n      }\n\n      def mergeIntoArray(def existing, def newValue) {\n          if (existing == null) {\n              return newValue;\n          }\n          if (existing instanceof List) {\n              if (newValue instanceof List) {\n                  existing.addAll(newValue);\n              } else {\n                  existing.add(newValue);\n              }\n          } else {\n              if (newValue instanceof List) {\n                  newValue.add(0, existing);\n                  return newValue;\n              } else {\n                  return [existing, newValue];\n              }\n          }\n          return existing;\n      }\n      void mergeObjects(def original, def enrich) {\n          if (original == null || enrich == null) {\n              return;\n          }\n          for (def entry : enrich.entrySet()) {\n              def key = entry.getKey();\n              def newValue = entry.getValue();\n              if (original.containsKey(key)) {\n                  def existingValue = original[key];\n                  if (existingValue instanceof Map && newValue instanceof Map) {\n                      mergeObjects(existingValue, newValue);\n                  } else {\n                      original[key] = deduplicate(mergeIntoArray(existingValue, newValue));\n                  }\n              } else {\n                  original[key] = newValue;\n              }\n          }\n          return;\n      }\n\n      // Iterate over the enriched data and merge into the document\n      mergeObjects(ctx, ctx.enrich_temp);\n\n      \"\"\".split(\"\\n\")\n    )\n      .map(String::trim)\n      .filter(line -> !line.startsWith(\"//\"))\n      .collect(Collectors.joining(\" \"));\n\n    scriptNode.put(\"source\", src);\n\n    final ObjectNode scriptObjectNode = mapper.createObjectNode();\n    scriptObjectNode.set(\"script\", scriptNode);\n\n    final ObjectNode pipelineRequestNode = mapper.createObjectNode();\n    final ArrayNode processorsNode = pipelineRequestNode.putArray(\"processors\");\n    processorsNode.add(enrichPolicyObjectNode);\n\n    // Rename the temp fields to the final fields\n    for (final Map.Entry<String, String> entry : renameFields.entrySet()) {\n      final ObjectNode renameObjectNode = mapper.createObjectNode();\n      final ObjectNode renameNode = mapper.createObjectNode();\n      renameNode.put(\"field\", \"enrich_temp.\" + entry.getKey());\n      renameNode.put(\"target_field\", \"enrich_temp.\" + entry.getValue());\n      renameNode.put(\"ignore_missing\", true);\n      renameObjectNode.set(\"rename\", renameNode);\n      processorsNode.add(renameObjectNode);\n    }\n\n    processorsNode.add(scriptObjectNode);\n\n    final String json = pipelineRequestNode.toString();\n    final ByteArrayInputStream inputStream = new ByteArrayInputStream(\n      json.getBytes(StandardCharsets.UTF_8)\n    );\n\n    putPipeline(enrichPipelineName, inputStream);\n    return enrichPipelineName;\n  }\n\n  /**\n   * Update the index with the given pipeline\n   *\n   * @param index    the index to update\n   * @param pipeline the pipeline to use\n   * @return the task id of the update\n   */\n  public String updateByQuery(final String index, final String pipeline) {\n    return updateByQuery(index, \"{\\\"match_all\\\": {}}\", pipeline);\n  }\n\n  /**\n   * Update the enrich pipeline with the given name\n   *\n   * @param index    the index to update\n   * @param query    the query to match a subset of the documents in the index\n   * @param pipeline the pipeline to use\n   * @return the task id of the update\n   */\n  public String updateByQuery(\n    final String index,\n    final String query,\n    final String pipeline\n  ) {\n    try {\n      log.info(\n        \"Updating by query for index {} with pipeline {}\",\n        index,\n        pipeline\n      );\n\n      final String jsonPayload = String.format(\"{\\\"query\\\": %s}\", query);\n\n      final HttpHeaders headers = new HttpHeaders();\n      headers.setContentType(MediaType.APPLICATION_JSON);\n      final HttpEntity<String> entity = new HttpEntity<>(jsonPayload, headers);\n\n      // This is executed by the rest client because the Java client appears to have a bug\n      // where the response cannot be deserialized\n      final ResponseEntity<JsonNode> response = getRestTemplate()\n        .exchange(\n          new URI(\n            url +\n            \"/\" +\n            index +\n            \"/_update_by_query?wait_for_completion=false&conflicts=proceed&pipeline=\" +\n            pipeline\n          ),\n          HttpMethod.POST,\n          entity,\n          JsonNode.class\n        );\n\n      // Get the taskId from the response\n      if (response.getBody() != null) {\n        return response.getBody().at(\"/task\").asText();\n      }\n    } catch (final Exception e) {\n      log.error(\"Error updating by query\", e);\n    }\n    return null;\n  }\n\n  /**\n   * Get the task status for the given task id. We search the .tasks index to get running and completed tasks\n   *\n   * @param taskId the task id to get the status for\n   * @return the historical task JSON representation. If the task is not found, an empty JSON object is returned.\n   * If the search fails, null is returned\n   */\n  public JsonNode getHistoricalTask(final String taskId) {\n    final Query query = Query.of(b -> b.ids(idb -> idb.values(taskId)));\n    final SearchRequest request = SearchRequest.of(b ->\n      b.index(\".tasks\").query(query)\n    );\n    final SearchResponse<JsonNode> response = search(request, JsonNode.class);\n    if (response == null) {\n      return null;\n    }\n    if (\n      response.hits() != null &&\n      response.hits().hits() != null &&\n      !response.hits().hits().isEmpty()\n    ) {\n      return response.hits().hits().getFirst().source();\n    }\n    return mapper.createObjectNode();\n  }\n\n  /**\n   * Get the task status for all tasks in the cluster. Returns up to 1000 running and completed tasks\n   *\n   * @return the list of task statuses\n   */\n  public List<JsonNode> getHistoricalTasks() {\n    final Query query = Query.of(b -> b.matchAll(m -> m));\n    final SearchRequest request = SearchRequest.of(b ->\n      b.index(\".tasks\").query(query).size(1000)\n    );\n    final SearchResponse<JsonNode> response;\n    try {\n      response = client.search(request, JsonNode.class);\n    } catch (final ElasticsearchException e) {\n      // The .tasks index may not exist, so we use the client directly to avoid exception log spamming\n      return new ArrayList<>();\n    } catch (final IOException e) {\n      log.error(\"Error getting historical tasks\", e);\n      return new ArrayList<>();\n    }\n    if (\n      response != null &&\n      response.hits() != null &&\n      response.hits().hits() != null &&\n      !response.hits().hits().isEmpty()\n    ) {\n      return response.hits().hits().stream().map(Hit::source).toList();\n    }\n    return new ArrayList<>();\n  }\n\n  /**\n   * Get the running task for the given task id\n   *\n   * @param id the task id to get the running task for\n   * @return the running task JSON representation\n   */\n  public JsonNode getRunningTask(final String id) {\n    // Use the rest client to call the task api\n    try {\n      final ResponseEntity<JsonNode> response = getRestTemplate()\n        .exchange(\n          new URI(url + \"/_tasks/\" + id),\n          HttpMethod.GET,\n          null,\n          JsonNode.class\n        );\n      return response.getBody();\n    } catch (final Exception e) {\n      log.error(\"Error getting running task\", e);\n    }\n    return null;\n  }\n\n  /**\n   * Get the running tasks in the cluster\n   *\n   * @return the list of running tasks\n   */\n  public List<JsonNode> getRunningTasks() {\n    try {\n      final ResponseEntity<JsonNode> response = getRestTemplate()\n        .exchange(\n          new URI(url + \"/_tasks?detailed=true\"),\n          HttpMethod.GET,\n          null,\n          JsonNode.class\n        );\n      final List<JsonNode> tasks = new ArrayList<>();\n      if (response.getBody() != null) {\n        response\n          .getBody()\n          .at(\"/nodes\")\n          .forEach(node -> node.at(\"/tasks\").forEach(tasks::add));\n      }\n      return tasks;\n    } catch (final Exception e) {\n      log.error(\"Error getting running tasks\", e);\n    }\n    return null;\n  }\n\n  /**\n   * Helper function to wait for task completion\n   *\n   * @param taskId         The task id to wait for\n   * @param timeoutSeconds The number of seconds to wait before timing out\n   * @return A completable future that completes when the task is done\n   */\n  public CompletableFuture<Void> waitForTaskCompletion(\n    final String taskId,\n    final long timeoutSeconds\n  ) {\n    final CompletableFuture<Void> future = new CompletableFuture<>();\n    final long startTime = System.currentTimeMillis();\n    new Thread(() -> {\n      while (System.currentTimeMillis() - startTime < timeoutSeconds * 1000) {\n        final JsonNode taskInfo = getHistoricalTask(taskId);\n        // Task is done when it is null as getTaskInfo only reports on running tasks\n        if (taskInfo != null && taskInfo.at(\"/completed\").asBoolean()) {\n          future.complete(null);\n          break;\n        }\n        try {\n          Thread.sleep(500);\n        } catch (final InterruptedException e) {\n          future.completeExceptionally(e);\n        }\n      }\n    }).start();\n    return future;\n  }\n\n  /**\n   * Helper function to wait for enrich task completion\n   *\n   * @param taskId         The task id to wait for\n   * @param timeoutSeconds The number of seconds to wait before timing out\n   * @return A completable future that completes when the task is done\n   */\n  public CompletableFuture<Void> waitForEnrichTaskCompletion(\n    final String taskId,\n    final long timeoutSeconds\n  ) {\n    final CompletableFuture<Void> future = new CompletableFuture<>();\n    final long startTime = System.currentTimeMillis();\n    new Thread(() -> {\n      while (System.currentTimeMillis() - startTime < timeoutSeconds * 1000) {\n        final List<ExecutingPolicy> policies = getRunningEnrichStats();\n        final Set<String> policyIds = policies\n          .stream()\n          .map(policy -> policy.task().node() + \":\" + policy.task().id())\n          .collect(Collectors.toSet());\n\n        if (policyIds.contains(taskId)) {\n          try {\n            Thread.sleep(500);\n          } catch (final InterruptedException e) {\n            future.completeExceptionally(e);\n          }\n        } else {\n          future.complete(null);\n          break;\n        }\n      }\n    }).start();\n    return future;\n  }\n\n  /**\n   * Get all running and historical tasks\n   *\n   * @return a list of {@link Task} objects\n   */\n  public List<Task> getTasks() {\n    final List<Task> historicalTasks = getHistoricalTasks()\n      .stream()\n      .map(source -> {\n        final Task task = new Task();\n        task.setStartTimeMillis(\n          source.at(\"/task/start_time_in_millis\").asLong()\n        );\n        task.setRunningTimeNanos(\n          source.at(\"/task/running_time_in_nanos\").asLong()\n        );\n        task.setCancellable(source.at(\"/task/cancellable\").asBoolean());\n        task.setDescription(source.at(\"/task/description\").asText());\n        task.setCompleted(true);\n        task.setNodeId(source.at(\"/task/node\").asText());\n        task.setTaskId(source.at(\"/task/id\").asLong());\n\n        final JsonNode errorNode = source.at(\"/error\");\n        if (errorNode != null && !errorNode.isEmpty()) {\n          task.setError(errorNode);\n        }\n\n        task.setCancelled(!source.at(\"/response/canceled\").isMissingNode());\n\n        final long total = source.at(\"/task/status/total\").asLong();\n        final long created = source.at(\"/task/status/created\").asLong();\n        final long updated = source.at(\"/task/status/updated\").asLong();\n        final long deleted = source.at(\"/task/status/deleted\").asLong();\n        final long operatedOn = Math.max(created, Math.max(updated, deleted));\n\n        final Double progress = (double) operatedOn / total;\n        task.setProgress(progress);\n        task.setTotal(total);\n        task.setCreated(created);\n        task.setUpdated(updated);\n        task.setDeleted(deleted);\n\n        final String typeStr = source.at(\"/task/action\").asText();\n        return switch (TASK_TYPES.get(typeStr)) {\n          case REINDEX -> {\n            task.setType(REINDEX);\n            yield task;\n          }\n          case DELETE_BY_QUERY -> {\n            task.setType(DELETE_BY_QUERY);\n            yield task;\n          }\n          case UPDATE_BY_QUERY -> {\n            task.setType(UPDATE_BY_QUERY);\n            yield task;\n          }\n          case null -> null;\n        };\n      })\n      .filter(Objects::nonNull)\n      .toList();\n\n    final List<Task> runningTasks = getRunningTasks()\n      .stream()\n      .map(source -> {\n        final Task task = new Task();\n\n        task.setNodeId(source.at(\"/node\").asText());\n        task.setTaskId(source.at(\"/id\").asLong());\n        task.setStartTimeMillis(source.at(\"/start_time_in_millis\").asLong());\n        task.setRunningTimeNanos(source.at(\"/running_time_in_nanos\").asLong());\n        task.setCancellable(source.at(\"/cancellable\").asBoolean());\n        task.setDescription(source.at(\"/description\").asText());\n        task.setCompleted(false);\n\n        task.setTotal(source.at(\"/status/total\").asLong());\n        task.setCreated(source.at(\"/status/created\").asLong());\n        task.setUpdated(source.at(\"/status/updated\").asLong());\n        task.setDeleted(source.at(\"/status/deleted\").asLong());\n\n        final long total = source.at(\"/status/total\").asLong();\n        final long operatedOn = Math.max(\n          task.getCreated(),\n          Math.max(task.getUpdated(), task.getDeleted())\n        );\n        final Double progress = (double) operatedOn / total;\n        task.setProgress(progress);\n\n        final String typeStr = source.at(\"/action\").asText();\n        return switch (TASK_TYPES.get(typeStr)) {\n          case REINDEX -> {\n            task.setType(REINDEX);\n            yield task;\n          }\n          case DELETE_BY_QUERY -> {\n            task.setType(DELETE_BY_QUERY);\n            yield task;\n          }\n          case UPDATE_BY_QUERY -> {\n            task.setType(UPDATE_BY_QUERY);\n            yield task;\n          }\n          case null -> null;\n        };\n      })\n      .filter(Objects::nonNull)\n      .toList();\n\n    final List<Task> tasks = new ArrayList<>();\n    tasks.addAll(historicalTasks);\n    tasks.addAll(runningTasks);\n    return tasks;\n  }\n\n  /**\n   * Delete all historical tasks\n   */\n  public void deleteTasks() {\n    deleteIndex(\".tasks\");\n    refresh(\".tasks\");\n  }\n\n  /**\n   * Delete the task with the given id\n   *\n   * @param nodeId the node id of the task\n   * @param taskId the task id to delete\n   */\n  public void deleteTask(final String nodeId, final Long taskId) {\n    final String id = nodeId + \":\" + taskId;\n    final DeleteRequest request = DeleteRequest.of(b -> b.id(id).index(\".tasks\")\n    );\n    try {\n      client.delete(request);\n      refresh(\".tasks\");\n    } catch (final IOException e) {\n      log.error(\"Error deleting task\", e);\n    }\n  }\n\n  /**\n   * Cancel the task with the given node id and task id\n   *\n   * @param nodeId the node id of the task\n   * @param taskId the task id to cancel\n   */\n  public void cancelTask(final String nodeId, final Long taskId) {\n    final CancelRequest request = CancelRequest.of(b ->\n      b.taskId(nodeId + \":\" + taskId)\n    );\n    try {\n      client.tasks().cancel(request);\n      refresh(\".tasks\");\n    } catch (final IOException e) {\n      log.error(\"Error cancelling task\", e);\n    }\n  }\n\n  public GetMappingResponse getMapping(final String index) {\n    // Get the mapping for the index\n    try {\n      final GetMappingResponse response = client\n        .indices()\n        .getMapping(b -> b.index(index));\n      return response;\n    } catch (final IOException e) {\n      log.error(\"Error getting mapping\", e);\n      return null;\n    }\n  }\n\n  public GetIndexTemplateResponse getIndexTemplate(final String name) {\n    try {\n      return client\n        .indices()\n        .getIndexTemplate(GetIndexTemplateRequest.of(b -> b.name(name)));\n    } catch (final IOException e) {\n      log.error(\"Error getting index template\", e);\n      return null;\n    }\n  }\n\n  /**\n   * Validates that the given template matches the existing template in the cluster.\n   * @param name          the name of the template (index or component)\n   * @param templateJson  the template JSON to validate\n   * @param type          the type of the template (component or index)\n   */\n  public void validateTemplate(\n    String name,\n    JsonNode templateJson,\n    final TEMPLATE_TYPE type\n  ) {\n    final JsonNode existingTemplate = getTemplate(name, type);\n    if (existingTemplate == null || existingTemplate.isEmpty()) {\n      log.error(\"Failed to fetch existing {} {}\", type, name);\n      throw new IllegalStateException(\n        \"Failed to fetch existing \" + type.getName() + \" \" + name\n      );\n    }\n    final String existingTemplateHash = existingTemplate\n      .at(\"/_meta/hash\")\n      .asText();\n    final String templateJsonHash = hash(templateJson.toString());\n    if (!existingTemplateHash.equals(templateJsonHash)) {\n      log.error(\n        \"{}  {} already exists and does not match the expected template.\",\n        type.getName(),\n        name\n      );\n      throw new IllegalStateException(\n        type.getName() +\n        \" \" +\n        name +\n        \" already exists in the cluster and does not match the expected template.\"\n      );\n    } else {\n      log.info(\"{} {} already exists and matches\", type.getName(), name);\n    }\n  }\n\n  /**\n   * Validates that the given component template matches the existing component template in the cluster.\n   * @param name          the name of the component template\n   * @param templateJson  the component template JSON to validate\n   * @throws IllegalStateException if there is a mismatch\n   */\n  public void validateComponentTemplate(\n    final String name,\n    final JsonNode templateJson\n  ) {\n    validateTemplate(name, templateJson, TEMPLATE_TYPE.COMPONENT);\n  }\n\n  /**\n   * Validates that the given index template matches the existing index template in the cluster.\n   * @param name          the name of the index template\n   * @param templateJson  the index template JSON to validate\n   * @throws IllegalStateException if there is a mismatch\n   */\n  public void validateIndexTemplate(\n    final String name,\n    final JsonNode templateJson\n  ) {\n    validateTemplate(name, templateJson, TEMPLATE_TYPE.INDEX);\n  }\n\n  /**\n   * Internal criteria structure for property value deletion\n   */\n  public static class DeletionCriteria {\n\n    public String propertyKey;\n    public String exactValue;\n    public String minValue;\n    public String maxValue;\n\n    public DeletionCriteria(\n      String propertyKey,\n      String exactValue,\n      String minValue,\n      String maxValue\n    ) {\n      this.propertyKey = propertyKey;\n      this.exactValue = exactValue;\n      this.minValue = minValue;\n      this.maxValue = maxValue;\n    }\n  }\n\n  /**\n   * Bulk remove property values from selected documents using multiple criteria with Painless script\n   *\n   * @param index the index to update\n   * @param deletionCriteria list of deletion criteria (exact matches or ranges)\n   * @param documentIds list of document IDs to update\n   * @return the task id of the update operation\n   */\n  public String bulkRemovePropertyValues(\n    final String index,\n    final List<Object> deletionCriteria,\n    final List<String> documentIds\n  ) {\n    if (deletionCriteria == null || deletionCriteria.isEmpty()) {\n      throw new IllegalArgumentException(\n        \"Deletion criteria list cannot be null or empty\"\n      );\n    }\n    if (documentIds == null || documentIds.isEmpty()) {\n      throw new IllegalArgumentException(\n        \"Document IDs list cannot be null or empty\"\n      );\n    }\n\n    try {\n      // Build the update by query request with Painless script\n      final ObjectNode queryNode = mapper.createObjectNode();\n\n      // Create terms query to target specific documents\n      final ObjectNode termsQuery = queryNode\n        .putObject(\"query\")\n        .putObject(\"terms\");\n      final ArrayNode docIdsArray = termsQuery.putArray(\"_id\");\n      documentIds.forEach(docIdsArray::add);\n\n      // Create script to remove property values based on criteria\n      final ObjectNode scriptNode = queryNode.putObject(\"script\");\n      final String painlessScript = buildPainlessScript();\n      scriptNode.put(\"source\", painlessScript);\n\n      // Add deletion criteria as parameters\n      final ObjectNode paramsNode = scriptNode.putObject(\"params\");\n      final ArrayNode criteriaArray = paramsNode.putArray(\"deletionCriteria\");\n\n      for (final Object criteriaObj : deletionCriteria) {\n        final ObjectNode criteriaNode = criteriaArray.addObject();\n\n        // Convert the criteria object to our internal structure using reflection\n        try {\n          final String propertyKey = (String) criteriaObj\n            .getClass()\n            .getMethod(\"getPropertyKey\")\n            .invoke(criteriaObj);\n          criteriaNode.put(\"propertyKey\", propertyKey);\n\n          final String exactValue = (String) criteriaObj\n            .getClass()\n            .getMethod(\"getExactValue\")\n            .invoke(criteriaObj);\n          if (exactValue != null) {\n            criteriaNode.put(\"exactValue\", exactValue);\n          }\n\n          final String minValue = (String) criteriaObj\n            .getClass()\n            .getMethod(\"getMinValue\")\n            .invoke(criteriaObj);\n          if (minValue != null) {\n            criteriaNode.put(\"minValue\", minValue);\n          }\n\n          final String maxValue = (String) criteriaObj\n            .getClass()\n            .getMethod(\"getMaxValue\")\n            .invoke(criteriaObj);\n          if (maxValue != null) {\n            criteriaNode.put(\"maxValue\", maxValue);\n          }\n        } catch (Exception e) {\n          throw new RuntimeException(\n            \"Failed to extract criteria properties\",\n            e\n          );\n        }\n      }\n\n      final String queryJson = queryNode.toString();\n\n      log.info(\n        \"Executing bulk property value removal with {} criteria in {} documents\",\n        deletionCriteria.size(),\n        documentIds.size()\n      );\n\n      // Debug: Log the actual query being sent\n      log.info(\"Update by query JSON: {}\", queryJson);\n\n      // Execute update by query directly with script (no pipeline needed)\n      final String taskId = updateByQueryWithScript(index, queryJson);\n\n      log.info(\n        \"Started bulk property value removal for index {} with task {}\",\n        index,\n        taskId\n      );\n\n      return taskId;\n    } catch (final Exception e) {\n      log.error(\n        \"Error executing bulk property value removal for index {}\",\n        index,\n        e\n      );\n      throw new RuntimeException(\n        \"Failed to execute bulk property value removal\",\n        e\n      );\n    }\n  }\n\n  /**\n   * Build the Painless script for removing properties based on multiple criteria\n   */\n  private String buildPainlessScript() {\n    try {\n      return new String(\n        getClass().getClassLoader()\n          .getResourceAsStream(\"bulk-property-deletion.painless\")\n          .readAllBytes()\n      );\n    } catch (Exception e) {\n      throw new RuntimeException(\"Failed to load Painless script\", e);\n    }\n  }\n\n  /**\n   * Execute update by query with a custom script\n   *\n   * @param index the index to update\n   * @param queryJson the update by query JSON with script\n   * @return the task id of the update operation\n   */\n  private String updateByQueryWithScript(\n    final String index,\n    final String queryJson\n  ) {\n    try {\n      final HttpHeaders headers = new HttpHeaders();\n      headers.setContentType(MediaType.APPLICATION_JSON);\n      final HttpEntity<String> entity = new HttpEntity<>(queryJson, headers);\n\n      final String url =\n        this.url + \"/\" + index + \"/_update_by_query?wait_for_completion=false\";\n      final ResponseEntity<JsonNode> response = getRestTemplate()\n        .exchange(new URI(url), HttpMethod.POST, entity, JsonNode.class);\n\n      final JsonNode body = response.getBody();\n      if (body != null && body.has(\"task\")) {\n        return body.get(\"task\").asText();\n      } else {\n        throw new RuntimeException(\"No task ID returned from update by query\");\n      }\n    } catch (final Exception e) {\n      log.error(\n        \"Error executing update by query with script for index {}\",\n        index,\n        e\n      );\n      throw new RuntimeException(\n        \"Failed to execute update by query with script\",\n        e\n      );\n    }\n  }\n}\n","structuredPatch":[{"oldStart":2225,"oldLines":8,"newStart":2225,"newLines":6,"lines":["         documentIds.size()","       );"," ","-      // Debug: Log the actual query being sent","-      log.info(\"Update by query JSON: {}\", queryJson);"," ","       // Execute update by query directly with script (no pipeline needed)","       final String taskId = updateByQueryWithScript(index, queryJson);"]}],"userModified":false,"replaceAll":false}}
{"parentUuid":"2d05d44c-0519-4ed0-a498-fffa184e8c5f","isSidechain":false,"userType":"external","cwd":"/Users/yparis/Developer/atlantis","sessionId":"2b976885-8546-4e96-b019-e86ba8fb0435","version":"1.0.123","gitBranch":"feat-delete-facet-values","message":{"id":"msg_011FxwomGKk2jSXn6mZZZQi8","type":"message","role":"assistant","model":"claude-sonnet-4-20250514","content":[{"type":"tool_use","id":"toolu_01RKa7Ab5ANgfLbewBgSvCWU","name":"TodoWrite","input":{"todos":[{"content":"Remove System.out.println statements from ElasticsearchServiceTests.java","status":"completed","activeForm":"Removing System.out.println statements from ElasticsearchServiceTests.java"},{"content":"Remove debug comment and logging from ElasticsearchService.java","status":"completed","activeForm":"Removing debug comment and logging from ElasticsearchService.java"}]}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":7,"cache_creation_input_tokens":322,"cache_read_input_tokens":59732,"cache_creation":{"ephemeral_5m_input_tokens":322,"ephemeral_1h_input_tokens":0},"output_tokens":40,"service_tier":"standard"}},"requestId":"req_011CTTtfY9a2JGYoPnxmo4RF","type":"assistant","uuid":"0af8d2cd-e408-4b7d-bebe-050648af12fc","timestamp":"2025-09-24T20:54:54.719Z"}
{"parentUuid":"0af8d2cd-e408-4b7d-bebe-050648af12fc","isSidechain":false,"userType":"external","cwd":"/Users/yparis/Developer/atlantis","sessionId":"2b976885-8546-4e96-b019-e86ba8fb0435","version":"1.0.123","gitBranch":"feat-delete-facet-values","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01RKa7Ab5ANgfLbewBgSvCWU","type":"tool_result","content":"Todos have been modified successfully. Ensure that you continue to use the todo list to track your progress. Please proceed with the current tasks if applicable"}]},"uuid":"b6e57075-50c2-4a1f-ad3a-7f08be8c41cf","timestamp":"2025-09-24T20:54:54.774Z","toolUseResult":{"oldTodos":[{"content":"Remove System.out.println statements from ElasticsearchServiceTests.java","status":"completed","activeForm":"Removing System.out.println statements from ElasticsearchServiceTests.java"},{"content":"Remove debug comment and logging from ElasticsearchService.java","status":"in_progress","activeForm":"Removing debug comment and logging from ElasticsearchService.java"}],"newTodos":[{"content":"Remove System.out.println statements from ElasticsearchServiceTests.java","status":"completed","activeForm":"Removing System.out.println statements from ElasticsearchServiceTests.java"},{"content":"Remove debug comment and logging from ElasticsearchService.java","status":"completed","activeForm":"Removing debug comment and logging from ElasticsearchService.java"}]}}
{"parentUuid":"b6e57075-50c2-4a1f-ad3a-7f08be8c41cf","isSidechain":false,"userType":"external","cwd":"/Users/yparis/Developer/atlantis","sessionId":"2b976885-8546-4e96-b019-e86ba8fb0435","version":"1.0.123","gitBranch":"feat-delete-facet-values","message":{"id":"msg_01DHoD9h4xMwgYFhEnNNViu1","type":"message","role":"assistant","model":"claude-sonnet-4-20250514","content":[{"type":"text","text":"Perfect! I've successfully addressed all the QA issues found in the uncommitted files:\n\n## **Changes Made:**\n\n### **ElasticsearchServiceTests.java**\n- ✅ **Removed all `System.out.println()` debug statements** (lines 1567, 1569, 1599, 1601, 1604, 1624, 1626, 1631)\n- ✅ **Cleaned up debug loops** while preserving essential test logic\n- ✅ **Maintained all assertions** and test functionality\n\n### **ElasticsearchService.java**\n- ✅ **Removed debug comment** \"Debug: Log the actual query being sent\" (line 2228)\n- ✅ **Removed debug logging statement** `log.info(\"Update by query JSON: {}\", queryJson);` (line 2229)\n\n### **Files Left Unchanged:**\n- ✅ **bulk-property-deletion.painless** - No issues found, script is clean and well-structured\n\n## **Summary:**\n- All debug print statements removed from test code\n- Debug logging removed from production code\n- Code is now production-ready\n- Test functionality preserved\n- No breaking changes introduced\n\nThe uncommitted files are now clean and ready for commit without any QA issues."}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":7,"cache_creation_input_tokens":190,"cache_read_input_tokens":60054,"cache_creation":{"ephemeral_5m_input_tokens":190,"ephemeral_1h_input_tokens":0},"output_tokens":298,"service_tier":"standard"}},"requestId":"req_011CTTtfotceyZe9cT9aJJH7","type":"assistant","uuid":"e66eed12-440c-4b6d-ad6d-7377152dc512","timestamp":"2025-09-24T20:55:03.754Z"}
